{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Crawling CNBC to get the article links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_to_scrape(start_year, end_year):\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    years = [year for year in range(start_year, end_year+1)]\n",
    "    \n",
    "    strainer = SoupStrainer('a', class_ = 'SiteMapArticleList-link')\n",
    "    article_links = []\n",
    "\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in range(1,32):\n",
    "                link = f'https://www.cnbc.com/site-map/{year}/{month}/{day}/'\n",
    "\n",
    "                page = str(requests.get(link).content)\n",
    "                page = page[str(page).find('body'):]\n",
    "                soup = BeautifulSoup(page, parse_only = strainer)\n",
    "\n",
    "                links = soup.find_all('a', class_='SiteMapArticleList-link')\n",
    "                if links != []:\n",
    "                    for a_link in links:\n",
    "                        article_links.append(a_link['href'])\n",
    "\n",
    "    return article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_links = get_links_to_scrape(2006, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Scrape articles from link list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cnbc_articles(list_of_links):\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup, SoupStrainer\n",
    "    import pandas as pd\n",
    "\n",
    "    df = []\n",
    "    index = 0\n",
    "    request = requests.Session()\n",
    "\n",
    "    for link in list_of_links:\n",
    "        page = request.get(link)\n",
    "        index += 1\n",
    "\n",
    "        if page.status_code == 200:\n",
    "            try:\n",
    "                page = str(page.content)\n",
    "                page = page[page.find('<div id=\"MainContent\"'):]\n",
    "\n",
    "                soup_link = BeautifulSoup(page)\n",
    "\n",
    "                title = soup_link.find('h1', class_='ArticleHeader-headline').get_text()\n",
    "                article = soup_link.find('div', class_='ArticleBody-articleBody').get_text()\n",
    "                date = f'{link[29:31]}-{link[26:28]}-{link[21:25]}'\n",
    "                topic = soup_link.find('a', class_='ArticleHeader-eyebrow').get_text()\n",
    "\n",
    "                df.append([title, topic, date, article, link])\n",
    "\n",
    "                print(f'({index}/{len(list_of_links)}) : {link}')\n",
    "            except:\n",
    "                print(f'({index}/{len(list_of_links)}) : Skipped')\n",
    "        else:\n",
    "            print(f'({index}/{len(list_of_links)}) : Skipped')\n",
    "\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_cnbc_articles(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Collecting the datasets, if collected over multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dataset(datasets_path):\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    datasets = [ds for ds in datasets_path if '.csv' in ds]\n",
    "\n",
    "    frames = []\n",
    "    for dataset in datasets:\n",
    "        df = pd.read_csv(dataset, error_bad_lines=False, index_col=False)\n",
    "        df = df[df.columns[-5:]]\n",
    "        df.columns = ['Title', 'Topic', 'Date', 'Content', 'Link']\n",
    "        frames.append(df)\n",
    "\n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df = collect_dataset(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].astype('str')\n",
    "df['Title'] = df['Title'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_clutter(text):\n",
    "    \n",
    "    import re\n",
    "\n",
    "    text = re.sub(r'\\\\x[A-Za-z0-9_]{2}', '', text)\n",
    "    text = re.sub(r'VIDEO([0-9]|[0-9]{2}):[0-9]{4}:[0-9]{2}', ' ', text)\n",
    "    text = text.replace('Getty Images', '')\n",
    "    text = re.sub(r',','', text)\n",
    "    text = re.sub(r\"n'\",'n', text)\n",
    "    text = re.sub(r'  ',' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df,column):\n",
    "\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    import pandas as pd\n",
    " \n",
    "    tokens = []\n",
    "\n",
    "    df[column].apply(remove_clutter)\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for article in nlp.pipe(df[column], disable=['parser']):\n",
    "\n",
    "        article_tok = [token.lemma_.lower() for token in article if _\n",
    "            token.is_alpha _\n",
    "            and not token.is_stop _\n",
    "            and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] _\n",
    "            and token.ent_type_ not in ['PERSON', 'MONEY', 'PERCENT', 'LOC', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL'] _\n",
    "            and len(token)>1]\n",
    "\n",
    "        tokens.append(article_tok)\n",
    "\n",
    "        index += 1\n",
    "        print(f'Processed {index}/{len(df[column])}')\n",
    "\n",
    "    df['tokens'] = tokens\n",
    "\n",
    "    df['clean_articles'] = df['tokens'].map(lambda row: \" \".join(row))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaning(df, 'Content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Attempting to classify topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output topics to construct final_topic_list manually\n",
    "pd.DataFrame(df.Topic.unique()).to_csv('unique_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read topic mapping\n",
    "topic_list = pd.read_csv('final_topic_list.csv', sep = \";\")\n",
    "topic_list_clean = pd.DataFrame.dropna(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Topics to articles\n",
    "predetermined = []\n",
    "index = 0\n",
    "for topic in df[\"Topic\"]:\n",
    "    index += 1\n",
    "    if topic in list(topic_list_clean[\"Topic\"]):\n",
    "        predetermined.append(topic_list_clean[topic_list_clean[\"Topic\"] == topic][\"Predetermined topic\"].to_numpy()[0])\n",
    "    else:\n",
    "        predetermined.append(\"Other\")\n",
    "    print(f'{index}')\n",
    "df['final_topic'] = predetermined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting to train and predict\n",
    "df_labelled = df[df['final_topic'] != 'Other']\n",
    "df_predict = df[df['final_topic'] == 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training into train and test and construct LSA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "r_state = 123\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_labelled['tokens'], df_labelled['final_topic'], test_size=0.25, random_state=r_state)\n",
    "\n",
    "from gensim import models, corpora\n",
    "import ast\n",
    "\n",
    "data_processed = x_train.to_numpy()\n",
    "data_conversion = []\n",
    "for line in data_processed:\n",
    "    line = ast.literal_eval(line)\n",
    "    data_conversion.append(line)\n",
    "data_processed = data_conversion\n",
    "\n",
    "dictionary = corpora.Dictionary(data_processed)\n",
    "corpus = [dictionary.doc2bow(line) for line in data_processed]\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "lsa_model = models.LsiModel(tfidf[corpus], id2word=dictionary, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost and conclude that it fails...\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state = r_state)\n",
    "xgb_model.fit(x_train, y_train)\n",
    "xgb_prediction = xgb_model.predict(x_test)\n",
    "print(classification_report(y_test,xgb_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Filtering articles instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['final_topic'] != 'Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Collecting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Collecting articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_articles(start_date, end_date, df, start_range, end_range):\n",
    "\n",
    "    from datetime import timedelta, datetime\n",
    "    import ast\n",
    "\n",
    "    date_list = [start_date + timedelta(days=x) for x in range(0,int((end_date - start_date).days)+1)]\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "\n",
    "    date_index = []\n",
    "    count = 0\n",
    "\n",
    "    for date in date_list:\n",
    "\n",
    "        articles = df[(df['Date'] <= date + timedelta(days=end_range)) & (df['Date'] >= date + timedelta(days=start_range))].head(30*(1+end_range-start_range))\n",
    "        count += len(articles)\n",
    "        processed = \"\"\n",
    "        for article in articles['tokens']:\n",
    "            try:\n",
    "                \n",
    "                article = str(article).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\",\"\")\n",
    "                processed += article\n",
    "                processed += \" \"\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        date_index.append([date, processed])\n",
    "        print(f'{date}: {count}')\n",
    "\n",
    "    return date_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "sdate = date(2006, 11, 27)\n",
    "edate = date(2020, 11, 30)\n",
    "\n",
    "x = rolling_articles(sdate, edate, df, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Calculating returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = pd.read_csv('bb_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(prices, interval):\n",
    "\n",
    "    prices['Dates'] = pd.to_datetime(prices['Dates']).dt.date\n",
    "    \n",
    "    date_index = []\n",
    "    for i in range(0,len(prices)):\n",
    "        try:\n",
    "            date = prices.iloc[i,0]\n",
    "            prices_at_date = prices.iloc[i,1:]\n",
    "            prices_at_future_date = prices.iloc[i+interval,1:]\n",
    "            return_at_date = list(prices_at_future_date / prices_at_date)\n",
    "            returns = [date]\n",
    "            for sector in return_at_date:\n",
    "                returns.append(sector)\n",
    "            date_index.append(returns)\n",
    "            print(f'{date}')\n",
    "        except:\n",
    "            pass\n",
    "    df = pd.DataFrame(date_index, columns = prices.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = calculate_returns(bb, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.set_index('Date')\n",
    "y = y.set_index('Dates')\n",
    "df = pd.concat([y, x.reindex(y.index)], axis = 1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural Network Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dates'] = pd.to_datetime(df['Dates']).dt.date\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tokens in list of articles(in a day) - We train the gensim model on all data prior to 2011\n",
    "from datetime import date\n",
    "token_list = list([token.split(\" \") for token in df[df['Dates'] <= date(2010, 12, 31)]['tokens']])\n",
    "size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training gensim word2vec\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences = token_list, size = size, window = 5, workers = 4, min_count = 20)\n",
    "# Vocab size:\n",
    "words = list(model.wv.vocab)\n",
    "print('vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename = 'article_embeddings.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index= {}\n",
    "f = open(os.path.join('', 'article_embeddings.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from datetime import date\n",
    "\n",
    "# vectorise the text samples into a 2D integer tensor with the data for token list before 2011\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(token_list)\n",
    "\n",
    "# Now convert all the tokens to sequences\n",
    "token_list = list([token.split(\" \") for token in df['tokens']])\n",
    "sequences = tokenizer.texts_to_sequences(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences to make input length constant\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Get average of list\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "\n",
    "max_length = int(Average([len(doc) for doc in token_list]))\n",
    "articles_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print('Shape of article tensor:', articles_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the embedding matrix\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccesary packages for NN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Untrainable model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                           size,\n",
    "                           embeddings_initializer=Constant(embedding_matrix),\n",
    "                           mask_zero = True,\n",
    "                           input_length=None,\n",
    "                           trainable=False)\n",
    "# Add embedding layer\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# Add a LSTM layer with 50 internal units.\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, dropout = 0.2))\n",
    "# Add a Dense layer with 12 units.\n",
    "model.add(Dense(12))\n",
    "# Add compiler with XXX\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "# Print summary of model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our Y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df.iloc[:,1:13]\n",
    "y = y - 1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y)\n",
    "y = scaler.transform(y)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define walk_forward function for our model training\n",
    "def walk_forward_validation(model, epochs, x, y, step_size, train_steps, val_window):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    n_records = len(x)\n",
    "    n_init_train = step_size * train_steps\n",
    "    train_pred = []\n",
    "    val_pred = []\n",
    "    mse_scores = []\n",
    "    for i in range(n_init_train, n_records, step_size):\n",
    "      \n",
    "        train_from = i-n_init_train\n",
    "        train_to = i\n",
    "        test_from = i+1\n",
    "        test_to = i+val_window\n",
    "\n",
    "        x_train, x_test = x[train_from:train_to], x[test_from:test_to]\n",
    "        y_train, y_test = y[train_from:train_to], y[test_from:test_to]\n",
    "        \n",
    "        print(f'Train from {i-n_init_train} to {i} and validate for {i+1} to {i+val_window}')\n",
    "        model.fit(x_train, y_train, epochs=epochs, verbose=1)\n",
    "\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        for y_train_day in y_train_pred:\n",
    "            train_pred.append(y_train_day.tolist())\n",
    "        \n",
    "        y_pred = model.predict(x_test)\n",
    "        for y_test_day in y_pred:\n",
    "            val_pred.append(y_test_day.tolist())\n",
    "\n",
    "        train_mse = mean_squared_error(y_train,y_train_pred)\n",
    "        val_mse = mean_squared_error(y_test,y_pred)\n",
    "        mse_scores.append([train_mse, val_mse])\n",
    "\n",
    "        print(f'     train: {train_mse} \\nvalidation: {val_mse} \\n')\n",
    "\n",
    "    return train_pred, val_pred, mse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch Hyper parameter tuning\n",
    "epoch_tuning_performance = []\n",
    "for epoch in range(1,11):\n",
    "    train_pred, val_pred, validation_metrics = walk_forward_validation(model = model, epochs = epoch, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
    "    validation_metrics_total = pd.DataFrame(validation_metrics).mean(axis=0)\n",
    "    mean_train_mse = validation_metrics_total.iloc[0]\n",
    "    mean_val_mse = validation_metrics_total.iloc[1]\n",
    "    epoch_tuning_performance.append([mean_train_mse, mean_val_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using plotly.express to display epoch tuning\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(epoch_tuning_performance)\n",
    "fig = px.line(df, x='epochs', y=[\"train\",\"validation\"], color_discrete_sequence=['cornflowerblue', 'indigo'])\n",
    "fig.update_xaxes(title_text='Epochs', showgrid=False)\n",
    "fig.update_yaxes(title_text='MSE')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and get MSE & Predicted values\n",
    "train_pred, val_pred, validation_metrics = walk_forward_validation(model = model, epochs = epoch, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
    "validation_metrics_total = pd.DataFrame(validation_metrics).mean(axis=0)\n",
    "mean_train_mse = validation_metrics_total.iloc[0]\n",
    "mean_val_mse = validation_metrics_total.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(train_pred)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(val_pred)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Trainable model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                           size,\n",
    "                           embeddings_initializer=Constant(embedding_matrix),\n",
    "                           mask_zero = True,\n",
    "                           input_length=None,\n",
    "                           trainable=True)\n",
    "# Add embedding layer\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# Add a LSTM layer with 50 internal units.\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, dropout = 0.2))\n",
    "# Add a Dense layer with 12 units.\n",
    "model.add(Dense(12))\n",
    "# Add compiler with XXX\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "# Print summary of model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and get MSE & Predicted values\n",
    "train_pred_trained, val_pred_trained, validation_metrics_trained = walk_forward_validation(model = model, epochs = epoch, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
    "validation_metrics_trained_total = pd.DataFrame(validation_metrics_trained).mean(axis=0)\n",
    "mean_train_trained_mse = validation_metrics_trained_total.iloc[0]\n",
    "mean_val_trained_mse = validation_metrics_trained_total.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(train_pred_trained)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(val_pred_trained)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(validation_metrics_trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
