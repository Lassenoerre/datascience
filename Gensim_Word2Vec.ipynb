{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Gensim_Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "751YVSW2Brs0"
      },
      "source": [
        "Import relevant packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kADR_C3WBrs0"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F472_TKmBrs1"
      },
      "source": [
        "Import dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZSXnx9FBrs1"
      },
      "source": [
        "df = pd.read_csv('https://www.dropbox.com/s/llxun0a85lpf6e3/df_final.csv?dl=1')\n",
        "df['Dates'] = pd.to_datetime(df['Dates']).dt.date\n",
        "#df = df[:100]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "vBkZPXsBBrs1",
        "outputId": "80c9d04a-f0d2-4345-b631-e02a31a260c2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>SPTR INDEX</th>\n",
              "      <th>SPTRINFT INDEX</th>\n",
              "      <th>SPTRENRS INDEX</th>\n",
              "      <th>SPTRFINL INDEX</th>\n",
              "      <th>SPTRHLTH INDEX</th>\n",
              "      <th>SPTRINDU INDEX</th>\n",
              "      <th>SPTRCOND INDEX</th>\n",
              "      <th>SPTRUTIL INDEX</th>\n",
              "      <th>SPTRMATR INDEX</th>\n",
              "      <th>SPTRCONS INDEX</th>\n",
              "      <th>SPTRTELS INDEX</th>\n",
              "      <th>SPTRRLST INDEX</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2006-12-12</td>\n",
              "      <td>1.010224</td>\n",
              "      <td>1.002885</td>\n",
              "      <td>1.004642</td>\n",
              "      <td>1.016601</td>\n",
              "      <td>1.014123</td>\n",
              "      <td>1.016900</td>\n",
              "      <td>1.010797</td>\n",
              "      <td>1.002999</td>\n",
              "      <td>1.016288</td>\n",
              "      <td>1.006769</td>\n",
              "      <td>0.993621</td>\n",
              "      <td>0.983808</td>\n",
              "      <td>imagine disney nbc universal team desperate co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2006-12-13</td>\n",
              "      <td>1.007461</td>\n",
              "      <td>1.001175</td>\n",
              "      <td>0.982023</td>\n",
              "      <td>1.017135</td>\n",
              "      <td>1.015294</td>\n",
              "      <td>1.022835</td>\n",
              "      <td>1.006014</td>\n",
              "      <td>0.995535</td>\n",
              "      <td>1.009593</td>\n",
              "      <td>1.005985</td>\n",
              "      <td>0.992353</td>\n",
              "      <td>0.999058</td>\n",
              "      <td>amc jump public market movie theater company a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006-12-14</td>\n",
              "      <td>0.995271</td>\n",
              "      <td>0.983649</td>\n",
              "      <td>0.959095</td>\n",
              "      <td>1.006377</td>\n",
              "      <td>1.007680</td>\n",
              "      <td>1.009989</td>\n",
              "      <td>0.991821</td>\n",
              "      <td>0.990393</td>\n",
              "      <td>0.988344</td>\n",
              "      <td>1.002454</td>\n",
              "      <td>0.997374</td>\n",
              "      <td>0.985457</td>\n",
              "      <td>red sox look go sign japanese pitcher report d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2006-12-15</td>\n",
              "      <td>0.989004</td>\n",
              "      <td>0.974092</td>\n",
              "      <td>0.962900</td>\n",
              "      <td>0.998287</td>\n",
              "      <td>0.997860</td>\n",
              "      <td>0.996399</td>\n",
              "      <td>0.994200</td>\n",
              "      <td>0.990149</td>\n",
              "      <td>0.974067</td>\n",
              "      <td>0.998513</td>\n",
              "      <td>0.988887</td>\n",
              "      <td>0.983588</td>\n",
              "      <td>official start award season nomination good hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2006-12-18</td>\n",
              "      <td>0.992207</td>\n",
              "      <td>0.980559</td>\n",
              "      <td>0.989430</td>\n",
              "      <td>0.993734</td>\n",
              "      <td>0.997623</td>\n",
              "      <td>0.993896</td>\n",
              "      <td>0.995171</td>\n",
              "      <td>0.997737</td>\n",
              "      <td>0.983931</td>\n",
              "      <td>0.998484</td>\n",
              "      <td>0.993293</td>\n",
              "      <td>0.982202</td>\n",
              "      <td>merck win vioxx case get victory federal case ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dates  ...                                             tokens\n",
              "0  2006-12-12  ...  imagine disney nbc universal team desperate co...\n",
              "1  2006-12-13  ...  amc jump public market movie theater company a...\n",
              "2  2006-12-14  ...  red sox look go sign japanese pitcher report d...\n",
              "3  2006-12-15  ...  official start award season nomination good hi...\n",
              "4  2006-12-18  ...  merck win vioxx case get victory federal case ...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxEQolo3Brs2"
      },
      "source": [
        "## Define x and y variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DhPJba_Brs2"
      },
      "source": [
        "# list of tokens in list of articles(in a day)\n",
        "from datetime import date\n",
        "token_list = list([token.split(\" \") for token in df[df['Dates'] <= date(2010, 12, 31)]['tokens']])"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPjcZFh6Brs2",
        "outputId": "2c74964a-0fc6-4c27-9c76-d98a829c361e"
      },
      "source": [
        "# day 1\n",
        "token_list[0]"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imagine',\n",
              " 'disney',\n",
              " 'nbc',\n",
              " 'universal',\n",
              " 'team',\n",
              " 'desperate',\n",
              " 'compete',\n",
              " 'pay',\n",
              " 'copyright',\n",
              " 'material',\n",
              " 'desperate',\n",
              " 'time',\n",
              " 'call',\n",
              " 'desperate',\n",
              " 'measure',\n",
              " 'cook',\n",
              " 'spoil',\n",
              " 'broth',\n",
              " 'movielink',\n",
              " 'collaboration',\n",
              " 'studio',\n",
              " 'offer',\n",
              " 'digital',\n",
              " 'download',\n",
              " 'politic',\n",
              " 'get',\n",
              " 'intense',\n",
              " 'take',\n",
              " 'longer',\n",
              " 'ground',\n",
              " 'trouble',\n",
              " 'need',\n",
              " 'team',\n",
              " 'stand',\n",
              " 'chance',\n",
              " 'terrible',\n",
              " 'weekend',\n",
              " 'warner',\n",
              " 'bros',\n",
              " 'totally',\n",
              " 'different',\n",
              " 'equally',\n",
              " 'disappointing',\n",
              " 'film',\n",
              " 'expect',\n",
              " 'movie',\n",
              " 'message',\n",
              " 'big',\n",
              " 'romantic',\n",
              " 'leo',\n",
              " 'pic',\n",
              " 'sight',\n",
              " 'set',\n",
              " 'lo',\n",
              " 'film',\n",
              " 'cost',\n",
              " 'bring',\n",
              " 'ouch',\n",
              " 'end',\n",
              " 'warner',\n",
              " 'brothers',\n",
              " 'spectrum',\n",
              " 'unaccompanied',\n",
              " 'minors',\n",
              " 'hope',\n",
              " 'home',\n",
              " 'bring',\n",
              " 'expensive',\n",
              " 'budget',\n",
              " 'est',\n",
              " 'look',\n",
              " 'bunch',\n",
              " 'dance',\n",
              " 'penguin',\n",
              " 'totally',\n",
              " 'knock',\n",
              " 'warner',\n",
              " 'prospect',\n",
              " 'strong',\n",
              " 'christmas',\n",
              " 'box',\n",
              " 'office',\n",
              " 'season',\n",
              " 'thing',\n",
              " 'cheap',\n",
              " 'tv',\n",
              " 'spot',\n",
              " 'company',\n",
              " 'call',\n",
              " 'cheap',\n",
              " 'tv',\n",
              " 'spot',\n",
              " 'lot',\n",
              " 'speculation',\n",
              " 'cheap',\n",
              " 'tv',\n",
              " 'spot',\n",
              " 'buy',\n",
              " 'tivo',\n",
              " 'company',\n",
              " 'deny',\n",
              " 'raise',\n",
              " 'question',\n",
              " 'buy',\n",
              " 'commercial',\n",
              " 'skip',\n",
              " 'company',\n",
              " 'maybe',\n",
              " 'question',\n",
              " 'comment',\n",
              " 'company',\n",
              " 'transform',\n",
              " 'transform',\n",
              " 'lot',\n",
              " 'work',\n",
              " 'leave',\n",
              " 'merck',\n",
              " 'analyst',\n",
              " 'day',\n",
              " 'company',\n",
              " 'headquarters',\n",
              " 'central',\n",
              " 'new',\n",
              " 'jersey',\n",
              " 'company',\n",
              " 'reveal',\n",
              " 'tine',\n",
              " 'street',\n",
              " 'suspect',\n",
              " 'call',\n",
              " 'cetp',\n",
              " 'inhibitor',\n",
              " 'drug',\n",
              " 'development',\n",
              " 'pipeline',\n",
              " 'type',\n",
              " 'drug',\n",
              " 'pfizerpulle',\n",
              " 'plug',\n",
              " 'increase',\n",
              " 'risk',\n",
              " 'death',\n",
              " 'long',\n",
              " 'clinical',\n",
              " 'trial',\n",
              " 'merck',\n",
              " 'say',\n",
              " 'see',\n",
              " 'heart',\n",
              " 'problem',\n",
              " 'mind',\n",
              " 'pfizers',\n",
              " 'independent',\n",
              " 'safety',\n",
              " 'date',\n",
              " 'monitoring',\n",
              " 'board',\n",
              " 'problem',\n",
              " 'torcetrapib',\n",
              " 'patient',\n",
              " 'merck',\n",
              " 'head',\n",
              " 'say',\n",
              " 'company',\n",
              " 'carefully',\n",
              " 'review',\n",
              " 'datum',\n",
              " 'try',\n",
              " 'decide',\n",
              " 'future',\n",
              " 'plan',\n",
              " 'drug',\n",
              " 'live',\n",
              " 'interview',\n",
              " 'ceo',\n",
              " 'lilly',\n",
              " 'analyst',\n",
              " 'meeting',\n",
              " 'pfizer',\n",
              " 'wake',\n",
              " 'torcetrapib',\n",
              " 'news',\n",
              " 'take',\n",
              " 'merck',\n",
              " 'interview',\n",
              " 'cnbc',\n",
              " 'analyst',\n",
              " 'meeting',\n",
              " 'year',\n",
              " 'merck',\n",
              " 'spokespeople',\n",
              " 'interview',\n",
              " 'make',\n",
              " 'available',\n",
              " 'print',\n",
              " 'reporter',\n",
              " 'reason',\n",
              " 'internal',\n",
              " 'company',\n",
              " 'backstory',\n",
              " 'think',\n",
              " 'unfair',\n",
              " 'accessible',\n",
              " 'news',\n",
              " 'medium',\n",
              " 'broadcast',\n",
              " 'print',\n",
              " 'necessarily',\n",
              " 'reporter',\n",
              " 'news',\n",
              " 'conference',\n",
              " 'exception',\n",
              " 'bbc',\n",
              " 'crew',\n",
              " 'tv',\n",
              " 'outlet',\n",
              " 'company',\n",
              " 'offer',\n",
              " 'first',\n",
              " 'exclusive',\n",
              " 'reporter',\n",
              " 'problem',\n",
              " 'especially',\n",
              " 'finally',\n",
              " 'merck',\n",
              " 'say',\n",
              " 'face',\n",
              " 'vioxx',\n",
              " 'lawsuit',\n",
              " 'case',\n",
              " 'set',\n",
              " 'trial',\n",
              " 'atlantic',\n",
              " 'city',\n",
              " 'give',\n",
              " 'hope',\n",
              " 'vioxx',\n",
              " 'successor',\n",
              " 'call',\n",
              " 'arcoxia',\n",
              " 'merck',\n",
              " 'expect',\n",
              " 'fda',\n",
              " 'decision',\n",
              " 'painkiller',\n",
              " 'reveal',\n",
              " 'fda',\n",
              " 'advisory',\n",
              " 'panel',\n",
              " 'examine',\n",
              " 'drug',\n",
              " 'mercks',\n",
              " 'headquarters',\n",
              " 'whitehouse',\n",
              " 'station',\n",
              " 'new',\n",
              " 'jersey',\n",
              " 'beautiful',\n",
              " 'campus',\n",
              " 'producer',\n",
              " 'see',\n",
              " 'gaggle',\n",
              " 'loud',\n",
              " 'wild',\n",
              " 'turkey',\n",
              " 'winding',\n",
              " 'drive',\n",
              " 'asian',\n",
              " 'inspire',\n",
              " 'main',\n",
              " 'building',\n",
              " 'deer',\n",
              " 'roam',\n",
              " 'area',\n",
              " 'security',\n",
              " 'relatively',\n",
              " 'tight',\n",
              " 'thankfully',\n",
              " 'near',\n",
              " 'bad',\n",
              " 'pfizer',\n",
              " 'major',\n",
              " 'drug',\n",
              " 'company',\n",
              " 'meet',\n",
              " 'big',\n",
              " 'pfizers',\n",
              " 'financial',\n",
              " 'focused',\n",
              " 'update',\n",
              " 'question',\n",
              " 'comment',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7THABLLiBrs2",
        "outputId": "342bfd7e-ab19-4fc5-daa4-02925d687e3a"
      },
      "source": [
        "# define size of token_list\n",
        "#size = len(token_list)\n",
        "size = 100\n",
        "size"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDO3TbV5Brs2"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af7ghbK6Brs3"
      },
      "source": [
        "Train word2vec model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR9QxHqjBrs3",
        "outputId": "f9869978-9874-4e8f-9efc-1c8c30a866e5"
      },
      "source": [
        "model = gensim.models.Word2Vec(sentences = token_list, size = size, window = 5, workers = 4, min_count = 20)\n",
        "# Vocab size:\n",
        "words = list(model.wv.vocab)\n",
        "print('vocabulary size: %d' % len(words))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size: 7311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpsHWXVFBrs3",
        "outputId": "25192c02-7f77-49a7-8316-0ee5a3534d7c"
      },
      "source": [
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['imagine', 'disney', 'nbc', 'universal', 'team', 'desperate', 'compete', 'pay', 'copyright', 'material', 'time', 'call', 'measure', 'cook', 'collaboration', 'studio', 'offer', 'digital', 'download', 'politic', 'get', 'intense', 'take', 'longer', 'ground', 'trouble', 'need', 'stand', 'chance', 'terrible', 'weekend', 'warner', 'totally', 'different', 'equally', 'disappointing', 'film', 'expect', 'movie', 'message', 'big', 'sight', 'set', 'cost', 'bring', 'ouch', 'end', 'brothers', 'spectrum', 'hope', 'home', 'expensive', 'budget', 'est', 'look', 'bunch', 'dance', 'knock', 'prospect', 'strong', 'christmas', 'box', 'office', 'season', 'thing', 'cheap', 'tv', 'spot', 'company', 'lot', 'speculation', 'buy', 'tivo', 'deny', 'raise', 'question', 'commercial', 'skip', 'maybe', 'comment', 'transform', 'work', 'leave', 'merck', 'analyst', 'day', 'headquarters', 'central', 'new', 'jersey', 'reveal', 'street', 'suspect', 'drug', 'development', 'pipeline', 'type', 'plug', 'increase', 'risk', 'death', 'long', 'clinical', 'trial', 'say', 'see', 'heart', 'problem', 'mind', 'pfizers', 'independent', 'safety', 'date', 'monitoring', 'board', 'patient', 'head', 'carefully', 'review', 'datum', 'try', 'decide', 'future', 'plan', 'live', 'interview', 'ceo', 'lilly', 'meeting', 'pfizer', 'wake', 'news', 'cnbc', 'year', 'make', 'available', 'print', 'reporter', 'reason', 'internal', 'think', 'unfair', 'accessible', 'medium', 'broadcast', 'necessarily', 'conference', 'exception', 'bbc', 'crew', 'outlet', 'exclusive', 'especially', 'finally', 'face', 'vioxx', 'lawsuit', 'case', 'atlantic', 'city', 'give', 'successor', 'fda', 'decision', 'advisory', 'panel', 'examine', 'mercks', 'station', 'beautiful', 'campus', 'producer', 'loud', 'wild', 'turkey', 'drive', 'asian', 'inspire', 'main', 'building', 'area', 'security', 'relatively', 'tight', 'near', 'bad', 'major', 'meet', 'financial', 'focused', 'update', '', 'amc', 'jump', 'public', 'market', 'theater', 'entertainment', 'private', 'holdings', 'file', 'sec', 'ipo', 'know', 'exactly', 'debt', 'report', 'revenue', 'loss', 'go', 'estimate', 'merger', 'screen', 'number', 'biz', 'flux', 'few', 'dollar', 'absolutely', 'system', 'well', 'weaken', 'route', 'redstones', 'national', 'amusements', 'destination', 'high', 'restaurant', 'video', 'game', 'bar', 'idea', 'good', 'way', 'stay', 'competitive', 'weigh', 'hollywood', 'blogosphere', 'la', 'weekly', 'columnist', 'blogger', 'sort', 'break', 'bash', 'mgm', 'release', 'teen', 'remake', 'black', 'sister', 'anti', 'theme', 'intend', 'audience', 'suppose', 'non', 'ask', 'moviegoer', 'sure', 'hate', 'violence', 'passion', 'blogge', 'anemic', 'opening', 'story', 'feature', 'moral', 'teenager', 'want', 'watch', 'have', 'open', 'present', 'hold', 'interested', 'fare', 'money', 'business', 'horror', 'sell', 'follow', 'saga', 'continue', 'status', 'times', 'pellicano', 'love', 'worthy', 'detail', 'allege', 'find', 'dead', 'fish', 'rose', 'stop', 'car', 'discourage', 'investigate', 'possible', 'mob', 'abc', 'join', 'launch', 'content', 'site', 'yahoo', 'partner', 'expand', 'deal', 'provide', 'ad', 'support', 'user', 'twice', 'include', 'blue', 'chip', 'stuff', 'will', 'flow', 'actually', 'battle', 'old', 'school', 'provider', 'run', 'online', 'viewer', 'tack', 'powerful', 'google', 'parent', 'fact', 'aggressively', 'cut', 'overhead', 'slash', 'hundred', 'job', 'help', 'part', 'disneys', 'struggle', 'bernstein', 'write', 'cable', 'network', 'remain', 'concerned', 'earning', 'growth', 'factor', 'rate', 'perform', 'concern', 'park', 'slow', 'rise', 'sport', 'tough', 'comp', 'stanleys', 'equal', 'weight', 'price', 'target', 'experience', 'gradual', 'note', 'issue', 'assumption', 'pixar', 'add', 'net', 'channel', 'espn', 'margin', 'hard', 'change', 'able', 'difference', 'domestic', 'advertising', 'china', 'road', 'lead', 'likely', 'overtake', 'japan', 'large', 'world', 'fast', 'grow', 'spend', 'japans', 'surprise', 'sale', 'total', 'internet', 'explode', 'madison', 'avenue', 'peanut', 'man', 'spalding', 'crisis', 'management', 'impressed', 'handle', 'nba', 'ball', 'recall', 'talk', 'check', 'web', 'base', 'nbas', 'switch', 'leather', 'start', 'refund', 'consumer', 'purchase', 'return', 'composite', 'claim', 'retail', 'value', 'cover', 'previously', 'taxis', 'shipping', 'charge', 'click', 'receive', 'authorization', 'phone', 'mail', 'ship', 'address', 'telephone', 'code', 'copy', 'receipt', 'ma', 'specific', 'feel', 'free', 'valid', 'allow', 'processing', 'apologize', 'cause', 'thank', 'patience', 'understanding', 'brand', 'alienate', 'bet', 'basketball', 'retailer', 'come', 'people', 'ethical', 'treatment', 'upset', 'excerpt', 'letter', 'obtain', 'manager', 'campaign', 'like', 'lifetime', 'supply', 'hand', 'cream', 'excuse', 'superstar', 'willing', 'shot', 'variety', 'rich', 'organic', 'perfect', 'player', 'critical', 'volunteer', 'test', 'play', 'surely', 'routine', 'schedule', 'ncaa', 'education', 'miss', 'adjust', 'interest', 'spare', 'thousand', 'cow', 'unnecessary', 'consider', 'suggestion', 'chain', 'tell', 'giants', 'fan', 'store', 'wall', 'retire', 'popular', 'cancel', 'order', 'rarely', 'night', 'true', 'opportunity', 'gatorade', 'certainly', 'cookie', 'dough', 'flavor', 'general', 'colon', 'blow', 'cereal', 'fiber', 'equivalent', 'bowl', 'serve', 'baseball', 'product', 'flex', 'steroid', 'limit', 'dish', 'fictional', 'radio', 'host', 'favorite', 'holiday', 'recipe', 'owner', 'seasons', 'segment', 'popcorn', 'cheese', 'line', 'resist', 'im', 'golf', 'dozen', 'speak', 'founder', 'surprised', 'trademark', 'newspaper', 'publisher', 'harbor', 'michigan', 'register', 'apparently', 'word', 'exist', 'patent', 'database', 'pga', 'location', 'red', 'sox', 'sign', 'japanese', 'pitcher', 'worth', 'pretty', 'close', 'posting', 'fee', 'lion', 'investment', 'contract', 'league', 'history', 'colorado', 'los', 'angeles', 'dodgers', 'agree', 'obviously', 'situation', 'agent', 'negotiate', 'promise', 'record', 'let', 'boras', 'demand', 'rescue', 'child', 'course', 'client', 'leverage', 'negotiation', 'wait', 'mean', 'brutal', 'effort', 'steal', 'ticket', 'happen', 'hear', 'buzz', 'fenway', 'sports', 'group', 'marketing', 'firm', 'angle', 'impressive', 'roster', 'sponsor', 'yankees', 'piece', 'nascar', 'rumor', 'initial', 'conversation', 'profitable', 'logical', 'connection', 'virtually', 'sponsorship', 'continental', 'airlines', 'official', 'york', 'promote', 'flight', 'promptly', 'there', 'marketer', 'players', 'association', 'probably', 'guarantee', 'incentive', 'count', 'overall', 'salary', 'account', 'luxury', 'tax', 'sharing', 'calculation', 'executive', 'series', 'title', 'nice', 'benefit', 'tampa', 'bay', 'devil', 'rays', 'television', 'split', 'cap', 'economically', 'irrelevant', 'unlikely', 'entire', 'capitalize', 'seat', 'basically', 'rating', 'own', 'fever', 'huge', 'guy', 'pitch', 'forget', 'signage', 'ebay', 'enable', 'savvy', 'turn', 'vacant', 'wear', 'career', 'currently', 'die', 'read', 'nfl', 'kansas', 'chiefs', 'originally', 'dallas', 'move', 'co', 'north', 'american', 'soccer', 'championship', 'tennis', 'family', 'credit', 'phrase', 'super', 'numerous', 'build', 'lord', 'style', 'epic', 'fly', 'dragon', 'fantasy', 'track', 'leak', 'brace', 'disappointment', 'awareness', 'low', 'tracking', 'poll', 'use', 'sense', 'kind', 'recognize', 'importantly', 'prompt', 'choice', 'hit', 'fall', 'flat', 'stir', 'forrester', 'research', 'heaven', 'forbid', 'music', 'apple', 'itunes', 'website', 'little', 'staggering', 'background', 'computer', 'powerhouse', 'ipod', 'map', 'place', 'incredibly', 'halo', 'effect', 'device', 'influence', 'contrary', 'popularity', 'success', 'typically', 'swift', 'faithful', 'flame', 'suggest', 'blog', 'email', 'loser', 'extreme', 'actual', 'poor', 'helpful', 'ready', 'analysis', 'interesting', 'gain', 'momentum', 'prior', 'plunge', 'shopper', 'average', 'song', 'swell', 'recently', 'conclude', 'merely', 'trade', 'pie', 'tired', 'incredible', 'seize', 'figure', 'dire', 'prediction', 'gloomy', 'ahead', 'share', 'award', 'nomination', 'hint', 'nominate', 'hungry', 'golden', 'seal', 'poster', 'beam', 'paramount', 'production', 'snag', 'crash', 'baby', 'trend', 'win', 'voter', 'winner', 'worldwide', 'specialty', 'division', 'gold', 'cash', 'lions', 'gates', 'cbs', 'records', 'spinoff', 'viacom', 'focus', 'eye', 'integrate', 'fill', 'slot', 'control', 'industry', 'troubled', 'smart', 'bother', 'cd', 'goal', 'venture', 'generate', 'prime', 'programming', 'strike', 'label', 'save', 'primetime', 'license', 'pricey', 'distribution', 'show', 'house', 'traction', 'right', 'worry', 'sony', 'definitely', 'pirate', 'chest', 'dvd', 'action', 'parody', 'occasionally', 'seriously', 'youtube', 'clip', 'nail', 'van', 'chicago', 'kick', 'tie', 'touchdown', 'clothe', 'soft', 'drink', 'category', 'paper', 'finish', 'hopefully', 'ill', 'forward', 'here', 'advice', 'young', 'marketable', 'position', 'quarterback', 'study', 'victory', 'federal', 'pick', 'alabama', 'stock', 'verdict', 'pace', 'judge', 'lump', 'confusing', 'jury', 'texas', 'plaintiff', 'lawyer', 'suit', 'far', 'damage', 'earmark', 'legal', 'corner', 'begin', 'quiet', 'beat', 'rest', 'pharma', 'biotechs', 'fine', 'suspension', 'knicks', 'smith', 'nuggets', 'suspend', 'hell', 'lose', 'shoe', 'suffer', 'reaction', 'electronic', 'arts', 'debut', 'assure', 'fighting', 'wonder', 'wii', 'tale', 'particularly', 'entertaining', 'hotel', 'console', 'lend', 'guest', 'throw', 'legend', 'injury', 'enthusiastic', 'send', 'controller', 'plenty', 'fear', 'wrist', 'strap', 'replacement', 'insist', 'wrong', 'gamer', 'workout', 'thick', 'clarify', 'upgrade', 'guess', 'depend', 'active', 'couch', 'potato', 'gaming', 'fun', 'admit', 'tournament', 'secret', 'source', 'expectation', 'bounce', 'range', 'relieve', 'teenage', 'boy', 'book', 'jordan', 'minute', 'entry', 'dividend', 'nickel', 'widely', 'speculate', 'early', 'name', 'chairman', 'effective', 'simply', 'director', 'pop', 'announcement', 'statement', 'post', 'fold', 'page', 'lillys', 'companys', 'profit', 'maker', 'responsible', 'half', 'sue', 'behalf', 'document', 'downplay', 'push', 'indication', 'diabetes', 'pound', 'allegation', 'regard', 'elderly', 'press', 'essentially', 'hide', 'vigorously', 'object', 'practice', 'article', 'multi', 'blockbuster', 'decline', 'couple', 'stabilize', 'publicity', 'franchise', 'stable', 'investor', 'clearly', 'heavy', 'volume', 'blunt', 'selloff', 'trading', 'announce', 'widespread', 'soon', 'separately', 'pony', 'shareholder', 'vote', 'impotence', 'cialis', 'pill', 'buck', 'fair', 'bid', 'proxy', 'advisor', 'undervalue', 'sweeten', 'pot', 'shoot', 'tomorrow', 'biotech', 'meantime', 'gear', 'coverage', 'jpmorgan', 'healthcare', 'san', 'francisco', 'nearly', 'attendance', 'ring', 'trojans', 'unusual', 'sit', 'icon', 'era', 'bit', 'power', 'broker', 'corporate', 'america', 'stage', 'aol', 'arguably', 'remarkable', 'personal', 'emotional', 'turnaround', 'murder', 'son', 'english', 'teacher', 'kill', 'student', 'mask', 'wreck', 'bitter', 'walk', 'person', 'answer', 'rock', 'solid', 'attend', 'levin', 'week', 'ago', 'wife', 'exact', 'intensive', 'undergo', 'complete', 'psychological', 'facility', 'staff', 'expert', 'mix', 'match', 'eastern', 'component', 'create', 'approach', 'prefer', 'celebrity', 'foot', 'bill', 'ongoing', 'successful', 'strategy', 'ultimately', 'trickle', 'advantage', 'ultimate', 'delivery', 'mental', 'health', 'medical', 'care', 'individual', 'tank', 'everyday', 'enthusiasm', 'project', 'mission', 'that', 'thoughtful', 'candid', 'thin', 'titan', 'remember', 'difficult', 'cope', 'life', 'distance', 'matter', 'tragedy', 'mother', 'brother', 'father', 'pain', 'realize', 'permanently', 'affect', 'activity', 'survivor', 'literally', 'circumstance', 'corporation', 'responsibility', 'wave', 'emotion', 'express', 'away', 'quest', 'point', 'easy', 'journey', 'outside', 'outset', 'revolutionary', 'connect', 'collective', 'doctor', 'force', 'complex', 'aspect', 'boom', 'reach', 'inner', 'peace', 'achieve', 'transformation', 'confident', 'meaning', 'purpose', 'heal', 'environment', 'important', 'core', 'center', 'recognition', 'legacy', 'mistake', 'abandon', 'determine', 'ruin', 'morning', 'journal', 'separation', 'separate', 'odd', 'extended', 'insight', 'wish', 'hbos', 'pioneer', 'warners', 'interactive', 'initiative', 'orlando', 'florida', 'supporter', 'occur', 'form', 'truly', 'exciting', 'dynamic', 'believe', 'hbo', 'citizen', 'liberty', 'malone', 'corps', 'transaction', 'ok', 'hang', 'happy', 'rare', 'fatal', 'viral', 'infection', 'multiple', 'biogen', 'idec', 'genentech', 'filing', 'agency', 'disclose', 'approve', 'concrete', 'evidence', 'nervous', 'rule', 'possibility', 'trim', 'commentary', 'represent', 'threat', 'biib', 'great', 'bear', 'expense', 'forge', 'late', 'disclosure', 'afraid', 'mid', 'small', 'teva', 'pharmaceuticals', 'glaxosmithkline', 'human', 'sciences', 'quarter', 'woman', 'minority', 'today', 'postpone', 'improve', 'somewhat', 'mull', 'complain', 'celgene', 'keep', 'stream', 'positive', 'cancer', 'recent', 'implosion', 'competitor', 'example', 'footprint', 'instant', 'access', 'googles', 'search', 'moment', 'usa', 'picture', 'real', 'cool', 'indicate', 'reader', 'reality', 'additional', 'episode', 'replace', 'decent', 'foreign', 'language', 'gross', 'films', 'canada', 'pull', 'publish', 'queen', 'colleague', 'boot', 'murdochs', 'corp', 'draw', 'conclusion', 'closet', 'expose', 'hardly', 'similarity', 'unusually', 'involve', 'renew', 'exec', 'working', 'bury', 'conglomerate', 'blame', 'slip', 'up', 'cruise', 'depression', 'rant', 'impossible', 'embarrassment', 'oscar', 'scandal', 'finance', 'produce', 'sick', 'spotlight', 'wary', 'debate', 'filmmaker', 'final', 'delay', 'hurt', 'easily', 'crucial', 'attention', 'ego', 'challenge', 'shift', 'distribute', 'relationship', 'navigate', 'summer', 'square', 'dark', 'pursue', 'rocky', 'result', 'okay', 'attract', 'ton', 'showtime', 'air', 'fix', 'rip', 'financier', 'disaster', 'bomb', 'girl', 'operation', 'performance', 'art', 'touch', 'expire', 'appeal', 'disappoint', 'denver', 'predict', 'allen', 'wade', 'playing', 'manage', 'seller', 'term', 'accord', 'establish', 'ucla', 'catch', 'color', 'reebok', 'acquire', 'dent', 'assume', 'pump', 'window', 'local', 'northwest', 'light', 'event', 'community', 'author', 'version', 'magazine', 'star', 'nugget', 'teammate', 'member', 'elect', 'remove', 'publication', 'plane', 'appear', 'behavior', 'current', 'edition', 'develop', 'aircraft', 'customer', 'kindler', 'altogether', 'door', 'committee', 'transition', 'role', 'key', 'influential', 'mandatory', 'retirement', 'age', 'ramp', 'illegal', 'outpace', 'alternative', 'household', 'peer', 'porn', 'adult', 'tune', 'later', 'playboy', 'nuts', 'auction', 'memo', 'advise', 'jean', 'nyc', 'charity', 'actor', 'plot', 'underdog', 'absolute', 'bodog', 'respectively', 'rank', 'comparison', 'sake', 'licensing', 'branded', 'mass', 'enhancement', 'bout', 'dream', 'settle', 'distributor', 'band', 'tiger', 'substantial', 'mirror', 'album', 'nike', 'await', 'roll', 'anniversary', 'nikes', 'visible', 'classic', 'white', 'alive', 'special', 'extremely', 'limited', 'quantity', 'court', 'oneal', 'al', 'vanity', 'publishing', 'investing', 'wealthy', 'waste', 'masse', 'straight', 'folk', 'president', 'invest', 'narrow', 'deep', 'skiing', 'beach', 'known', 'fortune', 'married', 'sun', 'valley', 'idaho', 'miami', 'spread', 'advertisement', 'zero', 'culture', 'phenomenon', 'put', 'upload', 'photo', 'toll', 'funny', 'random', 'toy', 'card', 'visitor', 'autograph', 'athlete', 'courtesy', 'collector', 'list', 'badly', 'kid', 'advent', 'businessman', 'parking', 'one', 'dramatically', 'act', 'dramatic', 'backdrop', 'california', 'crazy', 'state', 'fascinating', 'entrepreneur', 'discuss', 'publicly', 'lately', 'institute', 'partnership', 'hills', 'technology', 'advocate', 'ambitious', 'emphasize', 'healthy', 'living', 'maintenance', 'instead', 'rely', 'dole', 'foods', 'castle', 'southern', 'californias', 'country', 'club', 'employee', 'avid', 'plant', 'greenhouse', 'breed', 'horse', 'residential', 'estate', 'united', 'states', 'property', 'island', 'prize', 'asset', 'correctly', 'immediately', 'chunk', 'hawaii', 'mogul', 'respond', 'space', 'deck', 'certain', 'financially', 'forbes', 'personally', 'jet', 'view', 'frequent', 'land', 'excitement', 'rain', 'grass', 'flood', 'hotels', 'construction', 'income', 'penny', 'drop', 'detroit', 'army', 'war', 'ii', 'sleep', 'bush', 'food', 'cup', 'coffee', 'quick', 'ownership', 'pass', 'cat', 'equipment', 'desire', 'hot', 'oust', 'pension', 'defer', 'compensation', 'vacation', 'spokesman', 'honor', 'obligation', 'employment', 'americans', 'government', 'reverse', 'mouth', 'sole', 'mega', 'fruit', 'vegetable', 'developer', 'employ', 'directly', 'brick', 'flagship', 'shop', 'step', 'session', 'bone', 'scan', 'blood', 'beta', 'testing', 'hospital', 'designer', 'glass', 'technician', 'ray', 'preserve', 'room', 'decorate', 'chair', 'traditional', 'upscale', 'reluctant', 'practical', 'consult', 'suite', 'relax', 'guidance', 'learn', 'meal', 'class', 'body', 'fat', 'customize', 'trainer', 'affiliate', 'privately', 'retreat', 'afford', 'affordable', 'treat', 'institutes', 'completely', 'unique', 'selling', 'potential', 'creative', 'offering', 'normal', 'concept', 'vice', 'explore', 'aols', 'million', 'massive', 'identity', 'recover', 'steep', 'tailspin', 'shave', 'aftermath', 'faster', 'recovery', 'twist', 'wise', 'maintain', 'rimm', 'motion', 'past', 'rosy', 'projection', 'gift', 'forecast', 'subscription', 'blackberry', 'top', 'subscriber', 'soar', 'monumental', 'eps', 'consensus', 'operating', 'surprising', 'pressure', 'flexibility', 'slap', 'plastic', 'cell', 'squeeze', 'generation', 'noise', 'fox', 'tackle', 'length', 'college', 'football', 'cotton', 'lag', 'stick', 'recap', 'engine', 'msn', 'cowboys', 'philadelphia', 'eagles', 'idol', 'field', 'cheer', 'item', 'brown', 'index', 'measurable', 'attribute', 'trust', 'alliance', 'score', 'sour', 'bite', 'bold', 'accomplishment', 'anticipate', 'inflation', 'royal', 'poise', 'artist', 'timing', 'documentary', 'bench', 'flick', 'columbia', 'pictures', 'anytime', 'houston', 'jacksonville', 'nation', 'double', 'airport', 'international', 'proximity', 'advance', 'ocean', 'south', 'staple', 'presence', 'motorola', 'reserve', 'arena', 'party', 'snow', 'upper', 'tremendous', 'male', 'female', 'ratio', 'creativity', 'usual', 'compare', 'sweet', 'shortage', 'carry', 'shipment', 'damn', 'arm', 'fate', 'improvement', 'choose', 'fracture', 'osteoporosis', 'residual', 'beneficial', 'researcher', 'sanofi', 'aventis', 'procter', 'gamble', 'water', 'swallow', 'roche', 'month', 'bolster', 'efficacy', 'specialist', 'duke', 'university', 'quit', 'frequency', 'detect', 'formation', 'astrazeneca', 'joint', 'abbott', 'hip', 'british', 'thinking', 'stomach', 'hinder', 'digest', 'option', 'approval', 'investigation', 'stunning', 'taint', 'possibly', 'repair', 'image', 'pr', 'ft', 'caution', 'perspective', 'flush', 'grant', 'refer', 'preliminary', 'finding', 'improper', 'insider', 'notion', 'cfo', 'resign', 'reasonable', 'doubt', 'knowledge', 'fire', 'torch', 'aware', 'refuse', 'wrongdoing', 'resemble', 'defend', 'accuracy', 'guilty', 'sin', 'compelling', 'law', 'jobs', 'period', 'blank', 'bears', 'eat', 'closer', 'reports', 'apples', 'counsel', 'accountant', 'conduct', 'initiate', 'voluntary', 'discover', 'misconduct', 'relate', 'precede', 'instance', 'favorable', 'select', 'unaware', 'accounting', 'implication', 'officer', 'recording', 'reporting', 'character', 'resolve', 'quickly', 'proper', 'ensure', 'inform', 'auditor', 'audit', 'historical', 'impact', 'require', 'minimum', 'stadium', 'fluctuate', 'highest', 'seattle', 'orleans', 'baltimore', 'truck', 'combination', 'secondary', 'educate', 'patriot', 'saint', 'seahawks', 'mixed', 'martial', 'surpass', 'relevance', 'fight', 'witness', 'ufc', 'grand', 'gate', 'exceed', 'rutgers', 'athletic', 'department', 'reportedly', 'row', 'fence', 'swimming', 'aid', 'eliminate', 'voice', 'chill', 'royalty', 'supposedly', 'prince', 'tnt', 'broadcaster', 'mark', 'edge', 'casino', 'pursuit', 'neck', 'phenomenal', 'pirates', 'musical', 'dirt', 'micro', 'nasty', 'travel', 'floor', 'genius', 'listen', 'shock', 'backdating', 'unveil', 'ability', 'revolution', 'gadget', 'digitally', 'quality', 'def', 'broadband', 'adopter', 'simple', 'bristol', 'myers', 'squibb', 'merge', 'french', 'newsletter', 'information', 'unnamed', 'agreement', 'sensible', 'suitor', 'plavix', 'consolidation', 'sector', 'inevitable', 'drugmaker', 'relative', 'competition', 'generic', 'pricing', 'washington', 'democrat', 'banc', 'rep', 'tap', 'bell', 'gilead', 'flu', 'fighter', 'tamiflu', 'visit', 'inventor', 'remark', 'suddenly', 'overnight', 'dell', 'bag', 'surface', 'turmoil', 'thomson', 'equity', 'partners', 'bonus', 'cutback', 'sizeable', 'ugly', 'pacific', 'crest', 'securities', 'topline', 'glitch', 'payroll', 'worker', 'offshore', 'desperately', 'headcount', 'distress', 'best', 'excessive', 'hiring', 'pc', 'unit', 'rival', 'hp', 'dells', 'nasdaq', 'listing', 'exchange', 'enjoy', 'pose', 'vision', 'evening', 'landscape', 'duo', 'knee', 'jerk', 'engagement', 'program', 'advertiser', 'upfront', 'speech', 'resources', 'summit', 'model', 'toyota', 'survey', 'friends', 'shape', 'library', 'hd', 'definition', 'hour', 'satellite', 'operator', 'spark', 'frenzy', 'standard', 'carpet', 'tally', 'exposure', 'julius', 'associates', 'specialize', 'promotion', 'mention', 'dress', 'handful', 'oscars', 'eager', 'swing', 'regular', 'whatsoever', 'apology', 'stone', 'inning', 'spring', 'training', 'outstanding', 'better', 'clear', 'massachusetts', 'raw', 'dub', 'billionaire', 'readily', 'organization', 'level', 'contractor', 'wash', 'organize', 'reply', 'percent', 'scratch', 'legitimate', 'anyone', 'hill', 'concentrate', 'second', 'iac', 'solely', 'acquisition', 'exclude', 'satisfied', 'condition', 'appropriate', 'reflect', 'ambition', 'diller', 'portfolio', 'decrease', 'hsn', 'attempt', 'jazz', 'strength', 'revamp', 'killer', 'reiterate', 'thought', 'request', 'fit', 'drag', 'pharmas', 'pharmaceutical', 'casualty', 'buyback', 'pile', 'rid', 'heel', 'setback', 'slam', 'johnson', 'assignment', 'jnj', 'size', 'workforce', 'reduction', 'increasingly', 'apparent', 'scramble', 'weather', 'downturn', 'fiscally', 'earlier', 'medicare', 'crack', 'combat', 'anemia', 'amgn', 'diversify', 'amgen', 'coincidentally', 'hinge', 'pivotal', 'mab', 'short', 'beijing', 'translation', 'disturb', 'heavily', 'enter', 'notice', 'clean', 'crackdown', 'specifically', 'smoking', 'beverage', 'origin', 'sac', 'ban', 'reasoning', 'native', 'soda', 'motto', 'understand', 'myspace', 'social', 'networking', 'hook', 'bargain', 'iger', 'brilliant', 'traffic', 'virtual', 'fairly', 'safe', 'design', 'stamp', 'upside', 'magic', 'kingdom', 'hearing', 'ipods', 'macs', 'boon', 'tempt', 'understatement', 'nintendo', 'frankly', 'playground', 'racket', 'global', 'frame', 'guitar', 'hero', 'lover', 'jam', 'replica', 'branding', 'circuit', 'shelf', 'accessory', 'permission', 'placement', 'jockey', 'friend', 'useful', 'encourage', 'section', 'sporting', 'snap', 'handset', 'palm', 'adjusted', 'slightly', 'basis', 'crown', 'capital', 'infusion', 'elevation', 'reward', 'handsomely', 'plummet', 'ensue', 'maximize', 'affirm', 'surround', 'failure', 'portion', 'rim', 'razor', 'service', 'cooperation', 'palms', 'everybodys', 'blackberrys', 'technically', 'mets', 'playoff', 'pocket', 'argue', 'concession', 'table', 'automatically', 'citi', 'yankee', 'choke', 'mock', 'lease', 'postseason', 'clemens', 'rookie', 'costly', 'convert', 'starter', 'metric', 'fail', 'disagree', 'incorrect', 'calculate', 'boutique', 'rodman', 'renshaw', 'highly', 'controversial', 'therapeutic', 'vaccine', 'outperform', 'banking', 'reputation', 'intellectual', 'caveat', 'paradigm', 'horizon', 'preventive', 'diagnose', 'defense', 'eventually', 'overlook', 'oncology', 'prostate', 'dendreon', 'bind', 'dndn', 'usually', 'regulatory', 'moon', 'provenge', 'dendreons', 'prolong', 'survival', 'proof', 'interim', 'previous', 'prove', 'significant', 'underway', 'mouse', 'trap', 'warning', 'bank', 'subprime', 'de', 'volatility', 'institution', 'downside', 'careful', 'giant', 'citigroup', 'swiss', 'wealth', 'ubs', 'induce', 'energy', 'attractive', 'economist', 'commodity', 'precious', 'metal', 'utility', 'dow', 'largely', 'gainer', 'eli', 'percentage', 'schering', 'plough', 'deutsche', 'bullish', 'sgp', 'weak', 'single', 'beneficiary', 'concentration', 'mrk', 'tier', 'currency', 'lift', 'emphasis', 'consistent', 'accelerate', 'fundamental', 'furious', 'nokia', 'gps', 'mapping', 'leader', 'valuation', 'dig', 'stumble', 'isuppli', 'shopping', 'navigation', 'devices', 'extraordinary', 'iphone', 'capability', 'potentially', 'stroke', 'command', 'premium', 'capable', 'bandwagon', 'shell', 'anger', 'verizon', 'decidedly', 'mess', 'billion', 'hardware', 'successfully', 'software', 'ironically', 'collect', 'trick', 'marketplace', 'ingredient', 'lucrative', 'ignore', 'entirely', 'fulfil', 'initially', 'acknowledge', 'theory', 'opposite', 'fancy', 'physical', 'plus', 'fashioned', 'itune', 'listener', 'supreme', 'reject', 'tobacco', 'punitive', 'cigarette', 'overturn', 'ruling', 'argument', 'liability', 'smoker', 'danger', 'ail', 'certify', 'uphold', 'defendant', 'groups', 'morris', 'carolina', 'constitutional', 'process', 'prohibit', 'avoid', 'invoke', 'fraud', 'attorney', 'urge', 'litigation', 'proceeding', 'subject', 'prevail', 'dissent', 'havoc', 'statin', 'cholesterol', 'crestor', 'sequentially', 'be', 'seek', 'combo', 'vytorin', 'lipitor', 'unprecedented', 'assault', 'lack', 'robust', 'insurance', 'pharmacy', 'prescription', 'rush', 'zocor', 'chapter', 'unfold', 'lucentis', 'implementation', 'similar', 'paragraph', 'addition', 'lay', 'precise', 'engineer', 'propose', 'reduce', 'pro', 'facilitate', 'movement', 'circulation', 'minimize', 'takeaway', 'spell', 'substitute', 'significantly', 'years', 'confirm', 'wal', 'marts', 'toshiba', 'format', 'embrace', 'chief', 'garner', 'majority', 'spin', 'reference', 'sag', 'stake', 'speed', 'london', 'balance', 'sheet', 'preparation', 'gm', 'presumably', 'incentivize', 'repurchase', 'heat', 'dial', 'load', 'preside', 'pare', 'merrill', 'lynchs', 'stanley', 'trophy', 'candidate', 'stearns', 'parachute', 'package', 'outcry', 'punishment', 'negative', 'burst', 'bubble', 'startling', 'enron', 'congress', 'headline', 'wrap', 'conceive', 'lie', 'essence', 'criminal', 'pen', 'fool', 'victim', 'appearance', 'compliance', 'paperwork', 'consultant', 'warn', 'vest', 'authority', 'atmosphere', 'punish', 'severely', 'capitalism', 'willingness', 'endure', 'error', 'path', 'earn', 'outrage', 'amount', 'tenth', 'lynch', 'tumble', 'brokerage', 'cdo', 'screw', 'agenda', 'exit', 'mainly', 'wachovia', 'dare', 'formally', 'notify', 'inevitably', 'fallout', 'guard', 'cautious', 'bridge', 'column', 'smoke', 'bed', 'enemy', 'services', 'myriad', 'branch', 'americas', 'reel', 'holder', 'saudi', 'thq', 'ea', 'dunk', 'pre', 'original', 'driver', 'contribute', 'sustain', 'madden', 'less', 'microsoft', 'dominate', 'npd', 'outsell', 'combine', 'tilt', 'noticeably', 'rear', 'sequential', 'playstation', 'surge', 'wilson', 'cycle', 'ride', 'marketshare', 'reliance', 'softness', 'activision', 'optimistic', 'tone', 'bake', 'revision', 'bottomline', 'lower', 'sound', 'bump', 'welcome', 'las', 'vegas', 'referee', 'gambling', 'publicize', 'accurate', 'town', 'cross', 'sacramento', 'professional', 'commissioner', 'stern', 'betting', 'steiner', 'diego', 'union', 'tribune', 'accept', 'kudo', 'fabulous', 'rod', 'adventure', 'oil', 'economy', 'equities', 'crude', 'mf', 'terrific', 'digit', 'hedge', 'geopolitical', 'spike', 'exxon', 'earnings', 'concede', 'refining', 'gasoline', 'exploration', 'hurdle', 'temporary', 'ethanol', 'fuel', 'enterprise', 'senior', 'fellow', 'hudson', 'corn', 'endanger', 'population', 'games', 'instability', 'region', 'threaten', 'leadership', 'treasury', 'secretary', 'prepare', 'globes', 'reliable', 'clue', 'root', 'reminder', 'voting', 'iron', 'divide', 'anymore', 'machine', 'lucky', 'wide', 'importance', 'broad', 'afghanistan', 'dreamworks', 'awful', 'science', 'shooting', 'reign', 'pin', 'ge', 'kmart', 'humor', 'german', 'premiere', 'demographic', 'foxs', 'sundance', 'festival', 'utah', 'selection', 'resident', 'indie', 'accompany', 'cite', 'wage', 'tear', 'opaque', 'inconsistent', 'bellwether', 'scope', 'continued', 'insulate', 'diversified', 'walt', 'discount', 'boost', 'russian', 'capture', 'topic', 'disadvantage', 'et', 'midtown', 'manhattan', 'camera', 'tape', 'essential', 'tool', 'presentation', 'closed', 'bloodthinner', 'canadian', 'lean', 'acquirer', 'pole', 'shed', 'attack', 'simultaneous', 'oppose', 'confuse', 'counter', 'distinguish', 'blitz', 'devote', 'airwave', 'coming', 'mtv', 'pickup', 'worried', 'lebron', 'kobe', 'signature', 'communication', 'celebration', 'perfectly', 'compatible', 'confusion', 'respective', 'deliver', 'hop', 'platform', 'memorable', 'distract', 'unbelievable', 'warm', 'commission', 'beg', 'erectile', 'dysfunction', 'bmo', 'markets', 'peak', 'viagra', 'headquarter', 'nonetheless', 'monitor', 'vick', 'closure', 'saving', 'layoff', 'downgrade', 'barron', 'journalism', 'armour', 'dive', 'cramer', 'mad', 'deserve', 'rally', 'climb', 'dump', 'longtime', 'brands', 'umbrella', 'jordans', 'signal', 'priority', 'swoosh', 'gap', 'steve', 'barrys', 'undisclosed', 'mart', 'sized', 'merchandise', 'appointment', 'sub', 'womens', 'joe', 'racing', 'footwear', 'apparel', 'forever', 'associate', 'bath', 'famous', 'tail', 'closing', 'scene', 'standing', 'booth', 'coach', 'ironic', 'lacrosse', 'fastest', 'gasp', 'invite', 'expansion', 'titans', 'garden', 'inside', 'niche', 'bright', 'yesterday', 'everyone', 'explanation', 'academy', 'shut', 'ballot', 'depart', 'miramax', 'confirmation', 'vista', 'endorsement', 'zune', 'xbox', 'associated', 'lebrons', 'kit', 'endorser', 'mile', 'powerade', 'standalone', 'mall', 'id', 'ua', 'cleat', 'babolat', 'identify', 'clothing', 'wrestling', 'golfer', 'nature', 'tour', 'charter', 'logo', 'left', 'shirt', 'strange', 'weird', 'tech', 'dip', 'surprisingly', 'progress', 'panama', 'subsequently', 'certainty', 'underlie', 'recommendation', 'coat', 'stent', 'prevent', 'hire', 'permanent', 'takeout', 'erbitux', 'imclone', 'systems', 'discussion', 'opinion', 'overwhelming', 'jones', 'opponent', 'closely', 'kidney', 'disease', 'chemotherapy', 'amgens', 'dose', 'assessment', 'resource', 'uk', 'politician', 'grapple', 'federer', 'awhile', 'officially', 'palo', 'alto', 'bureau', 'jose', 'silicon', 'consolidate', 'operate', 'excited', 'ample', 'chipmaker', 'intel', 'amd', 'mac', 'squarely', 'innovation', 'ought', 'windows', 'overcome', 'contest', 'nvidia', 'graphic', 'advanced', 'technologies', 'conventional', 'wisdom', 'microprocessor', 'excel', 'realistic', 'principal', 'creator', 'vocal', 'critic', 'severe', 'disrupt', 'restriction', 'seamless', 'calendar', 'prop', 'sing', 'proud', 'obvious', 'sunshine', 'making', 'buyer', 'twc', 'drama', 'grace', 'dedication', 'lionsgate', 'tooth', 'comedy', 'pricier', 'busy', 'spanish', 'derby', 'awesome', 'generator', 'mini', 'museum', 'coo', 'artificial', 'permit', 'policy', 'unable', 'fulfill', 'stud', 'rake', 'master', 'chef', 'grill', 'menu', 'chicken', 'beef', 'rice', 'costco', 'mount', 'goodwill', 'defensive', 'captain', 'bonds', 'bond', 'awkward', 'subway', 'arrive', 'hat', 'urlacher', 'vitaminwater', 'bottle', 'podium', 'representative', 'england', 'patriots', 'offensive', 'whisper', 'ear', 'caa', 'talent', 'agencys', 'kitchen', 'assistant', 'god', 'behemoth', 'nickname', 'evil', 'empire', 'wars', 'suicide', 'suns', 'mvp', 'fitness', 'downtown', 'vancouver', 'interior', 'specialized', 'steel', 'accent', 'triple', 'race', 'rationale', 'kentucky', 'preakness', 'dubai', 'belmont', 'glory', 'ridiculous', 'scary', 'connected', 'birthday', 'formal', 'shy', 'holy', 'grail', 'dry', 'portal', 'yahoos', 'function', 'amazon', 'bidding', 'tag', 'pepsico', 'beauty', 'nextel', 'champion', 'needle', 'endorse', 'tropicana', 'blast', 'harder', 'element', 'sweep', 'grade', 'contestant', 'millionaire', 'invention', 'dumb', 'animation', 'animate', 'incur', 'cinema', 'scotland', 'financing', 'relativity', 'media', 'restructure', 'developmental', 'boston', 'crime', 'prosecute', 'dangerous', 'rack', 'inch', 'wire', 'affairs', 'wildly', 'inflate', 'novel', 'ten', 'hover', 'scare', 'driving', 'amazing', 'expenditure', 'attach', 'authentic', 'maryland', 'extend', 'succeed', 'europe', 'discipline', 'locker', 'pair', 'natural', 'hole', 'charlotte', 'pronounce', 'ally', 'diet', 'embarrassing', 'gsk', 'alli', 'direct', 'celebrate', 'piper', 'inventory', 'gauge', 'availability', 'capacity', 'respondent', 'stretch', 'shove', 'dad', 'mercury', 'shout', 'round', 'excite', 'crowd', 'block', 'overly', 'privacy', 'conscious', 'hoard', 'photographer', 'handler', 'circle', 'shoulder', 'revolutionize', 'ubiquitous', 'unfortunate', 'blip', 'iphones', 'kiss', 'awards', 'laugh', 'computing', 'server', 'laptop', 'handheld', 'honest', 'activate', 'activation', 'nightmare', 'definitive', 'anecdotal', 'substantive', 'fame', 'creation', 'thoroughly', 'hype', 'meaningful', 'feedback', 'wwe', 'bio', 'revert', 'tribute', 'discontinue', 'criticize', 'stockholder', 'investigator', 'wikipedia', 'police', 'coincidence', 'writer', 'actively', 'profile', 'graduate', 'heisman', 'tennessee', 'arkansas', 'strongly', 'math', 'organizer', 'wimbledon', 'luckily', 'buick', 'dolphins', 'polo', 'redskins', 'outfit', 'pittsburgh', 'steelers', 'rev', 'election', 'registration', 'spearhead', 'declare', 'contact', 'expertise', 'demo', 'turf', 'industrys', 'eagle', 'outfitters', 'underwrite', 'independence', 'trip', 'exhibit', 'democracy', 'partys', 'partisan', 'chinese', 'glaxo', 'cardiovascular', 'migraine', 'hamster', 'anomaly', 'lobby', 'hurry', 'scientific', 'reviewer', 'repeat', 'fdas', 'optimism', 'accelerated', 'timeline', 'sorry', 'absence', 'shanghai', 'pride', 'aside', 'coca', 'cola', 'occasional', 'billboard', 'wildlife', 'data', 'rocket', 'youth', 'informal', 'france', 'western', 'finals', 'swirl', 'puzzle', 'dedicate', 'mobile', 'wireless', 'upstart', 'upcoming', 'fcc', 'prototype', 'spending', 'catalyst', 'convince', 'monetize', 'fund', 'vp', 'insure', 'orient', 'animal', 'sniff', 'foray', 'lesson', 'approvable', 'spokesperson', 'professor', 'stanford', 'sadly', 'administration', 'tumor', 'promising', 'sufficient', 'warrant', 'regulation', 'erode', 'intervene', 'proponent', 'particular', 'braves', 'atlanta', 'rational', 'scream', 'interpret', 'translate', 'slump', 'turner', 'anticipation', 'sharp', 'assistance', 'earnhardt', 'respect', 'notre', 'dame', 'owe', 'dog', 'proposition', 'advertise', 'deduction', 'asleep', 'richmond', 'submit', 'illustrated', 'introduce', 'secure', 'onyx', 'liver', 'splash', 'asco', 'halt', 'chemo', 'lousy', 'clutter', 'wrigley', 'paint', 'cubs', 'opener', 'mlb', 'side', 'depth', 'intrigue', 'racer', 'originate', 'afternoon', 'citys', 'daytona', 'primary', 'gas', 'dutch', 'adidas', 'lifestyle', 'toe', 'flee', 'girlfriend', 'actress', 'arizona', 'cardinals', 'usc', 'countless', 'gay', 'sever', 'surgery', 'laser', 'correct', 'ski', 'yorks', 'women', 'men', 'rbs', 'horn', 'ranking', 'thrive', 'log', 'sleeve', 'telecast', 'grab', 'eyeball', 'ceremony', 'pure', 'motors', 'jc', 'penney', 'mcdonalds', 'intelligence', 'fatigue', 'sky', 'indianapolis', 'consulting', 'invent', 'stupid', 'oversee', 'administer', 'encrypt', 'vince', 'personnel', 'skill', 'regardless', 'employer', 'applicant', 'nfls', 'solve', 'application', 'workplace', 'tend', 'scale', 'evaluate', 'comparable', 'vary', 'degree', 'middle', 'draft', 'observe', 'indicator', 'involved', 'standpoint', 'implement', 'blind', 'algorithm', 'link', 'technique', 'sweat', 'evaporate', 'teach', 'batting', 'execs', 'xm', 'sirius', 'enormous', 'quadruple', 'nhl', 'hall', 'ohio', 'syracuse', 'crunch', 'stress', 'hockey', 'temperature', 'amylin', 'byetta', 'injectable', 'sugar', 'synthetic', 'monster', 'cooler', 'fbr', 'requirement', 'doc', 'prescribe', 'amln', 'alkermes', 'phase', 'repeatedly', 'outcome', 'industries', 'manufacturing', 'stark', 'contrast', 'lab', 'approximately', 'accommodate', 'distant', 'todays', 'ordinary', 'controversy', 'basic', 'premise', 'positively', 'self', 'nominee', 'stack', 'beverly', 'turnout', 'firmly', 'maximum', 'ditch', 'favor', 'stance', 'senator', 'editor', 'intimate', 'gathering', 'modern', 'crush', 'cast', 'fashion', 'synergy', 'string', 'packaging', 'impression', 'russia', 'conspiracy', 'shadow', 'absurd', 'bankruptcy', 'camp', 'consecutive', 'streak', 'favre', 'earth', 'parallel', 'lawn', 'king', 'cleveland', 'cavaliers', 'outdoor', 'saver', 'manufacturer', 'tout', 'operational', 'ease', 'depot', 'introduction', 'soup', 'political', 'motivate', 'mens', 'nielsen', 'australian', 'rightly', 'appealing', 'chart', 'seemingly', 'wood', 'madness', 'regularly', 'challenger', 'gray', 'productivity', 'consultancy', 'productive', 'truth', 'stunt', 'bracket', 'smack', 'lpga', 'hair', 'politically', 'cincinnati', 'uniform', 'retailing', 'coincide', 'eating', 'correlation', 'stat', 'foundation', 'briefly', 'beer', 'consumption', 'overblown', 'arrest', 'journalist', 'quote', 'context', 'annoying', 'immediate', 'correction', 'embarrass', 'cuban', 'commit', 'incoming', 'signing', 'petition', 'circulate', 'silver', 'attendee', 'macworld', 'auto', 'extensively', 'criterion', 'milestone', 'sheer', 'nearby', 'universe', 'venue', 'hazard', 'hallmark', 'unfamiliar', 'breakout', 'detailed', 'gossip', 'lunch', 'endeavor', 'shield', 'takeover', 'echostar', 'directv', 'unwilling', 'bang', 'cnn', 'frustrate', 'booster', 'subsidize', 'habit', 'interrupt', 'sentence', 'manning', 'bcs', 'boise', 'curious', 'tiny', 'investments', 'oracle', 'pour', 'found', 'trajectory', 'salesforce', 'eyebrow', 'clamor', 'conflict', 'misstep', 'penalize', 'back', 'neat', 'oracles', 'regulator', 'extent', 'transparency', 'exercise', 'involvement', 'kinda', 'tread', 'lightly', 'wallet', 'payday', 'immune', 'laboratory', 'vaccination', 'shrink', 'rethink', 'effectiveness', 'recommend', 'hormone', 'therapy', 'response', 'explain', 'passive', 'enhance', 'subsequent', 'sudden', 'embargo', 'inbox', 'interestingly', 'dont', 'carrier', 'protest', 'ink', 'yes', 'comcast', 'precedent', 'geek', 'apart', 'memory', 'supplier', 'samsung', 'rave', 'semiconductor', 'haul', 'harvest', 'viacoms', 'networks', 'nickelodeon', 'trail', 'scatter', 'dauman', 'aim', 'reinvent', 'retention', 'cw', 'fusion', 'loyal', 'friday', 'needless', 'flexible', 'sticky', 'api', 'olympic', 'bike', 'fry', 'roughly', 'plate', 'remotely', 'barely', 'orange', 'overweight', 'constantly', 'fresh', 'dr', 'clarification', 'harm', 'experiment', 'sacrifice', 'donate', 'inject', 'dwarf', 'intern', 'pet', 'northern', 'frequently', 'perception', 'profession', 'welfare', 'medicine', 'don', 'imus', 'inappropriate', 'comfortable', 'comedian', 'presidential', 'republican', 'lineup', 'relation', 'vehicle', 'soften', 'motor', 'ford', 'defy', 'automobile', 'housing', 'dampen', 'automaker', 'revise', 'passenger', 'slide', 'punch', 'sales', 'efficient', 'hybrid', 'embattle', 'import', 'duty', 'lincoln', 'rental', 'fleet', 'chrysler', 'nissan', 'honda', 'reuters', 'crossover', 'familiar', 'collapse', 'recession', 'goldman', 'sachs', 'outlook', 'sentiment', 'liquidity', 'countrywide', 'countrywides', 'common', 'jeopardy', 'missouri', 'west', 'virginia', 'les', 'compile', 'jim', 'tigers', 'chase', 'oklahoma', 'tri', 'tebow', 'dime', 'eligibility', 'airline', 'companies', 'illinois', 'georgia', 'participant', 'adida', 'cnbcs', 'writers', 'guild', 'leg', 'electric', 'rebound', 'olympics', 'grasp', 'sand', 'beginning', 'integration', 'magnitude', 'arrangement', 'warcraft', 'seed', 'simultaneously', 'spur', 'unlock', 'microsofts', 'genentechs', 'breast', 'direction', 'aranesp', 'reimbursement', 'lung', 'brief', 'dna', 'unpredictable', 'disagreement', 'harness', 'gargantuan', 'proportion', 'exclusively', 'napster', 'abide', 'provision', 'streaming', 'helicopter', 'infamous', 'beating', 'illegally', 'infringement', 'imagination', 'documentation', 'saying', 'stink', 'protection', 'negotiating', 'accusation', 'protect', 'liable', 'scenario', 'society', 'clash', 'resolution', 'cultural', 'profitability', 'cooperate', 'rbc', 'capitals', 'explosive', 'valuable', 'viewership', 'opt', 'chronicle', 'sideline', 'legendary', 'federation', 'mainstream', 'butter', 'disappear', 'joke', 'dicks', 'bode', 'feed', 'subscribe', 'managing', 'spy', 'chairwoman', 'plead', 'escape', 'fluid', 'faith', 'justice', 'courthouse', 'linger', 'wi', 'fi', 'app', 'capitalist', 'purport', 'skepticism', 'fake', 'smartphone', 'coordinate', 'dot', 'entrant', 'uphill', 'globally', 'tower', 'headache', 'sprint', 'reading', 'dictate', 'wield', 'showing', 'cloud', 'sophisticated', 'browser', 'embed', 'valleys', 'bud', 'ciscos', 'church', 'conservative', 'haircut', 'height', 'dispatch', 'annuity', 'acc', 'regional', 'smell', 'gorgeous', 'elite', 'outspoken', 'frustrating', 'chat', 'cardiology', 'katrina', 'acronym', 'courage', 'loom', 'showcase', 'squawk', 'metro', 'pulse', 'anecdote', 'suburb', 'overhaul', 'structure', 'labor', 'slate', 'mutual', 'mood', 'sequel', 'shrek', 'confidence', 'plague', 'technical', 'piracy', 'snack', 'dan', 'reform', 'austin', 'annual', 'appropriately', 'mechanism', 'steady', 'impulse', 'third', 'purchasing', 'img', 'extra', 'characterize', 'tenure', 'thompson', 'appoint', 'unacceptable', 'committed', 'restore', 'acceptable', 'heritage', 'allocation', 'demonstrate', 'superior', 'train', 'imminent', 'meltdown', 'higher', 'juggernaut', 'morgan', 'enlist', 'intensify', 'lake', 'blossom', 'difficulty', 'underestimate', 'strategic', 'vendor', 'elevated', 'os', 'trojan', 'handicap', 'please', 'impending', 'skyrocket', 'needham', 'participate', 'devastate', 'briefing', 'forth', 'connecticut', 'bull', 'boomer', 'century', 'vonage', 'shrug', 'tonight', 'georgetown', 'chris', 'juice', 'foul', 'wane', 'gambler', 'got', 'incident', 'cake', 'spill', 'duck', 'sooner', 'deadline', 'disappointed', 'integrity', 'sanction', 'violation', 'physician', 'ethic', 'imply', 'innovative', 'rampant', 'storage', 'geographic', 'guru', 'complaint', 'external', 'sink', 'amazons', 'necessary', 'pipe', 'instal', 'arrival', 'announcer', 'daily', 'fraction', 'chinas', 'shaquille', 'swedish', 'monkey', 'brain', 'shark', 'greatly', 'dining', 'deputy', 'peoples', 'swim', 'highlight', 'wander', 'display', 'recreate', 'crop', 'excellent', 'farm', 'keen', 'ranch', 'caffeine', 'calorie', 'taste', 'salt', 'daughter', 'divorce', 'tab', 'enterprises', 'memorial', 'heighten', 'planning', 'vermont', 'minor', 'colts', 'swear', 'newsday', 'sneaker', 'airways', 'peyton', 'compromise', 'reasonably', 'tide', 'bore', 'testimony', 'webcast', 'frustrated', 'harsh', 'devastating', 'inquiry', 'watcher', 'herald', 'distraction', 'cosmetic', 'structural', 'payment', 'departure', 'vast', 'scheme', 'payoff', 'hewlett', 'packard', 'communicate', 'quietly', 'lieutenant', 'leap', 'justify', 'ice', 'monopoly', 'antitrust', 'coalition', 'promotional', 'likeness', 'bidder', 'rogue', 'woods', 'masters', 'accenture', 'buddy', 'giveaway', 'relevant', 'gillette', 'outrageous', 'flip', 'glove', 'extension', 'mint', 'dice', 'patch', 'frontier', 'columbus', 'reliant', 'qvc', 'coast', 'africa', 'sonys', 'buying', 'spree', 'muscle', 'junk', 'radar', 'march', 'nose', 'cablevision', 'burn', 'james', 'fault', 'signs', 'luck', 'weakness', 'mason', 'inception', 'define', 'setting', 'heavyweight', 'unlimited', 'barrier', 'bread', 'ecosystem', 'realnetworks', 'portable', 'stores', 'reception', 'determined', 'unclear', 'tendency', 'minimal', 'appalachian', 'partly', 'payout', 'deduct', 'sad', 'feasible', 'cripple', 'redesign', 'ideal', 'casual', 'slowdown', 'tip', 'grain', 'anonymous', 'pfe', 'district', 'pink', 'restructuring', 'loop', 'gms', 'pleasant', 'strip', 'stocks', 'bmw', 'pullback', 'efficiency', 'diesel', 'hydrogen', 'mortgage', 'lexus', 'automotive', 'cadillac', 'chevrolet', 'pontiac', 'saturn', 'european', 'saab', 'economic', 'uncertainty', 'macroeconomic', 'generally', 'forecasting', 'sedan', 'lance', 'transcript', 'darren', 'yellow', 'fundraising', 'marathon', 'proceed', 'soul', 'protective', 'starbucks', 'goods', 'cycling', 'participation', 'fishing', 'trek', 'frs', 'radical', 'armstrong', 'devise', 'exceptionally', 'diversification', 'upbeat', 'shares', 'sharply', 'accumulate', 'observation', 'wonderful', 'erase', 'foreseeable', 'novartis', 'recipient', 'redeem', 'designate', 'forum', 'conquer', 'survive', 'emerge', 'innovator', 'instantly', 'affluent', 'overlap', 'whopping', 'counterpart', 'membership', 'bungie', 'studios', 'rub', 'retain', 'refusal', 'wholesale', 'ignite', 'burner', 'remarkably', 'complement', 'curve', 'striking', 'streets', 'attorneys', 'brooklyn', 'subpoena', 'structured', 'strategies', 'enhanced', 'worsen', 'thomas', 'indiana', 'sexual', 'harassment', 'clever', 'float', 'compound', 'covet', 'anchor', 'labs', 'greatest', 'gop', 'mayor', 'typical', 'abortion', 'polling', 'contender', 'romney', 'mccain', 'evenly', 'defeat', 'democratic', 'nets', 'naming', 'barclays', 'generous', 'experimental', 'enrollment', 'pende', 'arrange', 'alert', 'blackrock', 'praise', 'blackstone', 'woo', 'down', 'collateralized', 'headwind', 'intuitive', 'differ', 'voicemail', 'entitle', 'slight', 'nano', 'boast', 'taking', 'disc', 'transformers', 'seinfeld', 'encounter', 'yield', 'barrel', 'trader', 'turning', 'canadas', 'artery', 'exporter', 'fundamentally', 'pan', 'ministry', 'saudis', 'minister', 'opec', 'reap', 'tanker', 'countrys', 'banks', 'allergy', 'hq', 'fade', 'criticism', 'reinforce', 'occasion', 'drove', 'smash', 'absent', 'verge', 'recycle', 'recycling', 'nascent', 'contrarian', 'nowadays', 'packers', 'resurgence', 'addiction', 'europes', 'austrian', 'petroleum', 'prepared', 'fiercely', 'denounce', 'hostile', 'strategically', 'abolish', 'impose', 'effectively', 'eu', 'iranian', 'irans', 'undermine', 'iran', 'nuclear', 'holding', 'authorize', 'mitigate', 'contentious', 'mere', 'buoy', 'systemic', 'comprehensive', 'whack', 'align', 'engage', 'tradition', 'consume', 'output', 'quota', 'tension', 'weapon', 'darn', 'nbcs', 'correspondent', 'insufficient', 'gardasil', 'cervical', 'distinction', 'neutral', 'writing', 'expiration', 'manner', 'statute', 'limitation', 'developers', 'catastrophic', 'marginal', 'assurance', 'technological', 'benchmark', 'joy', 'texting', 'text', 'messaging', 'recruit', 'council', 'proposal', 'thumb', 'flock', 'retro', 'iii', 'spiral', 'sportsonesource', 'milwaukee', 'landing', 'academic', 'civil', 'engineering', 'mechanic', 'wisconsin', 'cold', 'freeze', 'frozen', 'doubleclick', 'scrutiny', 'merit', 'obsession', 'products', 'tube', 'bsx', 'jpm', 'bare', 'prone', 'medtronic', 'alleged', 'blackout', 'cry', 'outage', 'annoy', 'appreciate', 'wind', 'shake', 'resonate', 'disruption', 'credibility', 'materially', 'grid', 'dispute', 'exemption', 'lengthy', 'fierce', 'surf', 'nod', 'wound', 'stall', 'materialize', 'frustration', 'trough', 'fully', 'deeper', 'mute', 'alleviate', 'solution', 'convention', 'shortly', 'innocent', 'honestly', 'attitude', 'pond', 'ernst', 'biotechnology', 'consistently', 'notable', 'bullet', 'activist', 'entertain', 'wyeth', 'trigger', 'domino', 'topps', 'dinner', 'pundit', 'crystal', 'swinge', 'aggressive', 'gather', 'lure', 'scoop', 'sex', 'fort', 'pit', 'veteran', 'traveler', 'dirty', 'sexy', 'questionable', 'describe', 'resume', 'skeptical', 'zombie', 'ex', 'cop', 'anatomy', 'hpv', 'lightning', 'mandate', 'sexually', 'cervarix', 'strain', 'contribution', 'killing', 'facebook', 'banner', 'indirectly', 'affiliation', 'redstone', 'solutions', 'dc', 'narrowly', 'painful', 'heir', 'bleed', 'murdoch', 'inherit', 'believer', 'withhold', 'dual', 'contain', 'debacle', 'pig', 'coke', 'informed', 'vitamin', 'environmental', 'sustainability', 'eco', 'green', 'ambassador', 'pandemic', 'bird', 'considerable', 'seasonal', 'resistance', 'shore', 'inhalable', 'sock', 'confront', 'indefinitely', 'bust', 'imcl', 'bmy', 'mend', 'roller', 'mislead', 'persistent', 'apartment', 'breathe', 'flurry', 'breath', 'booking', 'pad', 'pleased', 'peg', 'rent', 'eligible', 'discretionary', 'restrict', 'sustained', 'newark', 'ledger', 'button', 'electronics', 'ces', 'pause', 'absorb', 'wine', 'drift', 'evolve', 'disk', 'microsystems', 'audio', 'hdtv', 'shapiro', 'planet', 'vacuum', 'ibm', 'cisco', 'plasma', 'interface', 'speaker', 'wirelessly', 'transmit', 'tire', 'led', 'refresh', 'lcd', 'movies', 'shall', 'sleek', 'freedom', 'info', 'minded', 'mountain', 'india', 'beatles', 'monetary', 'sweeping', 'biggest', 'buffet', 'suisse', 'specify', 'newsstand', 'cds', 'brake', 'tunnel', 'guilt', 'insulin', 'paris', 'champagne', 'blend', 'naturally', 'tongue', 'reaffirm', 'boat', 'gene', 'munster', 'milk', 'inaccurate', 'marry', 'massively', 'false', 'vet', 'task', 'talented', 'santa', 'monica', 'irony', 'abundant', 'solar', 'instruments', 'scrap', 'hike', 'lock', 'drum', 'garbage', 'ti', 'trash', 'environmentalist', 'epa', 'toxic', 'chemical', 'fedex', 'slowly', 'guide', 'utilize', 'storm', 'consumers', 'factory', 'centers', 'advisors', 'blogging', 'pm', 'shuffle', 'satisfaction', 'gig', 'starting', 'preview', 'thrill', 'snapshot', 'desktop', 'playback', 'scroll', 'historically', 'slideshow', 'battery', 'bundle', 'custom', 'normally', 'statistic', 'applause', 'msnbc', 'insurer', 'cocktail', 'remedy', 'tablet', 'following', 'unofficial', 'connectivity', 'input', 'relief', 'forthcoming', 'enlyten', 'buffalo', 'bills', 'scientist', 'credential', 'liquid', 'bypass', 'differently', 'shocked', 'substance', 'neiman', 'marcus', 'robot', 'catalog', 'spokeswoman', 'tall', 'tree', 'tent', 'diamond', 'impress', 'concert', 'hometown', 'modest', 'montana', 'chocolate', 'pledge', 'bulb', 'bulk', 'fourth', 'rig', 'sea', 'entity', 'estimates', 'midst', 'adams', 'execute', 'unite', 'ap', 'admission', 'fanboy', 'angry', 'burgeon', 'nimble', 'sum', 'observer', 'reorganization', 'outsource', 'compensate', 'drastic', 'languish', 'footing', 'rhetoric', 'wga', 'producers', 'formula', 'amptp', 'script', 'stockpile', 'cram', 'renewal', 'writedown', 'citigroups', 'loan', 'sir', 'whitney', 'citis', 'vikram', 'pandit', 'nyse', 'thain', 'emergency', 'hammer', 'tokyo', 'performer', 'industrial', 'fed', 'tuition', 'negotiator', 'collar', 'irish', 'buyout', 'louisville', 'navy', 'unexpectedly', 'obama', 'waiting', 'medication', 'tactic', 'persuade', 'rebate', 'prominent', 'primarily', 'miller', 'jewel', 'precisely', 'notably', 'generals', 'objective', 'hurd', 'accurately', 'description', 'digress', 'strict', 'finger', 'sample', 'convincing', 'chatter', 'median', 'chalk', 'quantify', 'penetration', 'prudent', 'interpretation', 'strictly', 'enforce', 'mature', 'nationwide', 'davis', 'wpp', 'sanford', 'viable', 'crests', 'strategist', 'telegraph', 'reorganize', 'assert', 'dominant', 'historic', 'persist', 'pattern', 'what', 'syndication', 'opposition', 'unrealistic', 'jurisdiction', 'pact', 'alltel', 'govern', 'render', 'tpg', 'gs', 'gallery', 'pack', 'unfortunately', 'shame', 'tiffany', 'avandia', 'stakeholder', 'lawmaker', 'incremental', 'dependent', 'diabetic', 'allegedly', 'moderate', 'skin', 'advocacy', 'uncommon', 'portland', 'lottery', 'educational', 'pant', 'balk', 'accomplish', 'depressed', 'editorial', 'premature', 'panic', 'calm', 'questions', 'last', 'voluntarily', 'breakup', 'precipitous', 'terminate', 'uncomfortable', 'apiece', 'indias', 'ag', 'lucas', 'spawn', 'birth', 'worlds', 'un', 'collection', 'memorabilia', 'spate', 'mantra', 'guideline', 'ftc', 'compose', 'lining', 'sway', 'rapidly', 'startup', 'prospectus', 'fiduciary', 'resignation', 'spelling', 'bee', 'speller', 'eventual', 'runner', 'renegotiate', 'nashville', 'mom', 'interact', 'overseas', 'capitol', 'wheel', 'basket', 'broadcasting', 'offline', 'photograph', 'bus', 'marriage', 'antonio', 'spurs', 'wedding', 'reimburse', 'footage', 'childhood', 'eve', 'expo', 'warranty', 'severity', 'doom', 'gloom', 'elephant', 'worse', 'traditionally', 'mire', 'intensity', 'recess', 'notch', 'backlash', 'pepsi', 'wing', 'cowen', 'someday', 'sunday', 'assign', 'withdraw', 'thorough', 'ultra', 'terminal', 'grocery', 'out', 'mystery', 'quirky', 'irs', 'alien', 'intriguing', 'hack', 'weave', 'anonymity', 'privilege', 'trace', 'curtain', 'imac', 'brush', 'border', 'practically', 'bryant', 'prison', 'clarity', 'mike', 'falcons', 'firework', 'ballpark', 'mascot', 'carbonate', 'bearish', 'sampling', 'communications', 'constitute', 'overwhelmingly', 'finalize', 'evolution', 'friendly', 'unleash', 'unveiling', 'qualcomm', 'android', 'globe', 'scoff', 'chop', 'sigh', 'flag', 'tweak', 'furniture', 'contemporary', 'array', 'flower', 'deteriorate', 'goldmans', 'conviction', 'windfall', 'apply', 'suv', 'gallon', 'aaa', 'tick', 'heating', 'incidentally', 'manufacture', 'macbook', 'flash', 'prosper', 'dykstra', 'conjunction', 'mover', 'overtime', 'mercedes', 'benz', 'aig', 'naysayer', 'divert', 'amazingly', 'lofty', 'junior', 'alcohol', 'anheuser', 'busch', 'bucket', 'loyalty', 'outweigh', 'yard', 'pepper', 'unidentified', 'planner', 'ebays', 'checkout', 'paypal', 'tea', 'warfare', 'ramification', 'silly', 'murky', 'brew', 'colorful', 'subside', 'cooperative', 'hub', 'deed', 'execution', 'balanced', 'dialogue', 'genre', 'cellular', 'iraq', 'tourist', 'telecom', 'theyre', 'swap', 'oppenheimer', 'domestically', 'derive', 'partial', 'cupertino', 'disastrous', 'critique', 'treo', 'flaw', 'mill', 'institutional', 'smooth', 'succession', 'vibrant', 'outsider', 'bless', 'personality', 'el', 'angel', 'par', 'african', 'front', 'towel', 'deposit', 'gut', 'virtue', 'unanimously', 'proclaim', 'drain', 'roof', 'foster', 'perk', 'breakfast', 'remind', 'dominance', 'withstand', 'targeted', 'neighborhood', 'oakland', 'accident', 'pilot', 'appreciation', 'presume', 'explosion', 'fbi', 'vow', 'misleading', 'slew', 'bathroom', 'prevention', 'wardrobe', 'halftime', 'offend', 'smile', 'paul', 'belt', 'cult', 'dealership', 'fend', 'brazil', 'ireland', 'italy', 'spain', 'sensitive', 'filter', 'accuse', 'prius', 'modify', 'electricity', 'mileage', 'philanthropic', 'zone', 'churn', 'funding', 'foresee', 'loophole', 'belief', 'lone', 'buys', 'supplement', 'utter', 'irresponsible', 'demise', 'portray', 'differentiate', 'mississippi', 'regulate', 'convey', 'violate', 'intent', 'reckless', 'congressional', 'dealing', 'confused', 'threshold', 'burden', 'comic', 'swimmer', 'longevity', 'pool', 'children', 'ana', 'variable', 'craze', 'obsess', 'consequence', 'drunk', 'ripple', 'darling', 'wsj', 'regret', 'dilute', 'faber', 'rattle', 'shine', 'bizarre', 'stranger', 'hunt', 'inherent', 'deploy', 'excess', 'commitment', 'dvr', 'measurement', 'correlate', 'cancellation', 'bow', 'moore', 'screening', 'aisle', 'cuba', 'swipe', 'congressman', 'indictment', 'grip', 'lobbyist', 'orchestrate', 'democrats', 'responsive', 'politics', 'sneak', 'cliff', 'anxious', 'transfer', 'defect', 'gun', 'efficiently', 'saddle', 'bandwidth', 'desk', 'rein', 'restraint', 'convenience', 'tailor', 'nonsense', 'equation', 'lessen', 'licensed', 'safely', 'greet', 'grind', 'craft', 'liberal', 'stellar', 'cnet', 'subtle', 'incorporate', 'myspaces', 'teeter', 'max', 'texte', 'spoof', 'diminish', 'frank', 'wager', 'protein', 'span', 'newly', 'doll', 'dome', 'commerce', 'jaw', 'pizza', 'injure', 'alarm', 'destroy', 'ada', 'soak', 'hawk', 'scared', 'settlement', 'collectible', 'homework', 'urban', 'silence', 'bankrupt', 'purely', 'musician', 'hardcore', 'dislike', 'running', 'marvel', 'vastly', 'obscure', 'banker', 'unstable', 'risky', 'diverse', 'investors', 'lady', 'bully', 'unexpected', 'adviser', 'knight', 'multiplayer', 'attraction', 'else', 'greek', 'freely', 'cardiologist', 'clinic', 'assist', 'spice', 'rat', 'visual', 'dimension', 'rebuild', 'famously', 'amendment', 'landmark', 'rap', 'qualify', 'loose', 'stare', 'constitution', 'dilemma', 'react', 'county', 'louis', 'existence', 'mentality', 'volatile', 'transformer', 'harry', 'potter', 'cars', 'devoted', 'northwestern', 'ubisoft', 'endless', 'sync', 'migrate', 'eclipse', 'redefine', 'spirit', 'mode', 'unethical', 'bargaining', 'czar', 'overpay', 'partially', 'offset', 'uncertain', 'machinery', 'allocate', 'adoption', 'bulls', 'celtics', 'representation', 'entrepreneurial', 'boss', 'panasonic', 'can', 'batter', 'sportsbiz', 'preferred', 'galaxy', 'unknown', 'upcome', 'satisfy', 'lehman', 'hasbro', 'feeling', 'perceive', 'forgo', 'affair', 'jail', 'dealer', 'suspicion', 'cantor', 'bronx', 'winning', 'economics', 'novelty', 'ny', 'terribly', 'mexican', 'rockstar', 'wrestle', 'collateral', 'likelihood', 'withdrawal', 'regain', 'lost', 'assess', 'lly', 'ivanovic', 'mineral', 'pat', 'alzheimers', 'jackson', 'stiff', 'netflix', 'postal', 'inaugural', 'extensive', 'motivation', 'egyptian', 'llc', 'cure', 'expression', 'hum', 'toronto', 'acceptance', 'spender', 'hopeful', 'barcelona', 'anticipated', 'amds', 'quad', 'keynote', 'lego', 'trailer', 'foremost', 'propel', 'cannibalize', 'gb', 'worst', 'charitable', 'pleasure', 'constant', 'acceleration', 'suck', 'vague', 'gulf', 'hefty', 'kuwait', 'alter', 'spray', 'printing', 'newcomer', 'martin', 'fragile', 'vc', 'disruptive', 'visa', 'mastercard', 'cartel', 'dems', 'predictable', 'commend', 'picket', 'iconic', 'theatrical', 'innovate', 'ventures', 'broaden', 'biofuel', 'coal', 'transportation', 'nanosolar', 'emission', 'pollution', 'cement', 'farmer', 'carbon', 'climate', 'appliance', 'minnesota', 'vikings', 'browns', 'raiders', 'offseason', 'gaap', 'chamber', 'australia', 'tentative', 'reconcile', 'boxing', 'undefeated', 'matchup', 'purse', 'boxer', 'exacerbate', 'tabak', 'restrictive', 'hesitant', 'nj', 'lap', 'lavish', 'meat', 'inc', 'skeptic', 'greenwich', 'supervisor', 'tong', 'swiftly', 'arbitration', 'budweiser', 'barometer', 'steam', 'intels', 'funds', 'uptick', 'inexpensive', 'david', 'mls', 'glad', 'coup', 'ticker', 'nationally', 'olympian', 'stringent', 'village', 'senators', 'representatives', 'plain', 'reflection', 'john', 'bernanke', 'cutting', 'harbinger', 'poker', 'legislation', 'compel', 'tragic', 'comscore', 'fabric', 'eagerly', 'bloomberg', 'breaking', 'forest', 'zoom', 'warehouse', 'merchandising', 'penalty', 'clause', 'kraft', 'zetia', 'chantix', 'exubera', 'parade', 'glance', 'shortfall', 'germany', 'afterward', 'culprit', 'hsbc', 'dependence', 'legally', 'tween', 'singer', 'minneapolis', 'jp', 'disturbing', 'related', 'brainer', 'sweater', 'boundary', 'creep', 'hacker', 'method', 'vicks', 'deem', 'mexico', 'skype', 'intention', 'untapped', 'governance', 'assemble', 'accountability', 'helm', 'cede', 'standoff', 'barnes', 'noble', 'ceiling', 'slim', 'committees', 'corrupt', 'temptation', 'cheat', 'dear', 'independently', 'reinstate', 'lockout', 'contend', 'fantastic', 'broadway', 'vagary', 'idc', 'gartner', 'oral', 'kliff', 'skew', 'backyard', 'drinking', 'underscore', 'dj', 'hilton', 'arise', 'abrupt', 'trustee', 'symbol', 'consideration', 'comfort', 'escalate', 'suspicious', 'balloon', 'chaos', 'zell', 'borrow', 'widen', 'ed', 'envelope', 'muster', 'donation', 'entrance', 'iv', 'flawed', 'variation', 'analyze', 'determination', 'fluctuation', 'trump', 'euphoria', 'troubling', 'facebooks', 'paltry', 'discovery', 'pervasive', 'legislator', 'obesity', 'cartoon', 'wisely', 'rider', 'mold', 'youtubes', 'amend', 'harvard', 'twin', 'dismiss', 'summers', 'princeton', 'medal', 'cerberus', 'lien', 'default', 'woe', 'recapitalize', 'refinance', 'reversal', 'dodge', 'leveraged', 'inability', 'downdraft', 'wipe', 'pinch', 'accordingly', 'enormously', 'ratchet', 'convict', 'buckle', 'envy', 'lame', 'revive', 'domain', 'contingent', 'cautiously', 'burger', 'congratulation', 'cue', 'blade', 'relay', 'staffer', 'hotly', 'lobbying', 'analog', 'incumbent', 'adopt', 'worthless', 'arabia', 'stifle', 'tame', 'simon', 'gush', 'moonves', 'moonve', 'messy', 'virgin', 'territory', 'sandwich', 'conversion', 'poverty', 'microphone', 'properly', 'antenna', 'rf', 'resort', 'pumpkin', 'evident', 'lighting', 'loosen', 'occupancy', 'blu', 'inspiration', 'rope', 'lg', 'clock', 'establishment', 'instinct', 'spains', 'slope', 'earner', 'multimedia', 'indian', 'administrator', 'medicaid', 'governor', 'environmentally', 'realty', 'condo', 'fidelity', 'inclined', 'dash', 'receiver', 'fragrance', 'badge', 'costume', 'mortar', 'defraud', 'seismic', 'construct', 'secs', 'com', 'prosecution', 'toss', 'rotate', 'evaluation', 'dedicated', 'violent', 'swath', 'nintendos', 'pale', 'visibility', 'exclusivity', 'steer', 'complicated', 'bettor', 'comb', 'outright', 'coin', 'journals', 'porges', 'sellout', 'espns', 'undoubtedly', 'ticketmaster', 'ticketing', 'homepage', 'passionate', 'fickle', 'tim', 'op', 'timely', 'tangible', 'achievement', 'hugely', 'indict', 'termination', 'detrimental', 'vmware', 'infrastructure', 'cart', 'obese', 'lazard', 'changer', 'longstanding', 'blowout', 'weary', 'echo', 'warming', 'conserve', 'syndicate', 'underwriter', 'poorly', 'conan', 'cafe', 'contributor', 'downsize', 'exploit', 'transparent', 'uncover', 'edit', 'military', 'nicely', 'credible', 'finale', 'rights', 'acute', 'urgent', 'donor', 'considerably', 'stimulate', 'burdensome', 'complicate', 'procedure', 'bleak', 'realtor', 'unload', 'brisk', 'bug', 'lender', 'haven', 'plea', 'unrelated', 'noticeable', 'encouraging', 'backup', 'promoter', 'fringe', 'whistle', 'bumpy', 'atom', 'illness', 'progressive', 'oregon', 'dakota', 'boone', 'pickens', 'timetable', 'grim', 'repay', 'fdic', 'foreclosure', 'bair', 'temporarily', 'thrift', 'lending', 'adjustment', 'envision', 'largest', 'apollo', 'exceptional', 'revisit', 'preference', 'administrative', 'falter', 'overvalue', 'belong', 'kkr', 'appetite', 'minds', 'irrational', 'denial', 'demonstration', 'characteristic', 'swimsuit', 'convertible', 'moodys', 'homeowner', 'borrowing', 'curtail', 'recessionary', 'northeast', 'dept', 'commonly', 'gym', 'durable', 'bend', 'backing', 'sauce', 'remote', 'backer', 'vintage', 'blower', 'pennsylvania', 'chrome', 'garage', 'bedroom', 'undertake', 'illiquid', 'skittish', 'fifth', 'instrument', 'underwriting', 'standards', 'proprietary', 'routinely', 'challenging', 'poors', 'winter', 'abroad', 'switzerland', 'basel', 'jewelry', 'statistical', 'supermarket', 'prestigious', 'rapid', 'upward', 'remainder', 'securitization', 'diversity', 'carlyle', 'installment', 'collectively', 'borrower', 'browse', 'insert', 'zip', 'freshman', 'renaissance', 'overwhelm', 'candy', 'whip', 'lapse', 'drown', 'lifeline', 'iowa', 'consent', 'hangover', 'spouse', 'plc', 'iraqi', 'collaborate', 'hulu', 'prominently', 'premier', 'invitation', 'moratorium', 'submission', 'probe', 'cumulative', 'ltd', 'emit', 'conservation', 'sierra', 'viability', 'mobil', 'runaway', 'dioxide', 'protocol', 'rivalry', 'stubhub', 'jeter', 'rough', 'severance', 'dismal', 'peek', 'secrecy', 'processor', 'comprise', 'hurricane', 'port', 'brokers', 'nar', 'affordability', 'speculator', 'citizens', 'southeast', 'intervention', 'backstop', 'wildcard', 'htc', 'slick', 'keyboard', 'compact', 'nasa', 'shuttle', 'fitch', 'cyclical', 'neighbor', 'rage', 'fiction', 'avatar', 'uks', 'desirable', 'enthusiast', 'vertical', 'watchdog', 'drexler', 'enforcement', 'commissions', 'shortcoming', 'oversight', 'reset', 'flyer', 'comptroller', 'yawn', 'cave', 'regime', 'recoup', 'rejection', 'underground', 'intentionally', 'vip', 'reservation', 'trio', 'mansion', 'testament', 'feat', 'comeback', 'thunder', 'princess', 'fraudulent', 'accountable', 'deceptive', 'illusion', 'counterpartie', 'diligence', 'distort', 'competitiveness', 'adverse', 'tivos', 'transport', 'strengthen', 'prospective', 'safeguard', 'query', 'qualified', 'confine', 'pessimism', 'beleaguered', 'breakdown', 'indians', 'glimpse', 'contemplate', 'unemployed', 'carnegie', 'universitys', 'highway', 'stanfords', 'sensor', 'spectacle', 'inefficient', 'interaction', 'mitchell', 'flooding', 'msg', 'barclay', 'advisers', 'midterm', 'contraction', 'apthe', 'atm', 'locate', 'wells', 'surcharge', 'convenient', 'fargo', 'edgy', 'compute', 'hyundai', 'mild', 'basement', 'airplane', 'elaborate', 'summary', 'judgment', 'netscape', 'armed', 'certificate', 'homebuilder', 'unsold', 'weve', 'boring', 'stem', 'levy', 'abuse', 'departments', 'unfairly', 'extraordinarily', 'scoreboard', 'tighten', 'opecs', 'abu', 'dhabi', 'bp', 'residence', 'amp', 'nrf', 'worrisome', 'yearly', 'injection', 'significance', 'handling', 'numbers', 'ward', 'stigma', 'bail', 'witch', 'reserves', 'coaches', 'overshadow', 'unpaid', 'sympathy', 'exaggerate', 'in', 'reconsider', 'jets', 'clog', 'workers', 'uaw', 'assembly', 'retiree', 'offense', 'coordinator', 'jay', 'longshot', 'creditor', 'funnel', 'broncos', 'uninsure', 'barack', 'subsidy', 'exempt', 'senate', 'repeal', 'veto', 'tricky', 'existing', 'yale', 'cease', 'hardest', 'hamper', 'businessweek', 'omission', 'internationally', 'fuzzy', 'reassure', 'chronic', 'deregulation', 'sizable', 'uninsured', 'occupy', 'rolling', 'realtors', 'downward', 'homes', 'egg', 'imbalance', 'playbook', 'turbine', 'whitacre', 'foreigner', 'sticker', 'functionality', 'fortunately', 'manageable', 'bias', 'cio', 'principle', 'steadily', 'calif', 'greed', 'netflixs', 'illustrate', 'vienna', 'iea', 'refined', 'afp', 'indonesia', 'arab', 'skilled', 'refinery', 'nigeria', 'delegate', 'insistence', 'regulated', 'deepen', 'korea', 'predatory', 'undercut', 'biovail', 'feds', 'rods', 'assertion', 'realistically', 'berlin', 'rodriguez', 'multiply', 'jacket', 'negatively', 'childrens', 'abercrombie', 'saks', 'discounter', 'spendingpulse', 'resilient', 'convinced', 'lane', 'prosperity', 'fannie', 'mae', 'freddie', 'builder', 'blanket', 'builders', 'ultimatum', 'collegiate', 'berkeley', 'actors', 'derail', 'bofa', 'depress', 'avastin', 'wet', 'raising', 'stave', 'hunter', 'tenant', 'unsustainable', 'adequate', 'phoenix', 'reed', 'adjustable', 'nevada', 'metropolitan', 'distressed', 'renovation', 'speculative', 'rural', 'curb', 'nobel', 'landlord', 'reit', 'east', 'weakening', 'probability', 'shallow', 'sharga', 'repossession', 'foreclose', 'inspect', 'michaels', 'grossing', 'nationals', 'bookmaker', 'silva', 'devalue', 'unchanged', 'handily', 'breakthrough', 'mrsa', 'infect', 'daunting', 'inadequate', 'decade', 'transmission', 'accustomed', 'vanish', 'cent', 'alternate', 'fairness', 'treasurer', 'orderly', 'malicious', 'virus', 'adobe', 'slice', 'continent', 'erupt', 'classify', 'prostitution', 'liquidate', 'intact', 'carmaker', 'sap', 'converse', 'philosophy', 'inclusion', 'openness', 'fiscal', 'hail', 'usage', 'hindsight', 'desert', 'inflow', 'euro', 'denominate', 'bpd', 'distillate', 'kurdish', 'iraqs', 'export', 'fortunate', 'backfire', 'excellence', 'sympathetic', 'bat', 'spook', 'sc', 'breadth', 'cord', 'taco', 'redemption', 'tepid', 'relocate', 'firefighter', 'fifa', 'simplify', 'gal', 'completion', 'administrations', 'louisiana', 'ups', 'stifel', 'nicolaus', 'pac', 'brave', 'showdown', 'roundtable', 'merchant', 'hunting', 'affinity', 'useless', 'ads', 'parity', 'lakers', 'urgency', 'manipulate', 'enrich', 'ericsson', 'broadly', 'nomura', 'glut', 'elimination', 'chevy', 'republic', 'mario', 'genuine', 'noteworthy', 'kudlow', 'cheney', 'vulnerability', 'mr', 'missile', 'hamas', 'topple', 'uranium', 'adapt', 'appropriation', 'overrun', 'pork', 'troop', 'necessity', 'ways', 'means', 'deficit', 'gdp', 'inequality', 'anxiety', 'destruction', 'immigrant', 'fearful', 'homebuyer', 'merrills', 'candidacy', 'sunlight', 'jack', 'penn', 'breach', 'unleaded', 'refiner', 'nerve', 'fossil', 'photovoltaic', 'copper', 'substantially', 'comply', 'broke', 'angels', 'lackluster', 'hispanic', 'rangers', 'outline', 'adversary', 'deterioration', 'abruptly', 'calling', 'aggregate', 'commander', 'hulus', 'biopharma', 'ds', 'purchaser', 'leading', 'deter', 'nut', 'bolt', 'notebook', 'monday', 'stun', 'consortium', 'solicit', 'interfere', 'misuse', 'chaotic', 'internally', 'directors', 'clara', 'akin', 'dissident', 'decisive', 'psp', 'gobble', 'duration', 'nadal', 'knowledgeable', 'scripps', 'israel', 'theft', 'fab', 'norm', 'td', 'testify', 'imagery', 'valve', 'bnp', 'paribas', 'licensee', 'realm', 'bailout', 'dissolve', 'taxpayer', 'nominal', 'soundness', 'depository', 'depositor', 'insolvent', 'fha', 'explicitly', 'da', 'prosecutor', 'policymaker', 'passage', 'budgetary', 'accommodation', 'enact', 'dwindle', 'vie', 'confidential', 'password', 'install', 'aspirational', 'headway', 'damaging', 'revolve', 'off', 'shatter', 'foreman', 'batch', 'phillies', 'asia', 'billing', 'likewise', 'heck', 'britain', 'commerzbank', 'adequately', 'weed', 'void', 'semi', 'walmart', 'custody', 'miracle', 'equip', 'imax', 'videogame', 'activisions', 'savings', 'hong', 'kong', 'ct', 'outreach', 'republicans', 'markdown', 'liquidation', 'thanksgiving', 'prices', 'immense', 'follower', 'usher', 'selective', 'mercantile', 'gmac', 'tourism', 'complexity', 'exert', 'resistant', 'alike', 'drilling', 'cyber', 'delinquency', 'furthermore', 'origination', 'indirect', 'kindle', 'dean', 'aeropostale', 'kohls', 'macys', 'beast', 'apps', 'groundwork', 'newswire', 'omit', 'overstate', 'funeral', 'bankers', 'jumbo', 'mba', 'caucus', 'poison', 'avert', 'issuance', 'improved', 'senates', 'constituent', 'intermediation', 'ripe', 'stars', 'muslims', 'chevron', 'vital', 'moscow', 'meter', 'greece', 'italian', 'coastal', 'citadel', 'fiasco', 'forgive', 'behave', 'shocking', 'carve', 'deftly', 'revenge', 'crumble', 'icahn', 'nortel', 'etf', 'nymex', 'butler', 'afloat', 'appeals', 'newsweek', 'subsidiary', 'terrorism', 'drastically', 'dilutive', 'verify', 'minutes', 'distinct', 'quo', 'unfounded', 'clamp', 'shaky', 'concentrated', 'lecture', 'stability', 'proactive', 'securitize', 'underlying', 'capitalization', 'tysabri', 'banco', 'bureaucracy', 'infinity', 'architecture', 'physically', 'streamline', 'husband', 'exhaust', 'retaliate', 'wyoming', 'transit', 'leerink', 'coffer', 'raiser', 'cameron', 'geothermal', 'underperform', 'birds', 'nest', 'assets', 'hearted', 'happening', 'ineffective', 'imperative', 'appointee', 'george', 'gesture', 'bove', 'pnc', 'tightening', 'blueprint', 'investigative', 'tangle', 'deeply', 'recapitalization', 'underwater', 'keenly', 'symptom', 'economys', 'examiner', 'thermal', 'delaware', 'hampshire', 'giving', 'nexus', 'installation', 'renewable', 'additionally', 'nuance', 'bottom', 'colligan', 'chorus', 'runway', 'sears', 'bloom', 'logic', 'cabinet', 'dust', 'venezuela', 'displace', 'harmful', 'behavioral', 'cellphone', 'individually', 'unsolicited', 'stealth', 'heated', 'singapore', 'derivative', 'toys', 'placeholder', 'britains', 'sovereign', 'ilike', 'aviation', 'shelter', 'telsey', 'helmet', 'ammunition', 'instruction', 'restart', 'predecessor', 'p.m.', 'looking', 'applaud', 'populist', 'barney', 'universals', 'alaska', 'mud', 'fords', 'explorer', 'stimulus', 'ballmer', 'ambac', 'municipal', 'societe', 'generale', 'mbia', 'alibaba', 'similarly', 'moderately', 'platinum', 'clearing', 'supportive', 'plight', 'improperly', 'fray', 'franks', 'embark', 'mine', 'miner', 'mechanical', 'revival', 'counterparty', 'fuld', 'washingtons', 'wooden', 'exxonmobil', 'kilowatt', 'deride', 'drill', 'wasteful', 'hostility', 'destabilize', 'terrorist', 'raid', 'provoke', 'admiral', 'diplomacy', 'convene', 'aftra', 'stoppage', 'implicit', 'secular', 'accompanying', 'automatic', 'cuomos', 'contingency', 'breeding', 'titanic', 'indymac', 'halfway', 'stabilization', 'unwind', 'gimmick', 'issuer', 'governmental', 'embolden', 'rogers', 'cato', 'houses', 'nationalize', 'brusca', 'solvency', 'gridlock', 'waiver', 'contagion', 'nationalization', 'reinvigorate', 'deleverage', 'lloyds', 'bancorp', 'mattel', 'macro', 'chesapeake', 'ounce', 'germanys', 'naked', 'ailing', 'census', 'rams', 'rates', 'aide', 'corruption', 'unhappy', 'sic', 'hart', 'maneuver', 'conceal', 'rail', 'raider', 'pst', 'otellini', 'pencil', 'bruise', 'yesterdays', 'motive', 'impairment', 'electorate', 'uneven', 'ideological', 'immigration', 'jpmorgans', 'downloadable', 'editing', 'scrutinize', 'constraint', 'taiwan', 'memphis', 'lax', 'subcommittee', 'anxiously', 'tremendously', 'appraisal', 'lehmans', 'problematic', 'delinquent', 'rules', 'contractual', 'ross', 'peripheral', 'overhang', 'unforgive', 'pessimistic', 'bipartisan', 'authenticity', 'coupon', 'constrain', 'cushion', 'charm', 'rewrite', 'onerous', 'entail', 'shutter', 'foothold', 'disbursement', 'bristle', 'prisoner', 'secretive', 'surplus', 'architect', 'christian', 'oval', 'con', 'taxation', 'presidency', 'flourish', 'bipartisanship', 'diplomatic', 'vulnerable', 'entrenched', 'horrible', 'entitlement', 'amass', 'unauthorized', 'stool', 'consist', 'citibank', 'bacon', 'blunder', 'symbolic', 'cme', 'supervision', 'postquestions', 'procurement', 'gradually', 'gao', 'vampire', 'brink', 'tee', 'contention', 'falk', 'hospitality', 'faulty', 'unregulated', 'prudential', 'gaza', 'options', 'yen', 'paulson', 'breaker', 'surrender', 'audits', 'surveillance', 'southwest', 'boeing', 'hydrocarbon', 'broadpoint', 'unemployment', 'db', 'interstate', 'leed', 'tesla', 'volt', 'suited', 'nations', 'midway', 'sees', 'obamas', 'biden', 'venezuelas', 'venezuelan', 'libertarian', 'modestly', 'affidavit', 'vivid', 'analytics', 'historian', 'delta', 'comcasts', 'chancellor', 'dealbook', 'undo', 'dinallo', 'restricted', 'revelation', 'pancreatic', 'cater', 'solvent', 'parliament', 'thwart', 'mining', 'justification', 'vessel', 'flesh', 'muni', 'municipality', 'mrs', 'rhode', 'battleground', 'operative', 'layer', 'speedo', 'phelps', 'meredith', 'schoenebaum', 'picken', 'encryption', 'electrical', 'classified', 'homeland', 'river', 'approves', 'combative', 'legislative', 'ms', 'eln', 'alberta', 'transocean', 'realtytrac', 'baidu', 'manipulation', 'influx', 'sustainable', 'bernankes', 'tinker', 'telecommunications', 'tariff', 'leisure', 'bewkes', 'addict', 'polish', 'thesis', 'grateful', 'portugal', 'jobless', 'easter', 'theoretically', 'framework', 'morgans', 'tipping', 'unsecured', 'transplant', 'feather', 'examination', 'unintended', 'locke', 'futures', 'refinancing', 'bartiromo', 'inflationary', 'mitsubishi', 'haunt', 'effient', 'pdcf', 'scam', 'biomass', 'prohibition', 'static', 'reassurance', 'paycheck', 'hardship', 'pa', 'sensitivity', 'obstacle', 'filibuster', 'quantitative', 'treasurys', 'tarp', 'ihs', 'cambridge', 'capex', 'possession', 'denmark', 'macquarie', 'crowded', 'programmer', 'scholar', 'judicial', 'jumpstart', 'spitzer', 'multitude', 'cpi', 'pimco', 'utterly', 'servicing', 'somber', 'patricks', 'cities', 'commentator', 'nonprofit', 'mentor', 'shun', 'unconventional', 'gse', 'disconnect', 'federally', 'pretend', 'hyper', 'colombia', 'vietnam', 'earthquake', 'chambers', 'fisherman', 'slack', 'modification', 'originator', 'twilight', 'reservoir', 'simmon', 'kbw', 'cit', 'lasting', 'vanguard', 'empower', 'posts', 'retailers', 'clinton', 'forces', 'ladder', 'brookings', 'nonpartisan', 'compress', 'utilization', 'civic', 'neutrality', 'augusta', 'sen', 'debit', 'actuary', 'agricultural', 'mcbride', 'cftc', 'stimulu', 'abt', 'objection', 'nonbank', 'constructive', 'foe', 'skilling', 'johns', 'lenos', 'ing', 'blizzard', 'delicate', 'backbone', 'symbian', 'farther', 'seoul', 'entice', 'gang', 'neuberger', 'si', 'soldier', 'decisively', 'lets', 'coors', 'rooftop', 'resorts', 'touts', 'volcker', 'nebraska', 'maine', 'mccains', 'alpha', 'inauguration', 'centerpiece', 'schwartz', 'pemex', 'rollout', 'fios', 'buffer', 'inspector', 'pave', 'implode', 'impede', 'spam', 'backlog', 'weakened', 'cautionary', 'multinational', 'tsunami', 'imperfect', 'targets', 'ply', 'quadrangle', 'bc', 'scarce', 'debtor', 'sluggish', 'porsche', 'emergence', 'ecb', 'arbitrary', 'bra', 'reopen', 'legislature', 'gamestop', 'prostitute', 'shutdown', 'iq', 'disapprove', 'flop', 'vacancy', 'kleiner', 'perkins', 'quillia', 'supervisory', 'cocaine', 'wagoner', 'shale', 'agriculture', 'brewer', 'eds', 'embryonic', 'hallway', 'straightforward', 'israeli', 'videotape', 'icahns', 'zealand', 'tender', 'analytic', 'openly', 'silent', 'artificially', 'bps', 'korean', 'handbag', 'elusive', 'cleanup', 'pollute', 'reactor', 'reckoning', 'grave', 'neglect', 'firefox', 'civilian', 'bracelet', 'santander', 'deloitte', 'unpopular', 'sweden', 'jolt', 'autonation', 'marijuana', 'inaugurate', 'idle', 'centrist', 'emerging', 'mellon', 'stoke', 'wiretap', 'commodities', 'splurge', 'thorny', 'bondholder', 'm', 'summon', 'weeklong', 'extract', 'aigs', 'shooter', 'broken', 'deepwater', 'marine', 'linkedin', 'narrative', 'formidable', 'riser', 'amateur', 'twitter', 'shiller', 'sterling', 'easing', 'fomc', 'outbreak', 'postbank', 'dismantle', 'catastrophe', 'netbook', 'freefall', 'mo', 'unforeseen', 'rattner', 'memorandum', 'gravitate', 'fhfa', 'conservatorship', 'mbs', 'subordinated', 'maturity', 'explicit', 'imf', 'conn', 'liken', 'prepackage', 'atari', 'spoiled', 'legalize', 'discretion', 'mer', 'elevate', 'arbitrage', 'mkm', 'optic', 'cuts', 'bancshares', 'deflation', 'kiosk', 'redbox', 'droid', 'graduation', 'cuomo', 'failures', 'clout', 'pew', 'sander', 'henderson', 'repayment', 'ping', 'leno', 'dreadful', 'infomercial', 'bigresearch', 'annualize', 'dashboard', 'fao', 'frugal', 'adobes', 'islamic', 'forefront', 'erian', 'vix', 'reshaping', 'bankshares', 'del', 'treaty', 'psl', 'alice', 'tron', 'beemer', 'westinghouse', 'servicer', 'clawback', 'ballooning', 'nationwidestruggling', 'hawks', 'harshly', 'unions', 'tightrope', 'deflationary', 'quotes', 'geithner', 'dove', 'impeachment', 'madoff', 'legged', 'ideologically', 'burnett', 'madoffs', 'sb', 'shameful', 'slideshows', 'mhuckman', 'swine', 'reinvestment', 'haiti', 'natal', 'unannounced', 'tweet', 'juggle', 'wedbush', 'epix', 'oppressive', 'tweets', 'reconstruct', 'cleanse', 'portuguese', 'containment', 'nadel', 'shamwow', 'copenhagen', 'underpayment', 'palestinian', 'jordanian', 'boldly', 'policies', 'bartz', 'bottler', 'olive', 'bonuses', 'wounded', 'itemized', 'clearthe', 'goals', 'inducement', 'changes', 'ofobamas', 'arrear', 'irresolvable', 'reached', 'laid', 'continues', 'imposes', 'withdrawals', 'freewheel', 'meetingonline', 'totter', 'overarching', 'stocksespecially', 'firmsrising', 'dsi', 'gta', 'clunker', 'coax', 'kneale', 'obamacare', 'amtech', 'antigua', 'bo', 'becky', 'positions', 'nominated', 'businesses', 'forthright', 'earmarks', 'projectsto', 'billpasse', 'weekwhich', 'gains', 'costliest', 'storch', 'safest', 'confrontation', 'methodology', 'austerity', 'odst', 'muslim', 'communion', 'rochdale', 'starcraft', 'galleon', 'corker', 'bing', 'timequestions', 'aca', 'images', 'cfpa', 'nokias', 'webos', 'crist', 'zynga', 'retransmission', 'groupon', 'strasburg', 'avatars', 'admob', 'procedural', 'dodd', 'sozzi', 'irelands', 'cloture', 'photographers', 'spongetech', 'clearinghouse', 'ipad', 'noaa', 'plume', 'abacus', 'finreg', 'fcic', 'atfollow', 'netnet', 'robo', 'eurozone', 'll', 'dealbreaker', 'techonomy', 'frontpoint', 'toning', 'sbnh', 'kinect', 'ops', 'putback', 'jg', 'wikileaks', 'assange', 'photodisc', 'qe', 'dispersant', 'oyster', 'pelican', 'hft', 'tepper', 'gamesa']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AuAg4f3Brs3"
      },
      "source": [
        "Test the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8L9uLEcBrs3",
        "outputId": "2b47c9d2-169c-4d25-df3e-7f22f576462b"
      },
      "source": [
        "model.wv.most_similar('fake')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('phrase', 0.8317193388938904),\n",
              " ('joke', 0.8226124048233032),\n",
              " ('funny', 0.8191583752632141),\n",
              " ('tweet', 0.8003485202789307),\n",
              " ('shame', 0.7905541658401489),\n",
              " ('blogger', 0.7843666076660156),\n",
              " ('crazy', 0.7837486267089844),\n",
              " ('weird', 0.777042806148529),\n",
              " ('gossip', 0.776073694229126),\n",
              " ('rant', 0.775067925453186)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGooeh53Brs3"
      },
      "source": [
        "### Importing word embeddings to Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BJnooRyBrs3"
      },
      "source": [
        "# save the model\n",
        "filename = 'article_embeddings.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVn1oee7Brs3",
        "outputId": "67a4cb5d-ee8e-45ab-8245-5060b0879494"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gensim.models.word2vec.Word2Vec"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QLgi1WtBrs3"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index= {}\n",
        "f = open(os.path.join('', 'article_embeddings.txt'), encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SYaR9dbBrs3"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from datetime import date\n",
        "token_list = list([token.split(\" \") for token in df[df['Dates'] <= date(2010, 12, 31)]['tokens']])\n",
        "\n",
        "# vectorise the text samples into a 2D integer tensor\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(token_list)\n",
        "\n",
        "token_list = list([token.split(\" \") for token in df['tokens']])\n",
        "sequences = tokenizer.texts_to_sequences(token_list)\n"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spIBjrJOBrs3",
        "outputId": "290cf339-e1fc-4c3a-b18c-7dd27ae0e0dd"
      },
      "source": [
        "# pad sequences\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Python program to get average of a list \n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "max_length = int(Average([len(doc) for doc in token_list]))\n",
        "articles_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "print('Shape of article tensor:', articles_pad.shape)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 54770 unique tokens.\n",
            "Shape of article tensor: (3468, 4901)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YltVUtA8Brs3"
      },
      "source": [
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, size))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > vocab_size:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br3FfkGnBrs3",
        "outputId": "55b8a265-70d7-4c0a-d254-46cf8193cb9c"
      },
      "source": [
        "print(embedding_matrix)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.3509108  -0.21185787 -0.6093342  ...  0.25355652  0.6357565\n",
            "   0.6436059 ]\n",
            " [ 0.00085938 -0.10840287 -0.2765963  ... -0.13175198  0.33479488\n",
            "   0.14926809]\n",
            " ...\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4owHvrWKBrs3"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.initializers import Constant"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKmpYy1pbpQD"
      },
      "source": [
        "### Keeping Embedding Layer Constant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jhf_CaybkXH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY2zdTSABrs3",
        "outputId": "1b5c7906-b97a-45b3-b71d-7cb2684e9ba1"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                           size,\n",
        "                           embeddings_initializer=Constant(embedding_matrix),\n",
        "                           input_length=max_length,\n",
        "                           trainable=False)\n",
        "# Add embedding layer\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Add a LSTM layer with 50 internal units.\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(100,12)))\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(100,12)))\n",
        "model.add(LSTM(50))\n",
        "# Add a Dense layer with 12 units.\n",
        "model.add(Dense(12))\n",
        "# Add compiler with XXX\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "# Print summary of model\n",
        "print(model.summary())"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 4901, 100)         5477100   \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 4901, 50)          30200     \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 4901, 50)          20200     \n",
            "_________________________________________________________________\n",
            "lstm_14 (LSTM)               (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 12)                612       \n",
            "=================================================================\n",
            "Total params: 5,548,312\n",
            "Trainable params: 71,212\n",
            "Non-trainable params: 5,477,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW0o-KToBrs3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b731b854-e6f8-458d-c96e-017ad5669878"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "y = df.iloc[:,1:13]\n",
        "y = y - 1\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(y)\n",
        "y = scaler.transform(y)\n",
        "y = pd.DataFrame(y)\n",
        "y"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.328228</td>\n",
              "      <td>-0.005662</td>\n",
              "      <td>0.114832</td>\n",
              "      <td>0.371870</td>\n",
              "      <td>0.515165</td>\n",
              "      <td>0.490431</td>\n",
              "      <td>0.279945</td>\n",
              "      <td>0.052145</td>\n",
              "      <td>0.432752</td>\n",
              "      <td>0.260712</td>\n",
              "      <td>-0.310627</td>\n",
              "      <td>-0.458875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.218686</td>\n",
              "      <td>-0.065795</td>\n",
              "      <td>-0.483748</td>\n",
              "      <td>0.384755</td>\n",
              "      <td>0.565919</td>\n",
              "      <td>0.685552</td>\n",
              "      <td>0.117066</td>\n",
              "      <td>-0.251364</td>\n",
              "      <td>0.232677</td>\n",
              "      <td>0.218241</td>\n",
              "      <td>-0.359639</td>\n",
              "      <td>-0.069889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.264687</td>\n",
              "      <td>-0.681856</td>\n",
              "      <td>-1.090487</td>\n",
              "      <td>0.125171</td>\n",
              "      <td>0.235788</td>\n",
              "      <td>0.263198</td>\n",
              "      <td>-0.366192</td>\n",
              "      <td>-0.460463</td>\n",
              "      <td>-0.402354</td>\n",
              "      <td>0.027229</td>\n",
              "      <td>-0.165538</td>\n",
              "      <td>-0.416807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.513139</td>\n",
              "      <td>-1.017837</td>\n",
              "      <td>-0.989788</td>\n",
              "      <td>-0.070064</td>\n",
              "      <td>-0.190034</td>\n",
              "      <td>-0.183631</td>\n",
              "      <td>-0.285186</td>\n",
              "      <td>-0.470386</td>\n",
              "      <td>-0.829013</td>\n",
              "      <td>-0.186015</td>\n",
              "      <td>-0.493619</td>\n",
              "      <td>-0.464488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.386152</td>\n",
              "      <td>-0.790483</td>\n",
              "      <td>-0.287718</td>\n",
              "      <td>-0.179931</td>\n",
              "      <td>-0.200319</td>\n",
              "      <td>-0.265923</td>\n",
              "      <td>-0.252105</td>\n",
              "      <td>-0.161845</td>\n",
              "      <td>-0.534231</td>\n",
              "      <td>-0.187590</td>\n",
              "      <td>-0.323295</td>\n",
              "      <td>-0.499822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>0.800756</td>\n",
              "      <td>-0.235675</td>\n",
              "      <td>4.526932</td>\n",
              "      <td>1.978131</td>\n",
              "      <td>0.710216</td>\n",
              "      <td>1.706362</td>\n",
              "      <td>-0.456198</td>\n",
              "      <td>1.177867</td>\n",
              "      <td>0.396249</td>\n",
              "      <td>1.837352</td>\n",
              "      <td>0.243138</td>\n",
              "      <td>1.297770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>0.795560</td>\n",
              "      <td>0.359765</td>\n",
              "      <td>2.447248</td>\n",
              "      <td>0.555283</td>\n",
              "      <td>0.306054</td>\n",
              "      <td>1.414601</td>\n",
              "      <td>0.439599</td>\n",
              "      <td>0.811077</td>\n",
              "      <td>0.319351</td>\n",
              "      <td>2.570999</td>\n",
              "      <td>0.586172</td>\n",
              "      <td>0.747912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>0.657002</td>\n",
              "      <td>0.863396</td>\n",
              "      <td>1.845687</td>\n",
              "      <td>0.326749</td>\n",
              "      <td>-0.169431</td>\n",
              "      <td>0.713848</td>\n",
              "      <td>0.719871</td>\n",
              "      <td>-0.594422</td>\n",
              "      <td>-0.137031</td>\n",
              "      <td>1.119623</td>\n",
              "      <td>0.546672</td>\n",
              "      <td>0.643403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>-0.105796</td>\n",
              "      <td>-0.360910</td>\n",
              "      <td>1.264487</td>\n",
              "      <td>0.226774</td>\n",
              "      <td>-0.861743</td>\n",
              "      <td>0.857999</td>\n",
              "      <td>-0.062321</td>\n",
              "      <td>-1.502181</td>\n",
              "      <td>0.028601</td>\n",
              "      <td>-0.053041</td>\n",
              "      <td>-0.053340</td>\n",
              "      <td>0.008449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>-0.097297</td>\n",
              "      <td>-0.038532</td>\n",
              "      <td>1.606930</td>\n",
              "      <td>0.319636</td>\n",
              "      <td>-1.314843</td>\n",
              "      <td>0.601774</td>\n",
              "      <td>0.050413</td>\n",
              "      <td>-1.662977</td>\n",
              "      <td>0.305761</td>\n",
              "      <td>-0.527075</td>\n",
              "      <td>-0.203325</td>\n",
              "      <td>-0.276631</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "0     0.328228 -0.005662  0.114832  ...  0.260712 -0.310627 -0.458875\n",
              "1     0.218686 -0.065795 -0.483748  ...  0.218241 -0.359639 -0.069889\n",
              "2    -0.264687 -0.681856 -1.090487  ...  0.027229 -0.165538 -0.416807\n",
              "3    -0.513139 -1.017837 -0.989788  ... -0.186015 -0.493619 -0.464488\n",
              "4    -0.386152 -0.790483 -0.287718  ... -0.187590 -0.323295 -0.499822\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "3463  0.800756 -0.235675  4.526932  ...  1.837352  0.243138  1.297770\n",
              "3464  0.795560  0.359765  2.447248  ...  2.570999  0.586172  0.747912\n",
              "3465  0.657002  0.863396  1.845687  ...  1.119623  0.546672  0.643403\n",
              "3466 -0.105796 -0.360910  1.264487  ... -0.053041 -0.053340  0.008449\n",
              "3467 -0.097297 -0.038532  1.606930  ... -0.527075 -0.203325 -0.276631\n",
              "\n",
              "[3468 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9gZj5YyEnO1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8yOXyCEBrs3"
      },
      "source": [
        "y = y - 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHT2O-84Brs4",
        "outputId": "dfc58533-0ec4-49b0-b23e-3fe6c44f5ec2"
      },
      "source": [
        "model.fit(articles_pad, y, epochs=10, verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "109/109 [==============================] - 61s 557ms/step - loss: 1.0018\n",
            "Epoch 2/10\n",
            "109/109 [==============================] - 61s 558ms/step - loss: 0.9975\n",
            "Epoch 3/10\n",
            "109/109 [==============================] - 60s 555ms/step - loss: 0.9955\n",
            "Epoch 4/10\n",
            "109/109 [==============================] - 60s 554ms/step - loss: 0.9876\n",
            "Epoch 5/10\n",
            "109/109 [==============================] - 61s 556ms/step - loss: 0.9750\n",
            "Epoch 6/10\n",
            "109/109 [==============================] - 61s 559ms/step - loss: 0.9642\n",
            "Epoch 7/10\n",
            "109/109 [==============================] - 61s 559ms/step - loss: 0.9397\n",
            "Epoch 8/10\n",
            "109/109 [==============================] - 61s 556ms/step - loss: 0.8957\n",
            "Epoch 9/10\n",
            "109/109 [==============================] - 60s 554ms/step - loss: 0.8675\n",
            "Epoch 10/10\n",
            "109/109 [==============================] - 60s 554ms/step - loss: 0.8365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7ec51f20f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "2qmQJUeSBrs4",
        "outputId": "cae31edb-da73-4ebb-eb52-a11bdfc1c3cc"
      },
      "source": [
        "xtest_docs = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "predicted = pd.DataFrame(model.predict(xtest_docs))\n",
        "predicted"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.046249</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.073936</td>\n",
              "      <td>0.029288</td>\n",
              "      <td>0.066098</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.025895</td>\n",
              "      <td>0.061931</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.104203</td>\n",
              "      <td>0.060377</td>\n",
              "      <td>0.045489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.046249</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.073936</td>\n",
              "      <td>0.029288</td>\n",
              "      <td>0.066098</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.025895</td>\n",
              "      <td>0.061931</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.104203</td>\n",
              "      <td>0.060377</td>\n",
              "      <td>0.045489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.046249</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.073936</td>\n",
              "      <td>0.029288</td>\n",
              "      <td>0.066098</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.025895</td>\n",
              "      <td>0.061931</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.104203</td>\n",
              "      <td>0.060377</td>\n",
              "      <td>0.045489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.046249</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.073936</td>\n",
              "      <td>0.029288</td>\n",
              "      <td>0.066098</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.025895</td>\n",
              "      <td>0.061931</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.104203</td>\n",
              "      <td>0.060377</td>\n",
              "      <td>0.045489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.046249</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.073936</td>\n",
              "      <td>0.029288</td>\n",
              "      <td>0.066098</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.025895</td>\n",
              "      <td>0.061931</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.104203</td>\n",
              "      <td>0.060377</td>\n",
              "      <td>0.045489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>0.231876</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>0.390002</td>\n",
              "      <td>0.253543</td>\n",
              "      <td>0.281606</td>\n",
              "      <td>0.339715</td>\n",
              "      <td>0.256838</td>\n",
              "      <td>0.528814</td>\n",
              "      <td>0.240218</td>\n",
              "      <td>0.290042</td>\n",
              "      <td>0.258705</td>\n",
              "      <td>0.297900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.745065</td>\n",
              "      <td>0.699661</td>\n",
              "      <td>0.614268</td>\n",
              "      <td>0.808219</td>\n",
              "      <td>0.826312</td>\n",
              "      <td>0.641546</td>\n",
              "      <td>0.775505</td>\n",
              "      <td>0.807437</td>\n",
              "      <td>0.931612</td>\n",
              "      <td>0.641758</td>\n",
              "      <td>0.623595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>0.046249</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.073936</td>\n",
              "      <td>0.029288</td>\n",
              "      <td>0.066098</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.025895</td>\n",
              "      <td>0.061931</td>\n",
              "      <td>0.028520</td>\n",
              "      <td>0.104203</td>\n",
              "      <td>0.060377</td>\n",
              "      <td>0.045489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>0.319739</td>\n",
              "      <td>0.310980</td>\n",
              "      <td>0.054761</td>\n",
              "      <td>0.263883</td>\n",
              "      <td>0.435656</td>\n",
              "      <td>0.312119</td>\n",
              "      <td>0.263095</td>\n",
              "      <td>-0.098665</td>\n",
              "      <td>0.143085</td>\n",
              "      <td>0.215959</td>\n",
              "      <td>0.209393</td>\n",
              "      <td>0.050602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>-0.626009</td>\n",
              "      <td>-0.432150</td>\n",
              "      <td>-0.496300</td>\n",
              "      <td>-0.472681</td>\n",
              "      <td>-0.678453</td>\n",
              "      <td>-0.569263</td>\n",
              "      <td>-0.542191</td>\n",
              "      <td>-0.520675</td>\n",
              "      <td>-0.434355</td>\n",
              "      <td>-0.668322</td>\n",
              "      <td>-0.510648</td>\n",
              "      <td>-0.339042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "0     0.046249  0.017442  0.073936  ...  0.104203  0.060377  0.045489\n",
              "1     0.046249  0.017442  0.073936  ...  0.104203  0.060377  0.045489\n",
              "2     0.046249  0.017442  0.073936  ...  0.104203  0.060377  0.045489\n",
              "3     0.046249  0.017442  0.073936  ...  0.104203  0.060377  0.045489\n",
              "4     0.046249  0.017442  0.073936  ...  0.104203  0.060377  0.045489\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "3463  0.231876 -0.000529  0.390002  ...  0.290042  0.258705  0.297900\n",
              "3464  0.881221  0.745065  0.699661  ...  0.931612  0.641758  0.623595\n",
              "3465  0.046249  0.017442  0.073936  ...  0.104203  0.060377  0.045489\n",
              "3466  0.319739  0.310980  0.054761  ...  0.215959  0.209393  0.050602\n",
              "3467 -0.626009 -0.432150 -0.496300  ... -0.668322 -0.510648 -0.339042\n",
              "\n",
              "[3468 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "dSLsO9gEI5WL",
        "outputId": "53e85a65-451f-48c1-a808-df7c1987d312"
      },
      "source": [
        "y\n",
        "\n",
        "y_test_reset = y.reset_index()\n",
        "y_test_comp = (y_test_reset.iloc[:,1:]+1).product()\n",
        "y_predict_comp = (predicted+1).product()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(y_test_comp)\n",
        "plt.plot(y_predict_comp)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "feGMvuPKhSyU",
        "outputId": "ef882657-10fe-4baa-cd24-27628830113f"
      },
      "source": [
        "predicted = pd.DataFrame(scaler.inverse_transform(predicted))\n",
        "predicted"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.002296</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.002296</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.002296</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.002296</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.002296</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>0.002056</td>\n",
              "      <td>0.002660</td>\n",
              "      <td>0.001768</td>\n",
              "      <td>0.001879</td>\n",
              "      <td>0.001990</td>\n",
              "      <td>0.001705</td>\n",
              "      <td>0.001889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>0.002011</td>\n",
              "      <td>0.003153</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.002069</td>\n",
              "      <td>0.002670</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.001994</td>\n",
              "      <td>0.001712</td>\n",
              "      <td>0.001909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.003136</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.001244</td>\n",
              "      <td>0.002296</td>\n",
              "      <td>0.002047</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.001871</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>0.002002</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>0.002301</td>\n",
              "      <td>0.002055</td>\n",
              "      <td>0.002660</td>\n",
              "      <td>0.001759</td>\n",
              "      <td>0.001875</td>\n",
              "      <td>0.001989</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>0.001874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>0.001986</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.002287</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.001984</td>\n",
              "      <td>0.001692</td>\n",
              "      <td>0.001851</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "0     0.001997  0.003136  0.000319  ...  0.001988  0.001701  0.001874\n",
              "1     0.001997  0.003136  0.000319  ...  0.001988  0.001701  0.001874\n",
              "2     0.001997  0.003136  0.000319  ...  0.001988  0.001701  0.001874\n",
              "3     0.001997  0.003136  0.000319  ...  0.001988  0.001701  0.001874\n",
              "4     0.001997  0.003136  0.000319  ...  0.001988  0.001701  0.001874\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "3463  0.002000  0.003135  0.000336  ...  0.001990  0.001705  0.001889\n",
              "3464  0.002011  0.003153  0.000353  ...  0.001994  0.001712  0.001909\n",
              "3465  0.001997  0.003136  0.000319  ...  0.001988  0.001701  0.001874\n",
              "3466  0.002002  0.003143  0.000318  ...  0.001989  0.001704  0.001874\n",
              "3467  0.001986  0.003125  0.000288  ...  0.001984  0.001692  0.001851\n",
              "\n",
              "[3468 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "NLqpDXAHQBaV",
        "outputId": "df1a923f-d109-4457-bc12-303783b6d344"
      },
      "source": [
        "df.iloc[0,13]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'imagine disney nbc universal team desperate compete pay copyright material desperate time call desperate measure cook spoil broth movielink collaboration studio offer digital download politic get intense take longer ground trouble need team stand chance terrible weekend warner bros totally different equally disappointing film expect movie message big romantic leo pic sight set lo film cost bring ouch end warner brothers spectrum unaccompanied minors hope home bring expensive budget est look bunch dance penguin totally knock warner prospect strong christmas box office season thing cheap tv spot company call cheap tv spot lot speculation cheap tv spot buy tivo company deny raise question buy commercial skip company maybe question comment company transform transform lot work leave merck analyst day company headquarters central new jersey company reveal tine street suspect call cetp inhibitor drug development pipeline type drug pfizerpulle plug increase risk death long clinical trial merck say see heart problem mind pfizers independent safety date monitoring board problem torcetrapib patient merck head say company carefully review datum try decide future plan drug live interview ceo lilly analyst meeting pfizer wake torcetrapib news take merck interview cnbc analyst meeting year merck spokespeople interview make available print reporter reason internal company backstory think unfair accessible news medium broadcast print necessarily reporter news conference exception bbc crew tv outlet company offer first exclusive reporter problem especially finally merck say face vioxx lawsuit case set trial atlantic city give hope vioxx successor call arcoxia merck expect fda decision painkiller reveal fda advisory panel examine drug mercks headquarters whitehouse station new jersey beautiful campus producer see gaggle loud wild turkey winding drive asian inspire main building deer roam area security relatively tight thankfully near bad pfizer major drug company meet big pfizers financial focused update question comment '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uObmjqhQQunp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "002d2a45-fff0-41b1-955d-67a8c87d0e8c"
      },
      "source": [
        "tokenizer.texts_to_sequences(['frederik', 'be', 'possibly', 'genius', 'raaaaw'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[159515], [8185], [1682], [5553], []]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwETn297uEs_",
        "outputId": "95d3e9f4-0f4e-4022-dec3-0e5005d2596d"
      },
      "source": [
        "len(sequences[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "293"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifsHO3lzutRC",
        "outputId": "31c63afb-5227-46b9-d690-8101ac138504"
      },
      "source": [
        "pad_sequences(sequences[0:1], maxlen=max_length)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ..., 107,  70, 987]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6oqBRKpvEan",
        "outputId": "583431bc-b2d5-49b5-f206-7864f560ca74"
      },
      "source": [
        "xtest_docs"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2049,   798,   448, ...,     0,     0,     0],\n",
              "       [ 4892,   688,    89, ...,     0,     0,     0],\n",
              "       [ 1026,  5816,    39, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 1031,  3269,     3, ...,     0,     0,     0],\n",
              "       [ 2446,  2290,   392, ...,     3,  7388,   987],\n",
              "       [  152,   570,   353, ...,    35, 23072,   987]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJhYdg1RXWGX"
      },
      "source": [
        "def walk_forward_validation(model, epochs, x, y, step_size, train_steps, val_window):\n",
        "    \n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    \n",
        "    n_records = len(x)\n",
        "    n_init_train = step_size * train_steps\n",
        "    train_pred = []\n",
        "    val_pred = []\n",
        "    mse_scores = []\n",
        "    for i in range(n_init_train, n_records, step_size):\n",
        "      \n",
        "        train_from = i-n_init_train\n",
        "        train_to = i\n",
        "        test_from = i+1\n",
        "        test_to = i+val_window\n",
        "\n",
        "        x_train, x_test = x[train_from:train_to], x[test_from:test_to]\n",
        "        y_train, y_test = y[train_from:train_to], y[test_from:test_to]\n",
        "        \n",
        "        print(f'Train from {i-n_init_train} to {i} and validate for {i+1} to {i+val_window}')\n",
        "        model.fit(x_train, y_train, epochs=epochs, verbose=1)\n",
        "\n",
        "        y_train_pred = model.predict(x_train)\n",
        "        for y_train_day in y_train_pred:\n",
        "            train_pred.append(y_train_day.tolist())\n",
        "        \n",
        "        y_pred = model.predict(x_test)\n",
        "        for y_test_day in y_pred:\n",
        "            val_pred.append(y_test_day.tolist())\n",
        "\n",
        "        train_mse = mean_squared_error(y_train,y_train_pred)\n",
        "        val_mse = mean_squared_error(y_test,y_pred)\n",
        "        mse_scores.append([train_mse, val_mse])\n",
        "\n",
        "        print(f'     train: {train_mse} \\nvalidation: {val_mse} \\n')\n",
        "\n",
        "    return train_pred, val_pred, mse_scores"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4JY7H9nXZPZ",
        "outputId": "dcaca6bb-261f-46ec-9957-e3b5ed050e55"
      },
      "source": [
        "epoch_tuning_performance = []\n",
        "for epoch in range(1,11):\n",
        "    train_pred, val_pred, validation_metrics = walk_forward_validation(model = model, epochs = epoch, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
        "    validation_metrics = pd.DataFrame(validation_metrics).mean(axis=0)\n",
        "    mean_train_mse = validation_metrics.iloc[0]\n",
        "    mean_val_mse = validation_metrics.iloc[1]\n",
        "    epoch_tuning_performance.append([mean_train_mse, mean_val_mse])"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2847\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2637\n",
            "     train: 0.25353948708669877 \n",
            "validation: 1.0987553284487783 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.5488\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.4291\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.3592\n",
            "     train: 0.32548059392790146 \n",
            "validation: 0.47104699895035496 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.3798\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.3097\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2807\n",
            "     train: 0.26086061755352735 \n",
            "validation: 0.4719334960045198 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.3509\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.3048\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2722\n",
            "     train: 0.25851675567285076 \n",
            "validation: 0.3898364941587415 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2814\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2334\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2075\n",
            "     train: 0.1960795665080043 \n",
            "validation: 0.39160936783160616 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2638\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2186\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1950\n",
            "     train: 0.18387999893533782 \n",
            "validation: 3.7742803757786176 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 1.2063\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.9434\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.8048\n",
            "     train: 0.7305396481493646 \n",
            "validation: 1.7944191096757158 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.2708\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.0051\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.8568\n",
            "     train: 0.7875095359549077 \n",
            "validation: 0.8504160100311791 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/3\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.9934\n",
            "Epoch 2/3\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.8452\n",
            "Epoch 3/3\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.7841\n",
            "     train: 0.7324729756376464 \n",
            "validation: 1.9604244728352491 \n",
            "\n",
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.9777\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.9494\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.9381\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.9396\n",
            "     train: 0.9204848698719842 \n",
            "validation: 1.1184404634625158 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 1.1369\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 1.1272\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 1.1225\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 1.1206\n",
            "     train: 1.1169531512281343 \n",
            "validation: 0.2778507660196899 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 1.0544\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 1.0529\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 1.0476\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 1.0481\n",
            "     train: 1.0470714393653069 \n",
            "validation: 0.596367221151849 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.6301\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.6292\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.6279\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.6269\n",
            "     train: 0.6263103224566678 \n",
            "validation: 0.3128914249436087 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.3873\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.3869\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.3863\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.3856\n",
            "     train: 0.3854936078858087 \n",
            "validation: 0.49674838587325953 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.4666\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.4499\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.4409\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.4413\n",
            "     train: 0.4392445713136941 \n",
            "validation: 0.4213480355941459 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3788\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.3151\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.3060\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.3017\n",
            "     train: 0.2979941424068122 \n",
            "validation: 0.43519373323371996 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.3350\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.3148\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3022\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2938\n",
            "     train: 0.2874686331438296 \n",
            "validation: 0.3956085580961619 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2817\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.2567\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2387\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2262\n",
            "     train: 0.21973388413062378 \n",
            "validation: 0.3201117411056041 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2828\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2606\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2413\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2297\n",
            "     train: 0.22125647529387368 \n",
            "validation: 0.3474680595023274 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2471\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2260\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2073\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1894\n",
            "     train: 0.1824741265941666 \n",
            "validation: 0.3601853707102796 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2255\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1911\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.1728\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.1607\n",
            "     train: 0.1546022302206892 \n",
            "validation: 0.29668953472150045 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2039\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1816\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1638\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1532\n",
            "     train: 0.1474756717387782 \n",
            "validation: 0.4102644509757387 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2314\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2057\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1794\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1670\n",
            "     train: 0.15841018455041572 \n",
            "validation: 0.44893678636952855 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2645\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2264\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2045\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1927\n",
            "     train: 0.1870856213668601 \n",
            "validation: 0.42445441234164866 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2810\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2554\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2350\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.2244\n",
            "     train: 0.2183434564772442 \n",
            "validation: 0.32462376467533927 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.2666\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2380\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2267\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2159\n",
            "     train: 0.21158364708585686 \n",
            "validation: 0.6428271248319947 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.3417\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2870\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2590\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2476\n",
            "     train: 0.2378544158804692 \n",
            "validation: 0.43082012886716425 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3069\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2749\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2579\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2478\n",
            "     train: 0.24029511261511796 \n",
            "validation: 0.6879534242956691 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.4139\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.3788\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.3582\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.3438\n",
            "     train: 0.33696212515271234 \n",
            "validation: 0.367609915628679 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3597\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.3377\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3237\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.3131\n",
            "     train: 0.30685458089516005 \n",
            "validation: 0.33925291253755147 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3425\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.3122\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2951\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2826\n",
            "     train: 0.2764686561158298 \n",
            "validation: 0.4980529787104189 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3033\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2752\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2550\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2427\n",
            "     train: 0.23317282905737888 \n",
            "validation: 0.2636682907928604 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2535\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2321\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2208\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2064\n",
            "     train: 0.19911773721393947 \n",
            "validation: 0.19375283476510255 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2147\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.1961\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1808\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1707\n",
            "     train: 0.16357937991394722 \n",
            "validation: 0.3197510728753079 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.1960\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1702\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1580\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1507\n",
            "     train: 0.14593768029783358 \n",
            "validation: 0.28364785838812995 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1891\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1782\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1684\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1606\n",
            "     train: 0.1565316480500634 \n",
            "validation: 0.36653482093474615 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2480\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2342\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.2194\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2087\n",
            "     train: 0.20295446469106493 \n",
            "validation: 0.579703496485292 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.3319\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2639\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2480\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2174\n",
            "     train: 0.20925452901451536 \n",
            "validation: 0.40063946471878226 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2929\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2647\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2451\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.2258\n",
            "     train: 0.2152222670364292 \n",
            "validation: 0.33336023369297635 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2587\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2311\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2139\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 6s 919ms/step - loss: 0.1991\n",
            "     train: 0.19127045640443072 \n",
            "validation: 0.9235861484454855 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.4523\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3112\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2722\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2460\n",
            "     train: 0.23166229986974507 \n",
            "validation: 0.37524760552680697 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2856\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.2453\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2209\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2057\n",
            "     train: 0.19704950213578074 \n",
            "validation: 0.42837901987573934 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2838\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2404\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.2223\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2084\n",
            "     train: 0.20117035865437996 \n",
            "validation: 0.29939189522607507 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.2217\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1908\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1723\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1605\n",
            "     train: 0.15440424984784737 \n",
            "validation: 0.3106109404525855 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2088\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1833\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1621\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1502\n",
            "     train: 0.14283105632288962 \n",
            "validation: 2.2952681825159362 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.7351\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.5632\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.5006\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.4274\n",
            "     train: 0.387808094184979 \n",
            "validation: 1.265319757755796 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.7557\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.6187\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.5903\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 5s 871ms/step - loss: 0.5364\n",
            "     train: 0.5072793343516563 \n",
            "validation: 0.612781035455809 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/4\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.6698\n",
            "Epoch 2/4\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.6138\n",
            "Epoch 3/4\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.5673\n",
            "Epoch 4/4\n",
            "6/6 [==============================] - 2s 366ms/step - loss: 0.5462\n",
            "     train: 0.5268625822866375 \n",
            "validation: 2.238014413692887 \n",
            "\n",
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.9445\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.9245\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 414ms/step - loss: 0.9214\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.9156\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.9157\n",
            "     train: 0.9076827029504937 \n",
            "validation: 1.0648904744196293 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 1.1186\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 1.1168\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 1.1192\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 1.1120\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 1.1157\n",
            "     train: 1.1127855455702176 \n",
            "validation: 0.2920540574003662 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 3s 542ms/step - loss: 1.0561\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 1.0487\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 1.0470\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 1.0477\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 1.0460\n",
            "     train: 1.0422291189534059 \n",
            "validation: 0.5905068641907293 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.6270\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.6301\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.6306\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.6302\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.6290\n",
            "     train: 0.6277632516549703 \n",
            "validation: 0.31209333248651056 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.3897\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.3876\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.3861\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.3854\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.3853\n",
            "     train: 0.3849657688171157 \n",
            "validation: 0.47418253136880456 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.4579\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.4479\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.4411\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.4393\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.4385\n",
            "     train: 0.43683411520247056 \n",
            "validation: 0.2513316765968239 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.3320\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.3032\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.2935\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 4s 651ms/step - loss: 0.2880\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.2844\n",
            "     train: 0.28211440175605446 \n",
            "validation: 0.3871789967729322 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.3061\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2842\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2714\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2645\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2609\n",
            "     train: 0.25808372331909507 \n",
            "validation: 0.35284199430529645 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2419\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2166\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2021\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1931\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1874\n",
            "     train: 0.18349386339911514 \n",
            "validation: 0.2571503832909025 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2339\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2100\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1996\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1925\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1880\n",
            "     train: 0.18494334487924466 \n",
            "validation: 0.2723199662510207 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2029\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1894\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1775\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1700\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1639\n",
            "     train: 0.16108142428724545 \n",
            "validation: 0.27924782041804747 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1848\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1612\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1470\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 12s 2s/step - loss: 0.1391\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1336\n",
            "     train: 0.13030064660165389 \n",
            "validation: 0.22082423983801114 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1620\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1433\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1356\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1292\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1245\n",
            "     train: 0.12179274585519627 \n",
            "validation: 0.3789801765516088 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2006\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1685\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1490\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1397\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.1334\n",
            "     train: 0.12836290388648627 \n",
            "validation: 0.38565345173626886 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2223\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1944\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1796\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1742\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1682\n",
            "     train: 0.16342866941479642 \n",
            "validation: 0.3636832907655188 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.2469\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2196\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2041\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1938\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1853\n",
            "     train: 0.18040546397357818 \n",
            "validation: 0.275732795470441 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.2257\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.2050\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1905\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1814\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1738\n",
            "     train: 0.16881837054604376 \n",
            "validation: 0.5574994095637783 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.2837\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.2374\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.2130\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.2013\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1894\n",
            "     train: 0.18384278996212564 \n",
            "validation: 0.39497848664783036 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2636\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.2335\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2198\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2088\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.2024\n",
            "     train: 0.19774906616240503 \n",
            "validation: 0.621924059876427 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.3605\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.3288\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.3142\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.3021\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2931\n",
            "     train: 0.28794485557615873 \n",
            "validation: 0.3526044137825406 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.3237\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2956\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2786\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.2678\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2570\n",
            "     train: 0.2511672649911686 \n",
            "validation: 0.29679937615946267 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2852\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2535\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2375\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.2275\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.2200\n",
            "     train: 0.2144254455319823 \n",
            "validation: 0.3930154257539311 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 6s 1s/step - loss: 0.2271\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1956\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1756\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.1618\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1530\n",
            "     train: 0.14725433106459904 \n",
            "validation: 0.2611555925416343 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1892\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1646\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1534\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1451\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.1373\n",
            "     train: 0.13400124047784148 \n",
            "validation: 0.19725950182579308 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1672\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1451\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1323\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1251\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1192\n",
            "     train: 0.11540155812239951 \n",
            "validation: 0.2833493749100405 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1591\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1371\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1267\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1197\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 334ms/step - loss: 0.1131\n",
            "     train: 0.10931236949113145 \n",
            "validation: 0.29739438376645705 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1688\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1494\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1369\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1279\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1211\n",
            "     train: 0.11598472043418451 \n",
            "validation: 0.3292936201630579 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.2022\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1803\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1651\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1522\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.1431\n",
            "     train: 0.13699279022814242 \n",
            "validation: 0.5207886792298003 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 334ms/step - loss: 0.2693\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 334ms/step - loss: 0.2065\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1812\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1639\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.1486\n",
            "     train: 0.14177340071732022 \n",
            "validation: 0.3778647140982474 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 334ms/step - loss: 0.2348\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2030\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1777\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1620\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1490\n",
            "     train: 0.14177869993566963 \n",
            "validation: 0.3144469739481393 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.2016\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1705\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1548\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1430\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1347\n",
            "     train: 0.1293178708550625 \n",
            "validation: 0.6793755472935898 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.3208\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.2443\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.2122\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1874\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1763\n",
            "     train: 0.16040804655718904 \n",
            "validation: 0.3181698643830127 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.2277\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 333ms/step - loss: 0.2036\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1796\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1612\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1524\n",
            "     train: 0.1456869635837281 \n",
            "validation: 0.35846842480723845 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.2339\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1983\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1803\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1685\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1593\n",
            "     train: 0.15262603849892342 \n",
            "validation: 0.30101673032342685 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1924\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.1527\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1378\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1250\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1148\n",
            "     train: 0.10923790687876904 \n",
            "validation: 0.29296180933495103 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1709\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1350\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1140\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1011\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.0915\n",
            "     train: 0.0862125666447728 \n",
            "validation: 2.170966109628503 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.6789\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.4718\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.4127\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.3680\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.3282\n",
            "     train: 0.30070763592568805 \n",
            "validation: 1.035940681900834 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.6158\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.5145\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.4914\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.4415\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.4029\n",
            "     train: 0.3764907628046233 \n",
            "validation: 0.6015889095902253 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.5493\n",
            "Epoch 2/5\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.4842\n",
            "Epoch 3/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.4554\n",
            "Epoch 4/5\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.4371\n",
            "Epoch 5/5\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.4162\n",
            "     train: 0.4047757166658148 \n",
            "validation: 2.2954031586473205 \n",
            "\n",
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.9586\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.9314\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.9217\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.9205\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.9110\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.9085\n",
            "     train: 0.9093040682959147 \n",
            "validation: 1.0646414676436469 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 1.1216\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 1.1187\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 1.1202\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 1.1146\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 1.1139\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 1.1135\n",
            "     train: 1.1123478213521691 \n",
            "validation: 0.28194647362938674 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 334ms/step - loss: 1.0540\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 1.0484\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 1.0464\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 1.0458\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 1.0467\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 1.0458\n",
            "     train: 1.0437659998531506 \n",
            "validation: 0.5972012794457229 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.6309\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.6280\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.6271\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.6252\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 406ms/step - loss: 0.6253\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.6246\n",
            "     train: 0.6241819105607592 \n",
            "validation: 0.31895128047047994 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.3876\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.3991\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.3941\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.3984\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.3898\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.3859\n",
            "     train: 0.38525461019926444 \n",
            "validation: 0.46705266236408166 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.4559\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.4449\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.4401\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 407ms/step - loss: 0.4388\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.4371\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.4355\n",
            "     train: 0.43465206908752035 \n",
            "validation: 0.2678393162540765 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.3324\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.3006\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2898\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2835\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.2800\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 3s 483ms/step - loss: 0.2774\n",
            "     train: 0.2757572182578331 \n",
            "validation: 0.32058951212484216 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.2793\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.2642\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 3s 488ms/step - loss: 0.2544\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.2488\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2448\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2416\n",
            "     train: 0.23948975025819874 \n",
            "validation: 0.3406332601961506 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 405ms/step - loss: 0.2160\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1952\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1816\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1738\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 7s 1s/step - loss: 0.1675\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 5s 789ms/step - loss: 0.1637\n",
            "     train: 0.16139060175922135 \n",
            "validation: 0.23170010725371512 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.2096\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1886\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1785\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1734\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1684\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1655\n",
            "     train: 0.16336638605086856 \n",
            "validation: 0.2463292404607539 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1823\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1679\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1569\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1538\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1498\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1465\n",
            "     train: 0.14342876664660512 \n",
            "validation: 0.2530860007037265 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1640\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1409\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1303\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1204\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1144\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.1102\n",
            "     train: 0.1074961798642519 \n",
            "validation: 0.2151858650819777 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1449\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1263\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1168\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1110\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1054\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1017\n",
            "     train: 0.09927831974113811 \n",
            "validation: 0.34385420252829174 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.1721\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1418\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1275\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1183\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1117\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1071\n",
            "     train: 0.10414077605063483 \n",
            "validation: 0.3980543706028509 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.2111\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 0.1711\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1591\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 336ms/step - loss: 0.1482\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.1414\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 335ms/step - loss: 0.1351\n",
            "     train: 0.13130692438839253 \n",
            "validation: 0.3095159806110067 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 340ms/step - loss: 0.2041\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1776\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1627\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1525\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 339ms/step - loss: 0.1440\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 338ms/step - loss: 0.1374\n",
            "     train: 0.13379671288662406 \n",
            "validation: 0.24116558762762683 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1761\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 342ms/step - loss: 0.1547\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1439\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1355\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 341ms/step - loss: 0.1292\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.1247\n",
            "     train: 0.1215127204949095 \n",
            "validation: 0.5861361522688961 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2643\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.2038\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1799\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 343ms/step - loss: 0.1734\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1590\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1528\n",
            "     train: 0.14760776982020463 \n",
            "validation: 0.34666997481948186 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.2268\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.1987\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1883\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.1799\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1741\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1691\n",
            "     train: 0.16454159307764826 \n",
            "validation: 0.6099453034510134 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3433\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2955\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2833\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2704\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2614\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2539\n",
            "     train: 0.24869772752853248 \n",
            "validation: 0.3111008236212966 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2812\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2554\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2383\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2285\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2208\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2151\n",
            "     train: 0.21062542555074729 \n",
            "validation: 0.33828318279106767 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2696\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2268\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2091\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1960\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1881\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1820\n",
            "     train: 0.1768197393799579 \n",
            "validation: 0.416395335957646 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2146\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1697\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1511\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1367\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1243\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1162\n",
            "     train: 0.11205609785919074 \n",
            "validation: 0.2643167005103289 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1600\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1339\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1192\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1099\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1030\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0968\n",
            "     train: 0.09457175094252918 \n",
            "validation: 0.18935191253173156 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1344\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1121\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1020\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0948\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0900\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0849\n",
            "     train: 0.0822523102106676 \n",
            "validation: 0.29000245491600274 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1443\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1192\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0998\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0915\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0848\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0800\n",
            "     train: 0.07664521278035231 \n",
            "validation: 0.3014110389648736 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1469\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1188\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1030\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0930\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0853\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0797\n",
            "     train: 0.07591820844652067 \n",
            "validation: 0.3385896164814952 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1701\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1428\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1242\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1134\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1054\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0993\n",
            "     train: 0.09528920104007015 \n",
            "validation: 0.4890449032569217 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2302\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1820\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1485\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1357\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1225\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1130\n",
            "     train: 0.10779167700440052 \n",
            "validation: 0.31648897682924637 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1896\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1569\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1368\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1239\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1154\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1080\n",
            "     train: 0.10286049279551097 \n",
            "validation: 0.2941552512912929 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1667\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1373\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1221\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1118\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1048\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0988\n",
            "     train: 0.0949120189722978 \n",
            "validation: 0.6220177431124178 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.2813\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2053\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1619\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1507\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1330\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1241\n",
            "     train: 0.11781327863111762 \n",
            "validation: 0.32755299231901375 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1992\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1600\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1428\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1309\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1231\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1166\n",
            "     train: 0.1129408612565566 \n",
            "validation: 0.2957441885278766 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1911\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1588\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1448\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1309\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1216\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1149\n",
            "     train: 0.11062840534917001 \n",
            "validation: 0.2717098006387126 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1513\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1130\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0993\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0887\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0813\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0751\n",
            "     train: 0.07151398875944666 \n",
            "validation: 0.23061466345275738 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1210\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0913\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0760\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0661\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0600\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0552\n",
            "     train: 0.05182958689159812 \n",
            "validation: 1.9359075556060397 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.5771\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.3703\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.3552\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2850\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2550\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2303\n",
            "     train: 0.2154843594359903 \n",
            "validation: 1.110043848124046 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.5415\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.4540\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.3881\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.3642\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.3325\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.3138\n",
            "     train: 0.30050848511698564 \n",
            "validation: 0.5885273646059989 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/6\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.4771\n",
            "Epoch 2/6\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.4245\n",
            "Epoch 3/6\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.3982\n",
            "Epoch 4/6\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.3729\n",
            "Epoch 5/6\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.3532\n",
            "Epoch 6/6\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.3370\n",
            "     train: 0.32672222434329545 \n",
            "validation: 2.2112819436335553 \n",
            "\n",
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.9596\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.9215\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.9203\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.9107\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.9097\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.9078\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.9045\n",
            "     train: 0.9019845894940315 \n",
            "validation: 1.073697681965811 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 1.1173\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 1.1141\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 1.1151\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 1.1137\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 1.1117\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 1.1121\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 1.1119\n",
            "     train: 1.1114365768385732 \n",
            "validation: 0.2833459222153298 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 1.0544\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 1.0471\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 1.0451\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 1.0451\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 1.0437\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 1.0455\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 1.0449\n",
            "     train: 1.0422746302166852 \n",
            "validation: 0.6055198981911007 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.6311\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.6280\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.6287\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.6247\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.6251\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.6249\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.6243\n",
            "     train: 0.6239018134459302 \n",
            "validation: 0.32302356336094235 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.3894\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.3859\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.3853\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.3848\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.3845\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.3838\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.3836\n",
            "     train: 0.3829717907492596 \n",
            "validation: 0.45004784803552145 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.4489\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.4389\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.4542\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.4386\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.4373\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.4347\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.4341\n",
            "     train: 0.4330067138149984 \n",
            "validation: 0.2360897840880011 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.3217\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.2944\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.2861\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 366ms/step - loss: 0.2789\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.2755\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.2730\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.2708\n",
            "     train: 0.26929539347718917 \n",
            "validation: 0.3584978822030746 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2852\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2635\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.2506\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2432\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.2377\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.2334\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2305\n",
            "     train: 0.2285261494176294 \n",
            "validation: 0.30569517147122083 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1976\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1769\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1663\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1598\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1564\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1526\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1495\n",
            "     train: 0.1476026227040778 \n",
            "validation: 0.2341588436410095 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2003\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1804\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1675\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1626\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1587\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1547\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1522\n",
            "     train: 0.15007526367996518 \n",
            "validation: 0.24785459476328506 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1746\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1542\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1454\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1392\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1350\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.1319\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1298\n",
            "     train: 0.12794351376115573 \n",
            "validation: 0.24988119032028588 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1508\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1247\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1136\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1061\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1009\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0964\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0934\n",
            "     train: 0.09158247520455448 \n",
            "validation: 0.1955026713291714 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1270\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1103\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1006\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0947\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0906\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0872\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0843\n",
            "     train: 0.08248015863267402 \n",
            "validation: 0.34122476569898574 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1579\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1202\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1036\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0972\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0913\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0871\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0840\n",
            "     train: 0.08163200894177279 \n",
            "validation: 0.3440964889609554 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1747\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1506\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1390\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1281\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.1188\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1129\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1076\n",
            "     train: 0.1043506972608293 \n",
            "validation: 0.30901359847988774 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1851\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.1535\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.1355\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1253\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1181\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1128\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1082\n",
            "     train: 0.10524104669142074 \n",
            "validation: 0.23156530996560268 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1513\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1317\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1193\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1121\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1071\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1038\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1011\n",
            "     train: 0.0990304015754754 \n",
            "validation: 0.5346669267268147 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2348\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1735\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1620\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1573\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1508\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1333\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1288\n",
            "     train: 0.12487246606126744 \n",
            "validation: 0.32727800564093384 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.2034\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1775\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1661\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1586\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1527\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1486\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1453\n",
            "     train: 0.14296931479559125 \n",
            "validation: 0.6151247759807456 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.3161\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2671\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2491\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2388\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2304\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2241\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2202\n",
            "     train: 0.21639488290833217 \n",
            "validation: 0.2931577558340225 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2522\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2253\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2104\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2018\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1951\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1900\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1855\n",
            "     train: 0.18243571426478353 \n",
            "validation: 0.3004736065982801 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2361\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1960\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1832\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1721\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1638\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1592\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1548\n",
            "     train: 0.15235640437183548 \n",
            "validation: 0.34982655946368024 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1781\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1350\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1164\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1036\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0956\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0895\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0846\n",
            "     train: 0.08154088945560205 \n",
            "validation: 0.23824320824529246 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1285\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1029\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0903\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0837\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0792\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0748\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0714\n",
            "     train: 0.06913410665199284 \n",
            "validation: 0.17764911002818715 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1121\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0894\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0797\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0736\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0702\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0668\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0642\n",
            "     train: 0.06266358646228186 \n",
            "validation: 0.23050641154105503 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1159\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0908\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0798\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0721\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0660\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0614\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0583\n",
            "     train: 0.056040824870163625 \n",
            "validation: 0.2580867011008577 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1177\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0894\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0770\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0681\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0617\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0571\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0537\n",
            "     train: 0.05156973602955354 \n",
            "validation: 0.2766395249809813 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1297\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1071\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0936\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0855\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0794\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0750\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0709\n",
            "     train: 0.06886058816357266 \n",
            "validation: 0.4222134193092512 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1879\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1443\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1204\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1152\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0995\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.0923\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0855\n",
            "     train: 0.0816791978709065 \n",
            "validation: 0.2553580834776443 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1485\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.1189\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1018\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0931\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0862\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0817\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0778\n",
            "     train: 0.07531633161344999 \n",
            "validation: 0.2510951037595816 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1305\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1060\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.0933\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0870\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0820\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0770\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.0733\n",
            "     train: 0.07097180185181261 \n",
            "validation: 0.5691050626914429 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.2371\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1692\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1369\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1258\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1106\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1019\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.0964\n",
            "     train: 0.09157844764404753 \n",
            "validation: 0.26063978724047504 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1572\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1250\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1131\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1065\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1016\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0966\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0914\n",
            "     train: 0.08925448678645893 \n",
            "validation: 0.2643159739426941 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1616\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1267\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1138\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1061\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0972\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0917\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0873\n",
            "     train: 0.08446262095966801 \n",
            "validation: 0.22922482556230817 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1167\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0881\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0758\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0675\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0618\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0574\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0537\n",
            "     train: 0.0515962033418139 \n",
            "validation: 0.16346898841823734 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.0842\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0644\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0541\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0465\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.0422\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0381\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0360\n",
            "     train: 0.03379754444810678 \n",
            "validation: 1.4711543643984282 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.4317\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.3571\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.3423\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.3429\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2749\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.2352\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2217\n",
            "     train: 0.2054310443190398 \n",
            "validation: 0.9052026369866084 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.4764\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.3908\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.3650\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.3219\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.3028\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.2857\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2688\n",
            "     train: 0.2605036775353408 \n",
            "validation: 0.5695396852794427 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.4332\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.3803\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.3482\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.3263\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.3117\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2984\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2900\n",
            "     train: 0.2816440819723485 \n",
            "validation: 2.2637022964487943 \n",
            "\n",
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.9578\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.9202\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.9153\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.9086\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.9055\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.9042\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.9042\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.9028\n",
            "     train: 0.900865355771844 \n",
            "validation: 1.065472439526889 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 1.1141\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 1.1122\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 1.1133\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 1.1123\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 1.1117\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 1.1111\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 1.1180\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 1.1126\n",
            "     train: 1.1102380335444104 \n",
            "validation: 0.2762513908424243 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 1.0485\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 1.0477\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 1.0436\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 1.0420\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 1.0434\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 1.0426\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 1.0401\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 1.0408\n",
            "     train: 1.0399271258729386 \n",
            "validation: 0.6027286220336227 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.6284\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.6308\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.6277\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.6264\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.6247\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.6254\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.6248\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.6237\n",
            "     train: 0.6233395574370235 \n",
            "validation: 0.3227308916941653 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.3884\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.3847\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.3841\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.3838\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.3835\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.3830\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.3845\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.3841\n",
            "     train: 0.3827470017200147 \n",
            "validation: 0.4559370103004457 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.4501\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.4378\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.4368\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.4350\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.4326\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 369ms/step - loss: 0.4311\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.4305\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.4291\n",
            "     train: 0.42648180575229566 \n",
            "validation: 0.21535885076534234 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.3129\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2892\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.2797\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.2720\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2696\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2667\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.2650\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.2633\n",
            "     train: 0.2621979679104874 \n",
            "validation: 0.3683040357909639 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2800\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2474\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.2371\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.2306\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.2253\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.2212\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.2179\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.2155\n",
            "     train: 0.21375709951204722 \n",
            "validation: 0.3039570541408727 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1826\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1629\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1503\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1504\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1459\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.1421\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1383\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1359\n",
            "     train: 0.13413264818400505 \n",
            "validation: 0.2544421371589192 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.1960\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.1655\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.1564\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.1494\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.1455\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1423\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.1394\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1371\n",
            "     train: 0.13522585642133286 \n",
            "validation: 0.2324074615400853 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1600\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.1414\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.1329\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1269\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1232\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1201\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1177\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.1156\n",
            "     train: 0.11409880644206355 \n",
            "validation: 0.20039093709883823 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1287\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1069\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0970\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.0914\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0876\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0839\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0825\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 366ms/step - loss: 0.0804\n",
            "     train: 0.07910535489256312 \n",
            "validation: 0.20067189224686607 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1204\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0992\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0890\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.0826\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.0783\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0749\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0722\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0702\n",
            "     train: 0.0687331636250484 \n",
            "validation: 0.305391552845445 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1374\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1041\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0916\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0847\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0795\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0791\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0754\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0727\n",
            "     train: 0.07058757025496533 \n",
            "validation: 0.2874832406532692 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1484\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1266\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1172\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1089\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1016\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0961\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0919\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0892\n",
            "     train: 0.08670113463825363 \n",
            "validation: 0.2684429559293244 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1574\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1315\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1169\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1081\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1035\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0990\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0981\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0934\n",
            "     train: 0.09126583540070386 \n",
            "validation: 0.2487290102453962 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1465\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1209\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1109\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1053\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1005\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0973\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0951\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0945\n",
            "     train: 0.09209049190233164 \n",
            "validation: 0.4659389667292018 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2061\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1633\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1452\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1354\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1301\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1233\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1199\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1157\n",
            "     train: 0.11373610685389506 \n",
            "validation: 0.3019816316139619 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1842\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1609\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.1509\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.1447\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1398\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1374\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1339\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1329\n",
            "     train: 0.13080077672701768 \n",
            "validation: 0.5168751013120448 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2739\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2450\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2291\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2186\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2120\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2071\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.2027\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2005\n",
            "     train: 0.19792626353496487 \n",
            "validation: 0.28874190657243354 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2349\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2050\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1932\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1851\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1789\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1755\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1721\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1692\n",
            "     train: 0.16721115785497417 \n",
            "validation: 0.20830463147253211 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1960\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1711\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1616\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1533\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1489\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1447\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1439\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1404\n",
            "     train: 0.13862197217082403 \n",
            "validation: 0.3045266581389361 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1547\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1202\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0987\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0884\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0820\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0760\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0721\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0693\n",
            "     train: 0.06703908906470389 \n",
            "validation: 0.20973928712094211 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1079\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0883\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0801\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0726\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0687\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0646\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0621\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0599\n",
            "     train: 0.058351106617707725 \n",
            "validation: 0.18719257575864479 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1074\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0836\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0733\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0670\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0625\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0598\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0577\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0561\n",
            "     train: 0.05476945998127005 \n",
            "validation: 0.20350269957422687 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1013\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0759\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0647\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0593\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0549\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0518\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0492\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0476\n",
            "     train: 0.046291718439197105 \n",
            "validation: 0.24728156662217107 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1063\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0736\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0611\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0537\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0491\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0455\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0430\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0411\n",
            "     train: 0.03986846450225994 \n",
            "validation: 0.2555060594686158 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1120\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 372ms/step - loss: 0.0879\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0791\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.0721\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.0677\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.0640\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0609\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0585\n",
            "     train: 0.056835409937405705 \n",
            "validation: 0.33584162742243645 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.1522\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1115\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.0970\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0879\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.0795\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.0757\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.0715\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.0693\n",
            "     train: 0.06752490727894547 \n",
            "validation: 0.2282722238840069 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.1281\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 368ms/step - loss: 0.0994\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.0861\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0808\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0734\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0716\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0677\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0653\n",
            "     train: 0.06287399800640327 \n",
            "validation: 0.22410847703157935 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1125\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0890\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0775\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0710\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0666\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0632\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0602\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0576\n",
            "     train: 0.055915388437362275 \n",
            "validation: 0.4446212981809377 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1963\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1390\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1164\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1052\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0939\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0863\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0807\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0779\n",
            "     train: 0.07442954756279045 \n",
            "validation: 0.21756890911439616 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1331\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1089\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0981\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0915\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0870\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0835\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0803\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0785\n",
            "     train: 0.07651921706835464 \n",
            "validation: 0.2354785375050417 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1444\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1098\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0951\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0876\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0817\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0778\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0752\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0729\n",
            "     train: 0.07082644711622714 \n",
            "validation: 0.20866127760084244 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0986\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0721\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0622\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0542\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0495\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0458\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0429\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0406\n",
            "     train: 0.03891328454871922 \n",
            "validation: 0.16956031586645506 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0788\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0535\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0433\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0381\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0341\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0304\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0283\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0265\n",
            "     train: 0.02526798163672407 \n",
            "validation: 1.0916610792803103 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.3276\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2464\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2289\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2085\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1901\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1777\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1699\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1640\n",
            "     train: 0.16001697584366617 \n",
            "validation: 0.7194790907211331 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.3810\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.3032\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2749\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2610\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2434\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2340\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2257\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2198\n",
            "     train: 0.21352266730052072 \n",
            "validation: 0.43950676058207705 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/8\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.3524\n",
            "Epoch 2/8\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.3157\n",
            "Epoch 3/8\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2930\n",
            "Epoch 4/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2787\n",
            "Epoch 5/8\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2702\n",
            "Epoch 6/8\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2587\n",
            "Epoch 7/8\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2527\n",
            "Epoch 8/8\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2477\n",
            "     train: 0.24281294882731644 \n",
            "validation: 2.2865537845496062 \n",
            "\n",
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.9509\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.9196\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.9121\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.9054\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.9024\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.9027\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.9008\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.9011\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.9023\n",
            "     train: 0.899828319141529 \n",
            "validation: 1.0628666374674438 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 1.1132\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.1129\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 1.1151\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 1.1140\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 1.1120\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 1.1138\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 1.1130\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.1110\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 1.1108\n",
            "     train: 1.1104365477447722 \n",
            "validation: 0.278919704973356 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.0486\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 1.0461\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 1.0445\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.0436\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 1.0427\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 1.0430\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.0420\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 1.0428\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 1.0415\n",
            "     train: 1.0401656516482676 \n",
            "validation: 0.5922468114275282 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.6264\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.8360\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.7254\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.6771\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.6504\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.6494\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.6340\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.6420\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.6468\n",
            "     train: 0.6356923282067369 \n",
            "validation: 0.3300225692739459 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.3976\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.3931\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.3988\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.4021\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.3938\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.3894\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.3858\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.3868\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3983\n",
            "     train: 0.39535498486403037 \n",
            "validation: 0.4891093889237889 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.4705\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.4448\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.4507\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.4485\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.4393\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.4388\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.4291\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.4296\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.4342\n",
            "     train: 0.44899062994406064 \n",
            "validation: 0.2056802817288802 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.3155\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2910\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2885\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2785\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2784\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2712\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2708\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2915\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2724\n",
            "     train: 0.2738368311702219 \n",
            "validation: 0.35770615447403986 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2844\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2764\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2604\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2544\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2646\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2741\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2646\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2690\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2548\n",
            "     train: 0.24951719076553688 \n",
            "validation: 0.3975503382248104 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2445\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2206\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2036\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1898\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1819\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1766\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1693\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1655\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1618\n",
            "     train: 0.15754333506772353 \n",
            "validation: 0.284196319742853 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2287\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2015\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1924\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1810\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1736\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1669\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1630\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1589\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1550\n",
            "     train: 0.15215768881090633 \n",
            "validation: 0.34960004159562924 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2140\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1778\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1612\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1539\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1460\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1425\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1382\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1340\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1317\n",
            "     train: 0.12961566516021134 \n",
            "validation: 0.2699588268102678 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1593\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1387\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1216\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1122\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1056\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1011\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0981\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0941\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0927\n",
            "     train: 0.08996364566460724 \n",
            "validation: 0.2338714535007632 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1429\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1235\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1160\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1050\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0975\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0931\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0882\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0843\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0818\n",
            "     train: 0.07989733594145045 \n",
            "validation: 0.37950659857952035 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1710\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1534\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1279\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1087\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1017\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0941\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0903\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0866\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0837\n",
            "     train: 0.07982729548076307 \n",
            "validation: 0.44115400958624834 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2076\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1765\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1511\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1353\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1251\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1168\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1119\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1074\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1035\n",
            "     train: 0.10110513924397381 \n",
            "validation: 0.3522813144988379 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1961\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1637\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1446\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1331\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1245\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1176\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1143\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1077\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1051\n",
            "     train: 0.1019057777254021 \n",
            "validation: 0.3424022410502279 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1873\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1505\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1344\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1258\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1179\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1124\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1085\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1040\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1012\n",
            "     train: 0.09886024351098045 \n",
            "validation: 0.6615035161611054 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2787\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1932\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1720\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1554\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1445\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1350\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1290\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1256\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1224\n",
            "     train: 0.1193420631979622 \n",
            "validation: 0.4087055204949334 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2240\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1923\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1748\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1639\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1559\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1504\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1465\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1428\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1394\n",
            "     train: 0.13742782373068224 \n",
            "validation: 0.7009072256591983 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.3424\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2891\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2712\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2518\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2384\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2307\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2244\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2182\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2150\n",
            "     train: 0.2105259332456911 \n",
            "validation: 0.39581146396538974 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2844\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2470\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2261\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.2127\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2038\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1961\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1915\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1870\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1837\n",
            "     train: 0.18032001436118453 \n",
            "validation: 0.3832475026032703 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2612\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2149\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1989\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1823\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1754\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1672\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1612\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1563\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1563\n",
            "     train: 0.15111418142431993 \n",
            "validation: 0.43126249904377034 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2021\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1548\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1307\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1143\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1039\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0955\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0892\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0843\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0814\n",
            "     train: 0.07809806003689508 \n",
            "validation: 0.3970296549570101 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1776\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1281\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1105\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0986\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0910\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0828\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0783\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0745\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0711\n",
            "     train: 0.06926814639219551 \n",
            "validation: 0.21996391079614294 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1282\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1028\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0877\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0812\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0749\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0706\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0675\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0655\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0634\n",
            "     train: 0.06204230144331745 \n",
            "validation: 0.3089842748203933 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1438\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1117\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0924\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0803\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0734\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0682\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0635\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0602\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0576\n",
            "     train: 0.055967414390615616 \n",
            "validation: 0.3340497779056433 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1426\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1073\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0884\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0774\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0687\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0634\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0586\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0553\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0523\n",
            "     train: 0.050459940843666375 \n",
            "validation: 0.3745064431223077 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1606\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1302\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1082\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0967\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0878\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0810\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0764\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0724\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0694\n",
            "     train: 0.0671407004473225 \n",
            "validation: 0.5674522851754045 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2345\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1596\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1350\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1179\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1122\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0988\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0917\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0862\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0826\n",
            "     train: 0.07984075525437896 \n",
            "validation: 0.37045687270637523 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1879\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1464\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1181\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1042\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0950\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0879\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0826\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0792\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0763\n",
            "     train: 0.07383067968199605 \n",
            "validation: 0.30830500550920287 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1485\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1166\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1040\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0916\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0837\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0773\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0730\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0692\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0660\n",
            "     train: 0.06398002897850806 \n",
            "validation: 0.8331920011777437 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.3282\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1863\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1469\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1304\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1151\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1029\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0980\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0922\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0877\n",
            "     train: 0.08355918593791715 \n",
            "validation: 0.42031313438582546 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2032\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1557\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1264\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1193\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1076\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1005\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0961\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0916\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0879\n",
            "     train: 0.08579689418779796 \n",
            "validation: 0.3971396822428143 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2041\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1562\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1374\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1219\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1114\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1013\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0953\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0896\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0861\n",
            "     train: 0.08310532229574766 \n",
            "validation: 0.41390772187660235 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1774\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1239\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0918\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0772\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0674\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0627\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0566\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0527\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0494\n",
            "     train: 0.04736052211539 \n",
            "validation: 0.2921484481539732 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1255\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0879\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0692\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0574\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0496\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0440\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0402\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0372\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0347\n",
            "     train: 0.03310322558812114 \n",
            "validation: 1.9149812272399602 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.5951\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.3864\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2919\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2561\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2430\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2136\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1940\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1825\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1751\n",
            "     train: 0.17027812106788764 \n",
            "validation: 1.0387654191022013 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.4937\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.4172\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.3502\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.3231\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2907\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2678\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2540\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2426\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2350\n",
            "     train: 0.2292684843636349 \n",
            "validation: 0.660747773148789 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/9\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.4360\n",
            "Epoch 2/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3581\n",
            "Epoch 3/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3237\n",
            "Epoch 4/9\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2990\n",
            "Epoch 5/9\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2867\n",
            "Epoch 6/9\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2753\n",
            "Epoch 7/9\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2678\n",
            "Epoch 8/9\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2608\n",
            "Epoch 9/9\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2565\n",
            "     train: 0.2521531962151061 \n",
            "validation: 2.228830601546823 \n",
            "\n",
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.9975\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.9286\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.9189\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.9166\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.9130\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.9115\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.9033\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.9101\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.9044\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.9018\n",
            "     train: 0.8998910582030736 \n",
            "validation: 1.0651556184308975 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 1.1158\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 1.1178\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 1.1154\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.1140\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 1.1138\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 1.1142\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 1.1126\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 1.1133\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 1.1144\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 1.1110\n",
            "     train: 1.1115867597620992 \n",
            "validation: 0.2807000386288859 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 1.0533\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 1.0501\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 1.0488\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 1.0461\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 1.0518\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 1.0483\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 1.0473\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 1.0467\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 1.0451\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 1.0449\n",
            "     train: 1.0442833707051384 \n",
            "validation: 0.6036822638225222 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.6335\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.6306\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.6284\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.6269\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.6292\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.6305\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.6279\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.6270\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.6269\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.6270\n",
            "     train: 0.625955019405288 \n",
            "validation: 0.321998332916331 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3914\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3890\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3883\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.3880\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.3868\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.3871\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.3864\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3862\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.3859\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.3865\n",
            "     train: 0.385580753328374 \n",
            "validation: 0.4398593262771064 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.4455\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.4381\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.4344\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.4342\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.4315\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.4314\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.4309\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.4307\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.4306\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.4302\n",
            "     train: 0.4294474509796103 \n",
            "validation: 0.28712285331783177 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.3351\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2902\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2804\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2739\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2691\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2674\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2636\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2621\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2613\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2604\n",
            "     train: 0.2594526081758312 \n",
            "validation: 0.36874653007559804 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2810\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2503\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2416\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2332\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2274\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.2227\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2205\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2174\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2142\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2118\n",
            "     train: 0.21052419756485052 \n",
            "validation: 0.327978209956773 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1900\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1693\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1529\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1455\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1405\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1368\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1342\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1325\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1304\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 0.1290\n",
            "     train: 0.12783619430085974 \n",
            "validation: 0.24236827120400709 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1882\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1608\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1545\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1496\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1451\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 345ms/step - loss: 0.1402\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1374\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1351\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1337\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1323\n",
            "     train: 0.1309442612184824 \n",
            "validation: 0.2969927436627278 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1781\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1480\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1382\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1324\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1268\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1237\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1205\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.1183\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1166\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1163\n",
            "     train: 0.1144075962305841 \n",
            "validation: 0.27462340860520235 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1528\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1216\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1056\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0986\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0926\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0886\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0855\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0835\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0815\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0820\n",
            "     train: 0.07955242509131372 \n",
            "validation: 0.2050463719399298 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1222\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1030\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0907\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0836\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0798\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0764\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0738\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0715\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0697\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0683\n",
            "     train: 0.06880850786024743 \n",
            "validation: 0.31713607370179026 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1400\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1111\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0936\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0856\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0799\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0761\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0730\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0702\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0683\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0666\n",
            "     train: 0.06533848339073732 \n",
            "validation: 0.3255726219774336 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1574\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1291\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1161\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1060\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0996\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0961\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0921\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0895\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0868\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0851\n",
            "     train: 0.08357207717500109 \n",
            "validation: 0.27815675072036145 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1584\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1287\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1160\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1085\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1024\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0971\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0941\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0916\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0902\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0883\n",
            "     train: 0.08678354095474827 \n",
            "validation: 0.2508006799236023 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1445\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1199\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1075\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1016\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0971\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0944\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0912\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0898\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0883\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0868\n",
            "     train: 0.08580506332492778 \n",
            "validation: 0.4272334761393408 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1918\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1565\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1397\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1312\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1223\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.1186\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1147\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1127\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1105\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1089\n",
            "     train: 0.10767534303059552 \n",
            "validation: 0.28925177756050374 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1772\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1604\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1505\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1420\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1381\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1347\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.1318\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1299\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1278\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1262\n",
            "     train: 0.12506594905096233 \n",
            "validation: 0.5535539420377624 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2826\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.2491\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2309\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.2189\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.2104\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2052\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2016\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1989\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.1972\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1942\n",
            "     train: 0.19298087903638192 \n",
            "validation: 0.265092218173526 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2263\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2037\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1925\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.1839\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1782\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1742\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1712\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1686\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1661\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1649\n",
            "     train: 0.16280415679807977 \n",
            "validation: 0.29785060608688674 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2205\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1848\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1691\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1576\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1512\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1467\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1433\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1402\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1382\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1363\n",
            "     train: 0.1348092655032901 \n",
            "validation: 0.3251226178309366 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1596\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1185\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1013\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0891\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0800\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0757\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0701\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0674\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0650\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0630\n",
            "     train: 0.06129094641054719 \n",
            "validation: 0.26008446660864376 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1223\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0931\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0791\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0720\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0670\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0629\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0600\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0582\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0565\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0551\n",
            "     train: 0.05417488829399392 \n",
            "validation: 0.19952328274506825 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1096\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0809\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0706\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0655\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0613\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0585\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0564\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0548\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0535\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0525\n",
            "     train: 0.05165513914144257 \n",
            "validation: 0.22274673442087678 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1069\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0790\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0668\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0606\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0564\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0525\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0502\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0480\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0465\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0453\n",
            "     train: 0.044522736427455384 \n",
            "validation: 0.24248467125396703 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1056\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0782\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0638\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0558\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0508\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0468\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0443\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0420\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0404\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0390\n",
            "     train: 0.03812099240370099 \n",
            "validation: 0.28215686740173496 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1205\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0933\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0797\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0708\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0649\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0610\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0579\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0559\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0542\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0531\n",
            "     train: 0.05199887646763165 \n",
            "validation: 0.3459812817276651 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.1511\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1105\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0959\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0872\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0797\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0749\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0715\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0689\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0671\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0656\n",
            "     train: 0.06442068652225515 \n",
            "validation: 0.23181376759240035 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1307\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.1001\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0829\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0762\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0708\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0668\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0645\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0625\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0605\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0589\n",
            "     train: 0.05846443105546273 \n",
            "validation: 0.22554399420551804 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.1097\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0826\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0740\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 346ms/step - loss: 0.0668\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0622\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0585\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0568\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 348ms/step - loss: 0.0544\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0528\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0513\n",
            "     train: 0.05037594026663874 \n",
            "validation: 0.4184886358190818 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1872\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.1237\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.1130\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0959\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0889\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.0820\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 350ms/step - loss: 0.0768\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 347ms/step - loss: 0.0740\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 369ms/step - loss: 0.0706\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0689\n",
            "     train: 0.0672453653012068 \n",
            "validation: 0.28417809792811727 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 364ms/step - loss: 0.1467\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.1150\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1035\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0965\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0880\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0821\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.0778\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0756\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0737\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 366ms/step - loss: 0.0721\n",
            "     train: 0.07100406476030174 \n",
            "validation: 0.24185206841489978 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.1422\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.1097\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 360ms/step - loss: 0.0951\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 365ms/step - loss: 0.0862\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 363ms/step - loss: 0.0812\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0765\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0740\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0709\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0690\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0676\n",
            "     train: 0.06632716639160506 \n",
            "validation: 0.21245385325054242 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0980\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.0680\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.0570\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0505\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0459\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0424\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0393\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 361ms/step - loss: 0.0372\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0359\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0346\n",
            "     train: 0.033276036168035567 \n",
            "validation: 0.1751674931364634 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.0768\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0538\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.0431\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.0372\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.0328\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.0298\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0271\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.0252\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.0239\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.0226\n",
            "     train: 0.02164681403957859 \n",
            "validation: 1.0416477453275907 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.3345\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2545\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2089\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1856\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1780\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.1653\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1612\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.1548\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.1516\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.1489\n",
            "     train: 0.14688947016656856 \n",
            "validation: 0.6979076181882081 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.3628\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2839\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 362ms/step - loss: 0.2685\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2438\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 359ms/step - loss: 0.2320\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2229\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2164\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 355ms/step - loss: 0.2122\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 357ms/step - loss: 0.2074\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2038\n",
            "     train: 0.19959560707834978 \n",
            "validation: 0.4133576172088007 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.3315\n",
            "Epoch 2/10\n",
            "6/6 [==============================] - 2s 358ms/step - loss: 0.2940\n",
            "Epoch 3/10\n",
            "6/6 [==============================] - 2s 349ms/step - loss: 0.2763\n",
            "Epoch 4/10\n",
            "6/6 [==============================] - 2s 352ms/step - loss: 0.2615\n",
            "Epoch 5/10\n",
            "6/6 [==============================] - 2s 354ms/step - loss: 0.2519\n",
            "Epoch 6/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2456\n",
            "Epoch 7/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2406\n",
            "Epoch 8/10\n",
            "6/6 [==============================] - 2s 351ms/step - loss: 0.2358\n",
            "Epoch 9/10\n",
            "6/6 [==============================] - 2s 356ms/step - loss: 0.2333\n",
            "Epoch 10/10\n",
            "6/6 [==============================] - 2s 353ms/step - loss: 0.2316\n",
            "     train: 0.22818983234282775 \n",
            "validation: 2.325880029341041 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "KF8GmiGBmgC3",
        "outputId": "48abd90c-5c05-4fee-ba3a-48afdfe46dd3"
      },
      "source": [
        "pd.DataFrame(epoch_tuning_performance)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.705335</td>\n",
              "      <td>0.757812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.486383</td>\n",
              "      <td>0.834476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.375779</td>\n",
              "      <td>0.658323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.315039</td>\n",
              "      <td>0.549830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.269697</td>\n",
              "      <td>0.497716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.238167</td>\n",
              "      <td>0.476488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.218421</td>\n",
              "      <td>0.436305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.204912</td>\n",
              "      <td>0.394862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.216120</td>\n",
              "      <td>0.523828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.201085</td>\n",
              "      <td>0.406803</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1\n",
              "0  0.705335  0.757812\n",
              "1  0.486383  0.834476\n",
              "2  0.375779  0.658323\n",
              "3  0.315039  0.549830\n",
              "4  0.269697  0.497716\n",
              "5  0.238167  0.476488\n",
              "6  0.218421  0.436305\n",
              "7  0.204912  0.394862\n",
              "8  0.216120  0.523828\n",
              "9  0.201085  0.406803"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khd0KaSBlEWX"
      },
      "source": [
        "pd.DataFrame(epoch_tuning_performance).to_csv('epoch_tuning.csv')"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "_ei6_iuM8RUB",
        "outputId": "f57b7484-ed39-40da-92bb-af806dea3a2d"
      },
      "source": [
        "pd.DataFrame(scaler.inverse_transform(val_pred)+1).head(900)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000566</td>\n",
              "      <td>1.002740</td>\n",
              "      <td>0.996617</td>\n",
              "      <td>0.997220</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>0.998195</td>\n",
              "      <td>1.001630</td>\n",
              "      <td>1.000635</td>\n",
              "      <td>0.998809</td>\n",
              "      <td>1.002897</td>\n",
              "      <td>1.000873</td>\n",
              "      <td>0.998328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000566</td>\n",
              "      <td>1.002740</td>\n",
              "      <td>0.996617</td>\n",
              "      <td>0.997220</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>0.998195</td>\n",
              "      <td>1.001630</td>\n",
              "      <td>1.000635</td>\n",
              "      <td>0.998809</td>\n",
              "      <td>1.002897</td>\n",
              "      <td>1.000873</td>\n",
              "      <td>0.998328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000566</td>\n",
              "      <td>1.002740</td>\n",
              "      <td>0.996617</td>\n",
              "      <td>0.997220</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>0.998195</td>\n",
              "      <td>1.001630</td>\n",
              "      <td>1.000635</td>\n",
              "      <td>0.998809</td>\n",
              "      <td>1.002897</td>\n",
              "      <td>1.000873</td>\n",
              "      <td>0.998328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.000566</td>\n",
              "      <td>1.002740</td>\n",
              "      <td>0.996617</td>\n",
              "      <td>0.997220</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>0.998195</td>\n",
              "      <td>1.001630</td>\n",
              "      <td>1.000635</td>\n",
              "      <td>0.998809</td>\n",
              "      <td>1.002897</td>\n",
              "      <td>1.000873</td>\n",
              "      <td>0.998328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000566</td>\n",
              "      <td>1.002740</td>\n",
              "      <td>0.996617</td>\n",
              "      <td>0.997220</td>\n",
              "      <td>1.001823</td>\n",
              "      <td>0.998195</td>\n",
              "      <td>1.001630</td>\n",
              "      <td>1.000635</td>\n",
              "      <td>0.998809</td>\n",
              "      <td>1.002897</td>\n",
              "      <td>1.000873</td>\n",
              "      <td>0.998328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>1.013466</td>\n",
              "      <td>1.012343</td>\n",
              "      <td>1.011132</td>\n",
              "      <td>1.012748</td>\n",
              "      <td>1.018925</td>\n",
              "      <td>1.015822</td>\n",
              "      <td>1.010650</td>\n",
              "      <td>1.013788</td>\n",
              "      <td>1.014109</td>\n",
              "      <td>1.009295</td>\n",
              "      <td>1.012848</td>\n",
              "      <td>1.012556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>0.999925</td>\n",
              "      <td>0.999567</td>\n",
              "      <td>0.995657</td>\n",
              "      <td>0.999254</td>\n",
              "      <td>1.002530</td>\n",
              "      <td>0.999703</td>\n",
              "      <td>1.000407</td>\n",
              "      <td>0.999612</td>\n",
              "      <td>0.997348</td>\n",
              "      <td>1.000844</td>\n",
              "      <td>0.998583</td>\n",
              "      <td>1.000861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>1.003549</td>\n",
              "      <td>1.003494</td>\n",
              "      <td>0.997377</td>\n",
              "      <td>0.999306</td>\n",
              "      <td>1.007671</td>\n",
              "      <td>1.003728</td>\n",
              "      <td>1.002894</td>\n",
              "      <td>1.008021</td>\n",
              "      <td>0.999810</td>\n",
              "      <td>1.006493</td>\n",
              "      <td>1.004335</td>\n",
              "      <td>1.004705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>1.015372</td>\n",
              "      <td>1.021024</td>\n",
              "      <td>1.013621</td>\n",
              "      <td>1.017878</td>\n",
              "      <td>1.013836</td>\n",
              "      <td>1.016712</td>\n",
              "      <td>1.018253</td>\n",
              "      <td>0.997044</td>\n",
              "      <td>1.016587</td>\n",
              "      <td>1.007255</td>\n",
              "      <td>1.008261</td>\n",
              "      <td>1.001904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>0.999925</td>\n",
              "      <td>0.999567</td>\n",
              "      <td>0.995657</td>\n",
              "      <td>0.999254</td>\n",
              "      <td>1.002530</td>\n",
              "      <td>0.999703</td>\n",
              "      <td>1.000407</td>\n",
              "      <td>0.999612</td>\n",
              "      <td>0.997348</td>\n",
              "      <td>1.000844</td>\n",
              "      <td>0.998583</td>\n",
              "      <td>1.000861</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2   ...        9         10        11\n",
              "0    1.000566  1.002740  0.996617  ...  1.002897  1.000873  0.998328\n",
              "1    1.000566  1.002740  0.996617  ...  1.002897  1.000873  0.998328\n",
              "2    1.000566  1.002740  0.996617  ...  1.002897  1.000873  0.998328\n",
              "3    1.000566  1.002740  0.996617  ...  1.002897  1.000873  0.998328\n",
              "4    1.000566  1.002740  0.996617  ...  1.002897  1.000873  0.998328\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "895  1.013466  1.012343  1.011132  ...  1.009295  1.012848  1.012556\n",
              "896  0.999925  0.999567  0.995657  ...  1.000844  0.998583  1.000861\n",
              "897  1.003549  1.003494  0.997377  ...  1.006493  1.004335  1.004705\n",
              "898  1.015372  1.021024  1.013621  ...  1.007255  1.008261  1.001904\n",
              "899  0.999925  0.999567  0.995657  ...  1.000844  0.998583  1.000861\n",
              "\n",
              "[900 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    }
  ]
}