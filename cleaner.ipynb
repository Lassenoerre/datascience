{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cleaner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9RyVkMQTELB"
      },
      "source": [
        "Import relevant packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVXfrsUmTCT_"
      },
      "source": [
        "import spacy\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLIwvgKTTRzh"
      },
      "source": [
        "Define a function to remove special unicode characters and other non-relevant N-grams from each article:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSit01rETd1Y"
      },
      "source": [
        "def remove_clutter(text):\n",
        "    #Trying to remove special unicode characters\n",
        "    text = re.sub(r'\\\\x[A-Za-z0-9_]{2}', '', text)\n",
        "    \n",
        "    #Trying to remove video annotation\n",
        "    text = re.sub(r'VIDEO([0-9]|[0-9]{2}):[0-9]{4}:[0-9]{2}', ' ', text)\n",
        "    \n",
        "    # Remove all \"â\" with space:\n",
        "    text = re.sub(r'â',' ', text)\n",
        "    text = re.sub(r'Â',' ', text)\n",
        "    \n",
        "    # Remove all \"Getty images\":\n",
        "    text = re.sub(r'getty imgages','', text)\n",
        "    return text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3WibnqLTp7l"
      },
      "source": [
        "Load the English language model instance in spaCy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSFYOvRdTpfK"
      },
      "source": [
        "nlp = spacy.load('en_core_web_lg')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs-y6HrzUPXv"
      },
      "source": [
        "Define a function to clean all our articles in a specific column in a dataset and return the tokenized data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isbwqt0VUTx4"
      },
      "source": [
        "def cleaning(df,column):\n",
        "    \n",
        "    # Create an empty list to store tokens:\n",
        "    tokens = []\n",
        "\n",
        "    #df[column].map(lambda article: remove_clutter(article))\n",
        "\n",
        "    ## Apply the remove_clutter function on all articles in the df:\n",
        "    df[column].apply(remove_clutter())\n",
        "    \n",
        "    # Iterate over all articles in the dataframe and create a nlp object for each:\n",
        "    for article in nlp.pipe(df[column], disable=[\"tagger\", \"parser\"]):\n",
        "        \n",
        "        # Store all cleaned tokens in a list using list comprehension:\n",
        "        article_tok = [token.lemma_.lower() for token in article if not token.is_stop and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] and token.ent_type_ not in [\"PERSON\", \"MONEY\", \"PERCENT\", \"LOC\", \"DATE\", \"TIME\", \"QUANTITY\", \"ORDINAL\"]]:\n",
        "    \n",
        "        # Insert list of tokens into the list \"tokens\":\n",
        "        tokens.append(article_tok)\n",
        "        \n",
        "    # Insert a column in the dataframe with all tokens for each article:\n",
        "    df['tokens'] = tokens\n",
        "    \n",
        "    # Insert tokens as a string to the dataframe:\n",
        "    df[\"clean_articles\"] = data[\"tokens\"].map(lambda row: \" \".join(row))\n",
        "\n",
        "    # Return the df\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTSZJ5d7UtCw"
      },
      "source": [
        "Call the cleaning function of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "cjUSjm71Uu_o",
        "outputId": "fc7f4611-36af-4f71-e22a-0d2c5bcb15d4"
      },
      "source": [
        "cleaning(df,'articles')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6b1de1b1f525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'articles'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'cleaning' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgaWVxIBUx4S"
      },
      "source": [
        "Example: How to entity recognition work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9U88Q2lXmn1"
      },
      "source": [
        "doc = nlp(\"SOME TEXT\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H1CIPugV9zM"
      },
      "source": [
        "sentence_spans = list(doc.sents)\n",
        "displacy.serve(sentence_spans, style=\"dep\")\n",
        "\n",
        "displacy.serve(doc, style=\"dep\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKJyCg0oUxMg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}