{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9RyVkMQTELB"
   },
   "source": [
    "Import relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XVXfrsUmTCT_"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "OG7liUQAaQ2O",
    "outputId": "6b102802-4eb0-4543-bcd6-9634904f1a53"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8fe54770d373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://raw.githubusercontent.com/Lassenoerre/datascience/master/datasets/articles_0_6000.csv?token=ARZVGJ34RODPQRSDIMC6LAC7XE3II'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'URL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Lassenoerre/datascience/master/datasets/articles_0_6000.csv?token=ARZVGJ34RODPQRSDIMC6LAC7XE3II', header=0, names=['Title', 'Topic', 'Date', 'Content', 'URL'], usecols=[1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "kOeoqD4_bMLW",
    "outputId": "7d2987c6-57eb-4f97-a543-adea262f052f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Date</th>\n",
       "      <th>Content</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shopping Malls Tracking Holiday Sales</td>\n",
       "      <td>Street Signs</td>\n",
       "      <td>27-11-2006</td>\n",
       "      <td>Its not only retailers who are closely watchin...</td>\n",
       "      <td>https://www.cnbc.com/2006/11/27/shopping-malls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Consumers Adore Gift Cards</td>\n",
       "      <td>U.S. News</td>\n",
       "      <td>27-11-2006</td>\n",
       "      <td>The National Retail Federation\\xe2\\x80\\x99s Ho...</td>\n",
       "      <td>https://www.cnbc.com/2006/11/27/consumers-ador...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What's Wrong With Wal-Mart?</td>\n",
       "      <td>Street Signs</td>\n",
       "      <td>27-11-2006</td>\n",
       "      <td>Despite the fine weather in much of the nation...</td>\n",
       "      <td>https://www.cnbc.com/2006/11/27/whats-wrong-wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shape Up, Ship Out</td>\n",
       "      <td>Holiday Central</td>\n",
       "      <td>27-11-2006</td>\n",
       "      <td>It helped nail the coffin shut on eToys. It\\xe...</td>\n",
       "      <td>https://www.cnbc.com/2006/11/27/shape-up-ship-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Box Boom: Gadgets Sell \\xe2\\x80\\x93 And Pr...</td>\n",
       "      <td>Holiday Central</td>\n",
       "      <td>27-11-2006</td>\n",
       "      <td>There\\xe2\\x80\\x99s plenty of time to shop for ...</td>\n",
       "      <td>https://www.cnbc.com/2006/11/27/big-box-boom-g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  ...                                                URL\n",
       "0              Shopping Malls Tracking Holiday Sales  ...  https://www.cnbc.com/2006/11/27/shopping-malls...\n",
       "1                        Consumers Adore Gift Cards   ...  https://www.cnbc.com/2006/11/27/consumers-ador...\n",
       "2                        What's Wrong With Wal-Mart?  ...  https://www.cnbc.com/2006/11/27/whats-wrong-wi...\n",
       "3                                 Shape Up, Ship Out  ...  https://www.cnbc.com/2006/11/27/shape-up-ship-...\n",
       "4  Big Box Boom: Gadgets Sell \\xe2\\x80\\x93 And Pr...  ...  https://www.cnbc.com/2006/11/27/big-box-boom-g...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpYRQLXLydwg"
   },
   "source": [
    "Convert all article content to stringe type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i7jyD2MteVak"
   },
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NmHXyLfy5aw"
   },
   "source": [
    "Replace all article content with the title if the title is longer than the article content: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "04jQFvZIpUs5"
   },
   "outputs": [],
   "source": [
    "for index in df.index:\n",
    "  if len(df['Content'][index]) < len(df['Title'][index]):\n",
    "    df['Content'][index]=df['Title'][index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLIwvgKTTRzh"
   },
   "source": [
    "Define a function to remove special unicode characters and other non-relevant N-grams from each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "iSit01rETd1Y"
   },
   "outputs": [],
   "source": [
    "def remove_clutter(text):\n",
    "\n",
    "    #Trying to remove special unicode characters\n",
    "    #text = re.sub(r'\\\\x[A-Za-z0-9_]{2}', '', text)\n",
    "    \n",
    "    #Trying to remove video annotation\n",
    "    text = re.sub(r'VIDEO([0-9]|[0-9]{2}):[0-9]{4}:[0-9]{2}', ' ', text)\n",
    "\n",
    "    # Remove unicode characters:\n",
    "    text = ''.join([x for x in text if ord(x) < 127])\n",
    "    \n",
    "    # Remove all \"Getty images\":\n",
    "    text = re.sub(r'getty images','', text) \n",
    "\n",
    "    # Remove commas:\n",
    "    text = re.sub(r',','', text)\n",
    "\n",
    "    # Replace abbrevations:\n",
    "    text = re.sub(r\"don't\",\"dont\", text)\n",
    "    text = re.sub(r\"can't\",\"cant\", text)\n",
    "    text = re.sub(r\"weren't\",\"werent\", text)\n",
    "    text = re.sub(r\"couldn't\",\"couldnt\", text)\n",
    "    text = re.sub(r\"doesn't\",\"doesnt\", text)\n",
    "    text = re.sub(r\"didn't\",\"didnt\", text)\n",
    "    \n",
    "     # Remove all double-spaces:\n",
    "    text = re.sub(r'  ',' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'didnt cant werent getty imgages  '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_clutter(\"didn't can't weren't getty imgages  â Â \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3WibnqLTp7l"
   },
   "source": [
    "Load the English language model instance in spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dSFYOvRdTpfK"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hs-y6HrzUPXv"
   },
   "source": [
    "Define a function to clean all our articles in a specific column in a dataset and return the tokenized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "isbwqt0VUTx4"
   },
   "outputs": [],
   "source": [
    "def cleaning(df,column):\n",
    "    \n",
    "    # Create an empty list to store tokens:\n",
    "    tokens = []\n",
    "\n",
    "    # Apply the remove_clutter function on all articles in the df:\n",
    "    df[column].apply(remove_clutter)\n",
    "\n",
    "    # Define an object to count the progress:\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate over all articles in the dataframe and create a nlp object for each:\n",
    "    for article in nlp.pipe(df[column], disable=[\"parser\"]):\n",
    "        \n",
    "        # Store all cleaned tokens in a list using list comprehension:\n",
    "        article_tok = [token.lemma_.lower() for token in article if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] and token.ent_type_ not in ['PERSON', 'MONEY', 'PERCENT', 'LOC', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL'] and len(token)>1]\n",
    "        \n",
    "        # Insert list of tokens into the list \"tokens\":\n",
    "        tokens.append(article_tok)\n",
    "\n",
    "        # Print the progress:\n",
    "        count += 1\n",
    "        print(f'processed {count}/{len(df[column])}')\n",
    "        \n",
    "    # Insert a column in the dataframe with all tokens for each article:\n",
    "    df['tokens'] = tokens\n",
    "    \n",
    "    # Insert tokens as a string in a new column in the dataframe:\n",
    "    df[\"clean_articles\"] = df[\"tokens\"].map(lambda row: \" \".join(row))\n",
    "\n",
    "    # Return the df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e168f726604b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0marticle_tok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alpha\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stop\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PROPN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ment_type_\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'PERSON'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MONEY'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PERCENT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LOC'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'DATE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TIME'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'QUANTITY'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ORDINAL'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0marticle_string\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0marticle_tok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle_tok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "test = \"this is a test, Frederik, Wednesday, 100m USD, tomorrow, najarian\"\n",
    "article = nlp(test)\n",
    "\n",
    "def test:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] and token.ent_type_ not in ['PERSON', 'MONEY', 'PERCENT', 'LOC', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL'] and len(token)>1:\n",
    "            print(toke.text)\n",
    "\n",
    "    \n",
    "    article_tok = [token.lemma_.lower() for token in article if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] and token.ent_type_ not in ['PERSON', 'MONEY', 'PERCENT', 'LOC', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL'] and len(token)>1]\n",
    "\n",
    "article_string =  \"\"\n",
    "\n",
    "for token in article_tok:\n",
    "    article_string.join()\n",
    "article_tok.map(lambda row: \" \".join(row))\n",
    "\n",
    "for token in article_tok:\n",
    "    nlp(token)\n",
    "    print(token.text, token.ent_type_)\n",
    "\n",
    "\n",
    "#[tok.lemma_ for tok in nlp(article_tok)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rejected this, DET, \n",
      "2 rejected is, AUX, \n",
      "3 rejected a, DET, \n",
      "4 test NOUN \n",
      "5 rejected ,, PUNCT, \n",
      "6 rejected Frederik, PROPN, PERSON\n",
      "7 rejected ,, PUNCT, \n",
      "8 rejected Wednesday, PROPN, DATE\n",
      "9 rejected ,, PUNCT, \n",
      "10 rejected 100, NUM, CARDINAL\n",
      "11 rejected m, PROPN, \n",
      "12 USD NOUN \n",
      "13 rejected ,, PUNCT, \n",
      "14 rejected tomorrow, NOUN, DATE\n",
      "15 rejected ,, PUNCT, \n",
      "16 najarian PROPN ORG\n",
      "17 rejected ,, PUNCT, \n",
      "18 rejected ijr, PROPN, PERSON\n",
      "19 rejected finerman, PROPN, PERSON\n",
      "20 rejected ,, PUNCT, \n",
      "21 rejected dis, PROPN, PERSON\n",
      "22 rejected ,, PUNCT, \n",
      "23 rejected xhb, PROPN, PERSON\n",
      "24 rejected finerman, PROPN, PERSON\n",
      "25 rejected ,, PUNCT, \n",
      "26 rejected Frederik, PROPN, PERSON\n",
      "27 rejected Permin, PROPN, PERSON\n"
     ]
    }
   ],
   "source": [
    "def test(text):\n",
    "    doc = nlp(text)\n",
    "    for i, token in enumerate(doc, start=1):\n",
    "        if token.is_alpha and not token.is_stop and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] and token.ent_type_ not in ['PERSON', 'MONEY', 'PERCENT', 'LOC', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL'] and len(token)>1:\n",
    "            print(i, token.text, token.pos_, token.ent_type_)\n",
    "        else:\n",
    "            print(i, f\"rejected {token.text}, {token.pos_}, {token.ent_type_}\")\n",
    "\n",
    "test(\"this is a test, Frederik, Wednesday, 100m USD, tomorrow, najarian, ijr finerman, dis, xhb finerman, Frederik Permin\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTSZJ5d7UtCw"
   },
   "source": [
    "Call the cleaning function on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "cjUSjm71Uu_o",
    "outputId": "8e1c6434-1314-4b29-fbfc-249def4b4496"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-899569e48b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "cleaning(df,'Content')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgaWVxIBUx4S"
   },
   "source": [
    "Example: How to entity recognition work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "S6DEVE7xoKil",
    "outputId": "36a441f7-938d-459f-dee8-04528949875e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'https://www.cnbc.com/2007/06/06/prudential-shuts-down-stock-research-trading-arm.html'"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['URL'][5167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "H9U88Q2lXmn1"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"don't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ev5dBbtHsJ_i",
    "outputId": "4cbb9285-497d-4b18-d423-d5bc8e0cc958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do\n",
      "n't\n"
     ]
    }
   ],
   "source": [
    "for tok in doc:\n",
    "  print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H1CIPugV9zM"
   },
   "outputs": [],
   "source": [
    "sentence_spans = list(doc.sents)\n",
    "displacy.serve(sentence_spans, style=\"dep\")\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKJyCg0oUxMg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cleaner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
