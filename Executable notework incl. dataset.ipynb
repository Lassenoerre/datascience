{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Executable notebook copy.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L5OX6x3b6gG6",
        "csrQZGwb6gG6",
        "KdU515V_6gG7",
        "nNCCxlAH6gG7",
        "ocK1JaFC6gG7",
        "mFofqhZD6gG7",
        "3rhuviNn6gG7",
        "85amaHml6gG7",
        "DofYZs246gG7",
        "mGpliYIr6gG7",
        "bMNWr5OR6gG7"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5OX6x3b6gG6"
      },
      "source": [
        "# 1. Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csrQZGwb6gG6"
      },
      "source": [
        "## 1.1 Crawling CNBC to get the article links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSwsI-856gG6"
      },
      "source": [
        "def get_links_to_scrape(start_year, end_year):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a starting year and an ending year and crawls through\n",
        "    CNBC's SiteMap structure to find all links to articles that are written\n",
        "    in the given time period.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import of the required packages\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup, SoupStrainer\n",
        "\n",
        "    # First, we define every month and year that should be scraped.\n",
        "    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "    years = [year for year in range(start_year, end_year+1)]\n",
        "\n",
        "    # In order to scrape the links faster, we make use of SoupStrainer\n",
        "    # to only parse the relevant part of the site: ALl the article links.\n",
        "    strainer = SoupStrainer('a', class_ = 'SiteMapArticleList-link')\n",
        "\n",
        "    # We instantiate an empty list that we can store all the links in.\n",
        "    article_links = []\n",
        "\n",
        "    # For every day, in every month in every year, construct a link\n",
        "    # that should be scraped. We do this, because CNBC's link structure\n",
        "    # in their archive is given af .../site-map/YEAR/MONTH/DAY_OF_MONTH.\n",
        "    for year in years:\n",
        "        for month in months:\n",
        "            for day in range(1,32):\n",
        "                link = f'https://www.cnbc.com/site-map/{year}/{month}/{day}/'\n",
        "\n",
        "                # Take the generated link and load it in through requests,\n",
        "                # then take the contents and convert it to a string.\n",
        "                page = str(requests.get(link).content)\n",
        "\n",
        "                # CNBC load in their CSS directly into the HTML, with usually\n",
        "                # would make it very heavy to scrape. We can fix this, simply\n",
        "                # by cutting remove the entire <head></head> part of the html.\n",
        "                page = page[str(page).find('body'):]\n",
        "\n",
        "                # Now, pass the remaining content through BeautifulSoup's parser\n",
        "                # and apply the strainer. This means it will only parse link\n",
        "                # elements with the class of SiteMapArticleList-Link and it\n",
        "                # will only search through body-section of the document.\n",
        "                soup = BeautifulSoup(page, parse_only = strainer)\n",
        "\n",
        "                # Now we make use of the find_all function in BeautifulSoup\n",
        "                # that simply generates a list of codeblocks that match the\n",
        "                # query. This means we get a list of links, and we store the\n",
        "                # href (actual links) in our instantiated list from before.\n",
        "                links = soup.find_all('a', class_='SiteMapArticleList-link')\n",
        "                if links != []:\n",
        "                    for a_link in links:\n",
        "                        article_links.append(a_link['href'])\n",
        "\n",
        "    # Return the final list of article links that we want to scrape.\n",
        "    return article_links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrWpdIcT6gG6"
      },
      "source": [
        "article_links = get_links_to_scrape(2006, 2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdU515V_6gG7"
      },
      "source": [
        "## 1.2 Scrape articles from link list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceWCsCu26gG7"
      },
      "source": [
        "def scrape_cnbc_articles(list_of_links):\n",
        "\n",
        "    \"\"\"\n",
        "    This function take a list of article links and attempts to scrape\n",
        "    them according to CNBC's HTML structure. It will return a pandas\n",
        "    DataFrame with the scraped data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import of the required packages\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup, SoupStrainer\n",
        "    import pandas as pd\n",
        "\n",
        "    # First, we instantiate an empty list, that we can append the scraped\n",
        "    # data to. Then we instantiate an index, which enables us to follow the\n",
        "    # scrapers progress, and lastly, we instantiate a request Session, which\n",
        "    # minimizes our load time per request by little bit, but that time will\n",
        "    # accumulate for all our links and should have quite an impact.\n",
        "    df = []\n",
        "    index = 0\n",
        "    request = requests.Session()\n",
        "\n",
        "    # For every link in the list, get the source code and add 1 to the index.\n",
        "    for link in list_of_links:\n",
        "        page = request.get(link)\n",
        "        index += 1\n",
        "\n",
        "        # If the pages HTTP request Code is 200 (Meaning \"I loaded the site\n",
        "        # correctly, and the server sent me the data correctly\"), then\n",
        "        # attempt to scrape the site.\n",
        "        if page.status_code == 200:\n",
        "            try:\n",
        "\n",
        "                # First, we convert the page content to a string, so we can\n",
        "                # remove most of the irrelevant HTML. We remove everything\n",
        "                # before a box with the class \"MainContent\"\n",
        "                page = str(page.content)\n",
        "                page = page[page.find('<div id=\"MainContent\"'):]\n",
        "\n",
        "                # Now we parse the remaining content of the page into\n",
        "                # BeautifulSoup and try to extract the text of:\n",
        "                soup_link = BeautifulSoup(page)\n",
        "\n",
        "                # The Header-1 tag with a class of 'ArticleHeader-headline'\n",
        "                title = soup_link.find('h1', class_='ArticleHeader-headline').get_text()\n",
        "\n",
        "                # The div (box) tag with a class of 'ArticleBody-articleBody'\n",
        "                article = soup_link.find('div', class_='ArticleBody-articleBody').get_text()\n",
        "\n",
        "                # The date generated from the link. The link will always\n",
        "                # contain the date if it is an article. We save it as\n",
        "                # DD/MM/YYYY.\n",
        "                date = f'{link[29:31]}-{link[26:28]}-{link[21:25]}'\n",
        "\n",
        "                # The link that have a class of 'ArticleHeader-eyebrow',\n",
        "                # which is their article topic.\n",
        "                topic = soup_link.find('a', class_='ArticleHeader-eyebrow').get_text()\n",
        "\n",
        "                # If they are all successfully gathered, we append it all\n",
        "                # into our list called df.\n",
        "                df.append([title, topic, date, article, link])\n",
        "\n",
        "                # If successful, print the progress as well as the link.\n",
        "                print(f'({index}/{len(list_of_links)}) : {link}')\n",
        "            except:\n",
        "                # If we get a status code 200, but somehow some of the elements\n",
        "                # wasn't there, then skip the entire article. This ensures that\n",
        "                # we get a dataset without missing variables.\n",
        "                print(f'({index}/{len(list_of_links)}) : Skipped')\n",
        "        else:\n",
        "            # If we didn't get a status code 200 (Meaning something went wrong\n",
        "            #  in the loading of the page), then skip the article.\n",
        "            print(f'({index}/{len(list_of_links)}) : Skipped')\n",
        "\n",
        "    # Lastly, we return a dataframe that contains all of our scraped articles.\n",
        "    return pd.DataFrame(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm72WwD46gG7"
      },
      "source": [
        "df = scrape_cnbc_articles(article_links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNCCxlAH6gG7"
      },
      "source": [
        "## 1.3 Collecting the datasets, if collected over multiple times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyr8FjXF6gG7"
      },
      "source": [
        "def collect_dataset(datasets_path):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes all the files in a folder and tries to concatenate them\n",
        "    into one dataframe. This is of course only possible if your datasets is\n",
        "    the only csv-files in the folder and if they have the same data-structure.\n",
        "    We made this, because we scraped the links over several sessions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import neccesary packages\n",
        "    import pandas as pd\n",
        "\n",
        "    # List all of our datasets in the folder\n",
        "    datasets = [ds for ds in datasets_path if '.csv' in ds]\n",
        "\n",
        "    # Load all the datasets in and append it to a list called 'frames'. We can\n",
        "    # use this list to concatenate all the dataframes according to the Pandas\n",
        "    # documentation.\n",
        "    frames = []\n",
        "    for dataset in datasets:\n",
        "        df = pd.read_csv(dataset, error_bad_lines=False, index_col=False)\n",
        "        df = df[df.columns[-5:]]\n",
        "        df.columns = ['Title', 'Topic', 'Date', 'Content', 'Link']\n",
        "        frames.append(df)\n",
        "\n",
        "    # Return a concatenated dataframe of all the dataframes.\n",
        "    return pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzu4Jj9A6gG7"
      },
      "source": [
        "import os\n",
        "df = collect_dataset(os.listdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocK1JaFC6gG7"
      },
      "source": [
        "# 2. Preprocessing & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KNBtdJ96gG7"
      },
      "source": [
        "df['Content'] = df['Content'].astype('str')\n",
        "df['Title'] = df['Title'].astype('str')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aub7BxXl6gG7"
      },
      "source": [
        "def remove_clutter(text):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a string and removes what we consider as clutter\n",
        "    in our data. This includes things such as special unicode characters\n",
        "    and video timestamps.\n",
        "    \"\"\"\n",
        "\n",
        "    # Importing neccesary packages\n",
        "    import re\n",
        "\n",
        "    # Trying to remove special unicode characters. Our regular expression finds\n",
        "    # any substrings that starts with a \\x followed by two char/num combinations\n",
        "    text = re.sub(r'\\\\x[A-Za-z0-9_]{2}', '', text)\n",
        "\n",
        "    # Trying to remove video annotation. We are using a regular expression,\n",
        "    # to find any pattern that matches the word video followed by a\n",
        "    # timestamp (length of video). We believe this is just clutter in\n",
        "    # our data as well.\n",
        "    text = re.sub(r'VIDEO([0-9]|[0-9]{2}):[0-9]{4}:[0-9]{2}', ' ', text)\n",
        "\n",
        "    # Trying to remove image references. Whenever an article contains an\n",
        "    # image, the page returns a string representation of the image as the\n",
        "    # source \"Getty Images\". We remove this representation, as it brings\n",
        "    # no value to the analysis.\n",
        "    text = text.replace('Getty Images', '')\n",
        "\n",
        "    # We now remove commas, apostrophes, and double spaces. We introduce\n",
        "    # double spaces in the line above, however this could mess up our\n",
        "    # tokenization, so we simply convert any doublespaces to single spaces.\n",
        "    # We remove apostrophes after n's to normalize contracted words like\n",
        "    # wasn't, couldn't etc. Some of these words are already normalized\n",
        "    # since some of these apostrophes already have been remove by the regex\n",
        "    # unicode decluttering.\n",
        "    text = re.sub(r',','', text)\n",
        "    text = re.sub(r\"n'\",'n', text)\n",
        "    text = re.sub(r'  ',' ', text)\n",
        "\n",
        "    # Finally, we return the decluttered text.\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk64FrPL6gG7"
      },
      "source": [
        "def cleaning(df,column):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a dataframe and a column name and cleans the entire\n",
        "    column for clutter (using remove_clutter function), then pass it through\n",
        "    spaCy to further clean and tokenize. It returns the original dataframe,\n",
        "    but the given column is now cleared of clutter and it contains two\n",
        "    additional columns (Tokens, cleaned_text) as well.\n",
        "    \"\"\"\n",
        "\n",
        "    # Import neccesary packages\n",
        "    import spacy\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    import pandas as pd\n",
        "\n",
        "    # First we instantiate a list that we can append all processed tokens in.\n",
        "    # This makes it possible for us to append it to the dataframe at a later\n",
        "    # stage.\n",
        "    tokens = []\n",
        "\n",
        "    # Now, we apply our remove_clutter function to the chosen column in the\n",
        "    # dataframe. This runs the remove_clutter function for every entry in\n",
        "    # the column.\n",
        "    df[column].apply(remove_clutter)\n",
        "\n",
        "    # Define an variable to count the progress of our cleaning.\n",
        "    index = 0\n",
        "\n",
        "    # Now, we iterate over all entries (articles in our case) in the column\n",
        "    # and create a nlp object for each, which we can work with.\n",
        "    for article in nlp.pipe(df[column], disable=['parser']):\n",
        "\n",
        "        # Now, we store all tokens that pass our requirements in a list for each\n",
        "        # article. That means that each article will have their own\n",
        "        # list of tokens.\n",
        "        article_tok = [token.lemma_.lower() for token in article if _\n",
        "            token.is_alpha _\n",
        "            and not token.is_stop _\n",
        "            and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] _\n",
        "            and token.ent_type_ not in ['PERSON', 'MONEY', 'PERCENT', 'LOC', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL'] _\n",
        "            and len(token)>1]\n",
        "\n",
        "        # Now, we append said list of tokens for each article in our tokens list.\n",
        "        tokens.append(article_tok)\n",
        "\n",
        "        # When each article is processed, we increase the index by one and print\n",
        "        # the progress. This allows us to keep track of how far it is in the\n",
        "        # cleaning process. When you are dealing with many thousands of\n",
        "        # articles, it might take a while, so this feature is quite nice.\n",
        "        index += 1\n",
        "        print(f'Processed {index}/{len(df[column])}')\n",
        "\n",
        "    # When all cleaned articles are appended to our tokens list, we simply\n",
        "    # add the list as a column in the original dataframe.\n",
        "    df['tokens'] = tokens\n",
        "\n",
        "    # Lastly, we reconstruct all the articles from the tokens, simply by joining\n",
        "    # all the tokens in each article_tok list. We achieve this by a simple\n",
        "    # combination of map & lambda functions.\n",
        "    df['clean_articles'] = df['tokens'].map(lambda row: \" \".join(row))\n",
        "\n",
        "    # Returning the df that contains cleaned data and new columns.\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFJ3GyZe6gG7"
      },
      "source": [
        "df = cleaning(df, 'Content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFofqhZD6gG7"
      },
      "source": [
        "# 3. Attempting to classify topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTd0lPxW6gG7"
      },
      "source": [
        "# Output topics to construct final_topic_list manually\n",
        "pd.DataFrame(df.Topic.unique()).to_csv('unique_topics.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKEbJTno6gG7"
      },
      "source": [
        "# Read topic mapping\n",
        "topic_list = pd.read_csv('final_topic_list.csv', sep = \";\")\n",
        "topic_list_clean = pd.DataFrame.dropna(topic_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1G3lbfr6gG7"
      },
      "source": [
        "# Map Topics to articles\n",
        "predetermined = []\n",
        "index = 0\n",
        "for topic in df[\"Topic\"]:\n",
        "    index += 1\n",
        "    if topic in list(topic_list_clean[\"Topic\"]):\n",
        "        predetermined.append(topic_list_clean[topic_list_clean[\"Topic\"] == topic][\"Predetermined topic\"].to_numpy()[0])\n",
        "    else:\n",
        "        predetermined.append(\"Other\")\n",
        "    print(f'{index}')\n",
        "df['final_topic'] = predetermined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkifbRmn6gG7"
      },
      "source": [
        "# Splitting to train and predict\n",
        "df_labelled = df[df['final_topic'] != 'Other']\n",
        "df_predict = df[df['final_topic'] == 'Other']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp_F8gfn6gG7"
      },
      "source": [
        "# Split training into train and test and construct LSA\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "r_state = 123\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    df_labelled['tokens'], df_labelled['final_topic'], test_size=0.25, random_state=r_state)\n",
        "\n",
        "from gensim import models, corpora\n",
        "import ast\n",
        "\n",
        "data_processed = x_train.to_numpy()\n",
        "data_conversion = []\n",
        "for line in data_processed:\n",
        "    line = ast.literal_eval(line)\n",
        "    data_conversion.append(line)\n",
        "data_processed = data_conversion\n",
        "\n",
        "dictionary = corpora.Dictionary(data_processed)\n",
        "corpus = [dictionary.doc2bow(line) for line in data_processed]\n",
        "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
        "lsa_model = models.LsiModel(tfidf[corpus], id2word=dictionary, num_topics=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPeG5aVf6gG7"
      },
      "source": [
        "# Evaluate XGBoost and conclude that it fails...\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model = XGBClassifier(random_state = r_state)\n",
        "xgb_model.fit(x_train, y_train)\n",
        "xgb_prediction = xgb_model.predict(x_test)\n",
        "print(classification_report(y_test,xgb_prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rhuviNn6gG7"
      },
      "source": [
        "## 3.1 Filtering articles instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOcLtoXY6gG7"
      },
      "source": [
        "df = df[df['final_topic'] != 'Other']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85amaHml6gG7"
      },
      "source": [
        "# 4. Collecting datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DofYZs246gG7"
      },
      "source": [
        "## 4.1 Collecting articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4v1jg076gG7"
      },
      "source": [
        "def rolling_articles(start_date, end_date, df, start_range, end_range):\n",
        "\n",
        "    \"\"\"\n",
        "    Just like the rolling returns, this function concatenates all the articles\n",
        "    into a dataframe consisting of dates as rows. Intially, we played with the\n",
        "    thought of concatenating the articles rolling with a weeks lag, which is\n",
        "    why it supprts end and start range, however this demanded too much\n",
        "    compututional power for our time scope, which is why we simply used\n",
        "    concatenated them daily.\n",
        "    \"\"\"\n",
        "\n",
        "    # Importing neccesary packages\n",
        "    from datetime import timedelta, datetime\n",
        "    import ast\n",
        "\n",
        "    # Generating a list of dates that we can use to filter the articles from.\n",
        "    date_list = [start_date + timedelta(days=x) for x in range(0,int((end_date - start_date).days)+1)]\n",
        "\n",
        "    # Converting all date-strings in date column to actual date objects\n",
        "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
        "\n",
        "    # Generate new dataframe and instantiating a count variable that we can use\n",
        "    # to display the progress whilst running it.\n",
        "    date_index = []\n",
        "    count = 0\n",
        "\n",
        "    # For every date in our generated list of dates, find all the articles that\n",
        "    # lies within the range. Then take their tokens (because of re-import,\n",
        "    # these were actually a string) and convert to a string objects without\n",
        "    # list characters. Also, if any date has more than 30 articles,\n",
        "    # just take the 30 first articles. We integrated the last condition because\n",
        "    # of diminishing marginal benefit compared to the extra compututional effort.\n",
        "    for date in date_list:\n",
        "\n",
        "        # Here we get the articles\n",
        "        articles = df[(df['Date'] <= date + timedelta(days=end_range)) & (df['Date'] >= date + timedelta(days=start_range))].head(30*(1+end_range-start_range))\n",
        "        count += len(articles)\n",
        "        processed = \"\"\n",
        "        for article in articles['tokens']:\n",
        "            try:\n",
        "                # Here we attempt to remove the list characters. I didn't matter\n",
        "                # if we passed our articles as tokens or strings to gensim's\n",
        "                # Word2Vec algo, so we chose as string for the ease of it.\n",
        "                article = str(article).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\",\"\")\n",
        "                processed += article\n",
        "                processed += \" \"\n",
        "            except:\n",
        "                pass\n",
        "        # Now, append the date and the related articles to our date_index list,\n",
        "        # which we can turn into a dataframe, once it is returned.\n",
        "        date_index.append([date, processed])\n",
        "        print(f'{date}: {count}')\n",
        "\n",
        "    # Finally, return the date_index\n",
        "    return date_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JjnRhQb6gG7"
      },
      "source": [
        "from datetime import date\n",
        "\n",
        "sdate = date(2006, 11, 27)\n",
        "edate = date(2020, 11, 30)\n",
        "\n",
        "x = rolling_articles(sdate, edate, df, 0, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGpliYIr6gG7"
      },
      "source": [
        "## 4.2 Calculating returns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJf88LQU6gG7"
      },
      "source": [
        "bb = pd.read_csv('bb_prices.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhQgMPsy6gG7"
      },
      "source": [
        "def calculate_returns(prices, interval):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a dataset with prices for different securities over\n",
        "    time, where each row is a point in time and each column is a security.\n",
        "    It then calculates the returns for each security for a given interval\n",
        "    for at each date. It returns a dataset with dates as rows and securities as\n",
        "    columns, with returns for the given interval as values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Importing neccesary packages\n",
        "    import pandas as pd\n",
        "\n",
        "    # Converting all date-strings in date column to actual date objects. We can\n",
        "    # use these at a later stage to match returns to news articles.\n",
        "    prices['Dates'] = pd.to_datetime(prices['Dates']).dt.date\n",
        "\n",
        "    # Now we instantiate a new list to store our returns in.\n",
        "    date_index = []\n",
        "\n",
        "    # For every entry in the prices dataframe, try to fetch the current prices\n",
        "    # and the prices 'interval' periods in the future. If successful, get the\n",
        "    # return and append it to a list called 'returns'\n",
        "    for i in range(0,len(prices)):\n",
        "        try:\n",
        "            # Getting the current date of the entry\n",
        "            date = prices.iloc[i,0]\n",
        "\n",
        "            # Getting the prices for said date\n",
        "            prices_at_date = prices.iloc[i,1:]\n",
        "\n",
        "            # Getting the prices 'interval' periods in the future\n",
        "            prices_at_future_date = prices.iloc[i+interval,1:]\n",
        "\n",
        "            # Attempt to calculate the returns between the two periods.\n",
        "            return_at_date = list(prices_at_future_date / prices_at_date)\n",
        "\n",
        "            # Create a list called returns that contains the date. We can then\n",
        "            # append the returns in this list as well.\n",
        "            returns = [date]\n",
        "            for sector in return_at_date:\n",
        "                # For every column (sector) in our returns data, append it to\n",
        "                # the returns list.\n",
        "                returns.append(sector)\n",
        "\n",
        "            # Now, we can take the returns for each date and append it to our\n",
        "            # date_index list, which will make up our final dataframe in the end.\n",
        "            date_index.append(returns)\n",
        "        except:\n",
        "            # If we can't calculate the returns, simply pass the date.\n",
        "            pass\n",
        "\n",
        "    # Now, convert date_index to a dataframe and return the dataframe.\n",
        "    df = pd.DataFrame(date_index, columns = prices.columns)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BokfCEUB6gG7"
      },
      "source": [
        "y = calculate_returns(bb, 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMNWr5OR6gG7"
      },
      "source": [
        "## 4.3 Collect dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKOCN-qS6gG7"
      },
      "source": [
        "x = x.set_index('Date')\n",
        "y = y.set_index('Dates')\n",
        "df = pd.concat([y, x.reindex(y.index)], axis = 1).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZEkBe286gG7"
      },
      "source": [
        "# 5. Neural Network Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6-MTGzG6gG8"
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kHxqct_t6gG8",
        "outputId": "8eefd866-a04a-482a-a215-db65d7117b00"
      },
      "source": [
        "df = pd.read_csv('https://www.dropbox.com/s/llxun0a85lpf6e3/df_final.csv?dl=1')\n",
        "df['Dates'] = pd.to_datetime(df['Dates']).dt.date\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dates</th>\n",
              "      <th>SPTR INDEX</th>\n",
              "      <th>SPTRINFT INDEX</th>\n",
              "      <th>SPTRENRS INDEX</th>\n",
              "      <th>SPTRFINL INDEX</th>\n",
              "      <th>SPTRHLTH INDEX</th>\n",
              "      <th>SPTRINDU INDEX</th>\n",
              "      <th>SPTRCOND INDEX</th>\n",
              "      <th>SPTRUTIL INDEX</th>\n",
              "      <th>SPTRMATR INDEX</th>\n",
              "      <th>SPTRCONS INDEX</th>\n",
              "      <th>SPTRTELS INDEX</th>\n",
              "      <th>SPTRRLST INDEX</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2006-12-12</td>\n",
              "      <td>1.010224</td>\n",
              "      <td>1.002885</td>\n",
              "      <td>1.004642</td>\n",
              "      <td>1.016601</td>\n",
              "      <td>1.014123</td>\n",
              "      <td>1.016900</td>\n",
              "      <td>1.010797</td>\n",
              "      <td>1.002999</td>\n",
              "      <td>1.016288</td>\n",
              "      <td>1.006769</td>\n",
              "      <td>0.993621</td>\n",
              "      <td>0.983808</td>\n",
              "      <td>imagine disney nbc universal team desperate co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2006-12-13</td>\n",
              "      <td>1.007461</td>\n",
              "      <td>1.001175</td>\n",
              "      <td>0.982023</td>\n",
              "      <td>1.017135</td>\n",
              "      <td>1.015294</td>\n",
              "      <td>1.022835</td>\n",
              "      <td>1.006014</td>\n",
              "      <td>0.995535</td>\n",
              "      <td>1.009593</td>\n",
              "      <td>1.005985</td>\n",
              "      <td>0.992353</td>\n",
              "      <td>0.999058</td>\n",
              "      <td>amc jump public market movie theater company a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006-12-14</td>\n",
              "      <td>0.995271</td>\n",
              "      <td>0.983649</td>\n",
              "      <td>0.959095</td>\n",
              "      <td>1.006377</td>\n",
              "      <td>1.007680</td>\n",
              "      <td>1.009989</td>\n",
              "      <td>0.991821</td>\n",
              "      <td>0.990393</td>\n",
              "      <td>0.988344</td>\n",
              "      <td>1.002454</td>\n",
              "      <td>0.997374</td>\n",
              "      <td>0.985457</td>\n",
              "      <td>red sox look go sign japanese pitcher report d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2006-12-15</td>\n",
              "      <td>0.989004</td>\n",
              "      <td>0.974092</td>\n",
              "      <td>0.962900</td>\n",
              "      <td>0.998287</td>\n",
              "      <td>0.997860</td>\n",
              "      <td>0.996399</td>\n",
              "      <td>0.994200</td>\n",
              "      <td>0.990149</td>\n",
              "      <td>0.974067</td>\n",
              "      <td>0.998513</td>\n",
              "      <td>0.988887</td>\n",
              "      <td>0.983588</td>\n",
              "      <td>official start award season nomination good hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2006-12-18</td>\n",
              "      <td>0.992207</td>\n",
              "      <td>0.980559</td>\n",
              "      <td>0.989430</td>\n",
              "      <td>0.993734</td>\n",
              "      <td>0.997623</td>\n",
              "      <td>0.993896</td>\n",
              "      <td>0.995171</td>\n",
              "      <td>0.997737</td>\n",
              "      <td>0.983931</td>\n",
              "      <td>0.998484</td>\n",
              "      <td>0.993293</td>\n",
              "      <td>0.982202</td>\n",
              "      <td>merck win vioxx case get victory federal case ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Dates  ...                                             tokens\n",
              "0  2006-12-12  ...  imagine disney nbc universal team desperate co...\n",
              "1  2006-12-13  ...  amc jump public market movie theater company a...\n",
              "2  2006-12-14  ...  red sox look go sign japanese pitcher report d...\n",
              "3  2006-12-15  ...  official start award season nomination good hi...\n",
              "4  2006-12-18  ...  merck win vioxx case get victory federal case ...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhRPlVD26gG8"
      },
      "source": [
        "# list of tokens in list of articles(in a day) - We train the gensim model on all data prior to 2011\n",
        "from datetime import date\n",
        "token_list = list([token.split(\" \") for token in df[df['Dates'] <= date(2010, 12, 31)]['tokens']])\n",
        "size = 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM0oogiq6gG8",
        "outputId": "84d29680-949c-428e-9495-b731896a7497"
      },
      "source": [
        "# Training gensim word2vec\n",
        "import gensim\n",
        "\n",
        "model = gensim.models.Word2Vec(sentences = token_list, size = size, window = 5, workers = 4, min_count = 20)\n",
        "# Vocab size:\n",
        "words = list(model.wv.vocab)\n",
        "print('vocabulary size: %d' % len(words))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabulary size: 7311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a2WTPbK6gG9"
      },
      "source": [
        "# save the model\n",
        "filename = 'article_embeddings.txt'\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlU598ho6gG9"
      },
      "source": [
        "# Load in the model\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "embeddings_index= {}\n",
        "f = open(os.path.join('', 'article_embeddings.txt'), encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGQ1k3vY6gG9"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from datetime import date\n",
        "\n",
        "# vectorise the text samples into a 2D integer tensor with the data for token list before 2011\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(token_list)\n",
        "\n",
        "# Now convert all the tokens to sequences\n",
        "token_list = list([token.split(\" \") for token in df['tokens']])\n",
        "sequences = tokenizer.texts_to_sequences(token_list)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUMdsZTS6gG9",
        "outputId": "cf6a5332-06d1-4505-b829-e0530cf04f58"
      },
      "source": [
        "# pad sequences to make input length constant\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "# Get average of list\n",
        "def Average(lst): \n",
        "    return sum(lst) / len(lst) \n",
        "\n",
        "max_length = int(Average([len(doc) for doc in token_list]))\n",
        "articles_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "print('Shape of article tensor:', articles_pad.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 54770 unique tokens.\n",
            "Shape of article tensor: (3468, 4901)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4wnx5aX6gG9"
      },
      "source": [
        "# Constructing the embedding matrix\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, size))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > vocab_size:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjo342Z76gG9"
      },
      "source": [
        "# Import neccesary packages for NN model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.initializers import Constant"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1BBbVsf6gG9",
        "outputId": "88fb0cc8-cc4c-4329-9850-4d7459afe54b"
      },
      "source": [
        "# define Untrainable model\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                           size,\n",
        "                           embeddings_initializer=Constant(embedding_matrix),\n",
        "                           mask_zero = True,\n",
        "                           input_length=None,\n",
        "                           trainable=False)\n",
        "# Add embedding layer\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Add a LSTM layer with 50 internal units.\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
        "model.add(LSTM(50, dropout = 0.2))\n",
        "# Add a Dense layer with 12 units.\n",
        "model.add(Dense(12))\n",
        "# Add compiler with XXX\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "# Print summary of model\n",
        "print(model.summary())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         5477100   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 50)          30200     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 50)          20200     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 12)                612       \n",
            "=================================================================\n",
            "Total params: 5,548,312\n",
            "Trainable params: 71,212\n",
            "Non-trainable params: 5,477,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1KUGcrq6gG9"
      },
      "source": [
        "# Scale our Y\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "y = df.iloc[:,1:13]\n",
        "y = y - 1\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(y)\n",
        "y = scaler.transform(y)\n",
        "y = pd.DataFrame(y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCblQmx46gG9"
      },
      "source": [
        "def walk_forward_validation(model, epochs, x, y, step_size, train_steps, val_window):\n",
        "\n",
        "    \"\"\"\n",
        "    This function takes a model, specifically our neural net with multiple\n",
        "    LSTM-layers, the desired number of epochs, x data, y data, desired step\n",
        "    size, desired number of steps that should be trained per round, and the\n",
        "    desired validation window. It then trains a model through a Walk Forward\n",
        "    Validation method that stores MSE-scores for both training and validation\n",
        "    steps over time. It returns our backtesting predicted y, our trained y's\n",
        "    and our MSE-scores.\n",
        "    \"\"\"\n",
        "\n",
        "    # First, we import the required packages. We only have one dependency which\n",
        "    # we uses to calculate the mean squared error each period\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    # Now we instantiate a couple of things. First we define how many records\n",
        "    # that we have - We use this to loop through our data. Then we define the\n",
        "    # initial training size, which gives us the point in time where we\n",
        "    # should start over test. Then we instantiate three empty lists that we\n",
        "    # later will use to store our results.\n",
        "    n_records = len(x)\n",
        "    n_init_train = step_size * train_steps\n",
        "    train_pred = []\n",
        "    val_pred = []\n",
        "    mse_scores = []\n",
        "\n",
        "    # This for loop goes from the starting point in time (as defined above)\n",
        "    # to the end of our data and step through the data, enabling us to make the\n",
        "    # walk forward validation. Our current point in time, i, will jump by the\n",
        "    # step size each iteration.\n",
        "    for i in range(n_init_train, n_records, step_size):\n",
        "\n",
        "        # We know that the starting point for the training data, must be the\n",
        "        # current point in time minus the training period.\n",
        "        train_from = i-n_init_train\n",
        "\n",
        "        # We need to train it to the current point in time.\n",
        "        train_to = i\n",
        "\n",
        "        # We then need to validate starting from tomorrow relative to\n",
        "        # the current point in time.\n",
        "        test_from = i+1\n",
        "        # And validate the desired window in the future relative to the\n",
        "        # point in time\n",
        "        test_to = i+val_window\n",
        "\n",
        "        # Now we can split our data at this point in time\n",
        "        x_train, x_test = x[train_from:train_to], x[test_from:test_to]\n",
        "        y_train, y_test = y[train_from:train_to], y[test_from:test_to]\n",
        "\n",
        "        # And then use the data to train the model\n",
        "        print(f'Train from {i-n_init_train} to {i} and validate for {i+1} to {i+val_window}')\n",
        "        model.fit(x_train, y_train, epochs=epochs, verbose=1)\n",
        "\n",
        "        # Here, we can store the training phase's historical predictions of seen y.\n",
        "        y_train_pred = model.predict(x_train)\n",
        "        for y_train_day in y_train_pred:\n",
        "            train_pred.append(y_train_day.tolist())\n",
        "\n",
        "        # Here, we store the validation phase's future predictions of unseen y.\n",
        "        y_pred = model.predict(x_test)\n",
        "        for y_test_day in y_pred:\n",
        "            val_pred.append(y_test_day.tolist())\n",
        "\n",
        "        # Here, we calculate MSE for both and append it to our MSE-scores list.\n",
        "        train_mse = mean_squared_error(y_train,y_train_pred)\n",
        "        val_mse = mean_squared_error(y_test,y_pred)\n",
        "        mse_scores.append([train_mse, val_mse])\n",
        "\n",
        "        print(f'     train: {train_mse} \\nvalidation: {val_mse} \\n')\n",
        "\n",
        "    # Lastly, we return the training predictions, the actual validation\n",
        "    # predictions as well as the observed MSE-scores.\n",
        "    return train_pred, val_pred, mse_scores"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "BloIK4a16gG9",
        "outputId": "17a7bcc9-eb0a-4b27-b945-9758ed5f5312"
      },
      "source": [
        "# Epoch Hyper parameter tuning\n",
        "epoch_tuning_performance = []\n",
        "for epoch in range(1,11):\n",
        "    train_pred, val_pred, validation_metrics = walk_forward_validation(model = model, epochs = epoch, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
        "    validation_metrics_total = pd.DataFrame(validation_metrics).mean(axis=0)\n",
        "    mean_train_mse = validation_metrics_total.iloc[0]\n",
        "    mean_val_mse = validation_metrics_total.iloc[1]\n",
        "    epoch_tuning_performance.append([mean_train_mse, mean_val_mse])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train from 0 to 180 and validate for 181 to 240\n",
            "6/6 [==============================] - 2s 379ms/step - loss: 1.0594\n",
            "     train: 1.0366643882529083 \n",
            "validation: 1.0932809439798452 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "6/6 [==============================] - 2s 337ms/step - loss: 1.2404\n",
            "     train: 1.2221448387646097 \n",
            "validation: 0.2751915217980937 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "6/6 [==============================] - 2s 344ms/step - loss: 1.1418\n",
            "     train: 1.1164253230069383 \n",
            "validation: 0.6169644231006765 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "2/6 [=========>....................] - ETA: 0s - loss: 0.5474"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bce44eb03675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepoch_tuning_performance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwalk_forward_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles_pad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m970\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m970\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_metrics_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmean_train_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_metrics_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-fd37b4c2b88f>\u001b[0m in \u001b[0;36mwalk_forward_validation\u001b[0;34m(model, epochs, x, y, step_size, train_steps, val_window)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# And then use the data to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Train from {i-n_init_train} to {i} and validate for {i+1} to {i+val_window}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Here, we can store the training phase's historical predictions of seen y.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "mvqMvjJt6gG9",
        "outputId": "e5650295-6d22-4584-ede5-61893c238ae7"
      },
      "source": [
        "# Using plotly.express to display epoch tuning\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(epoch_tuning_performance)\n",
        "fig = px.line(df, x='epochs', y=[\"train\",\"validation\"], color_discrete_sequence=['cornflowerblue', 'indigo'])\n",
        "fig.update_xaxes(title_text='Epochs', showgrid=False)\n",
        "fig.update_yaxes(title_text='MSE')\n",
        "fig.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-77df330af2d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_tuning_performance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_discrete_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cornflowerblue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'indigo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_xaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_yaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/plotly/express/_chart_types.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(data_frame, x, y, line_group, color, line_dash, hover_name, hover_data, custom_data, text, facet_row, facet_col, facet_col_wrap, error_x, error_x_minus, error_y, error_y_minus, animation_frame, animation_group, category_orders, labels, color_discrete_sequence, color_discrete_map, line_dash_sequence, line_dash_map, log_x, log_y, range_x, range_y, line_shape, render_mode, title, template, width, height)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mpolyline\u001b[0m \u001b[0mmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmake_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36mmake_figure\u001b[0;34m(args, constructor, trace_patch, layout_patch)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     args, trace_specs, grouped_mappings, sizeref, show_colorbar = infer_config(\n\u001b[0;32m-> 1172\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m     )\n\u001b[1;32m   1174\u001b[0m     \u001b[0mgrouper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mone_group\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_mappings\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mone_group\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36minfer_config\u001b[0;34m(args, constructor, trace_patch)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mall_attrables\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroup_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_attrables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_attrables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrables\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36mbuild_dataframe\u001b[0;34m(args, attrables, array_attrables)\u001b[0m\n\u001b[1;32m    944\u001b[0m                             \u001b[0;34m\"\\n To use the index, pass it in directly as `df.index`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                         )\n\u001b[0;32m--> 946\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                     raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Value of 'x' is not the name of a column in 'data_frame'. Expected one of [] but received: epochs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4DIqEbI6gG9",
        "outputId": "68f4d015-1f05-44ca-ede8-14aecd122a5c"
      },
      "source": [
        "# Train model and get MSE & Predicted values\n",
        "train_pred, val_pred, validation_metrics = walk_forward_validation(model = model, epochs = 7, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
        "validation_metrics_total = pd.DataFrame(validation_metrics).mean(axis=0)\n",
        "mean_train_mse = validation_metrics_total.iloc[0]\n",
        "mean_val_mse = validation_metrics_total.iloc[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 522ms/step - loss: 1.0552\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 525ms/step - loss: 1.0173\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 522ms/step - loss: 0.9969\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 533ms/step - loss: 0.9617\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 533ms/step - loss: 0.9062\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 531ms/step - loss: 0.8764\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 535ms/step - loss: 0.7923\n",
            "     train: 0.6942882596549129 \n",
            "validation: 1.3387487139963603 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 513ms/step - loss: 1.0270\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 499ms/step - loss: 0.9331\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 513ms/step - loss: 0.8496\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 492ms/step - loss: 0.7838\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 510ms/step - loss: 0.7159\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 493ms/step - loss: 0.6868\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 519ms/step - loss: 0.6264\n",
            "     train: 0.432780366696452 \n",
            "validation: 0.4921329955089613 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 494ms/step - loss: 0.6582\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 506ms/step - loss: 0.5328\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 508ms/step - loss: 0.4959\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 515ms/step - loss: 0.4332\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 515ms/step - loss: 0.3910\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 508ms/step - loss: 0.3825\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 516ms/step - loss: 0.3545\n",
            "     train: 0.29094088271567325 \n",
            "validation: 1.2590004770169247 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 505ms/step - loss: 0.5644\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 505ms/step - loss: 0.4559\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 501ms/step - loss: 0.4407\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 514ms/step - loss: 0.3805\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 505ms/step - loss: 0.3702\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 504ms/step - loss: 0.3040\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 472ms/step - loss: 0.2869\n",
            "     train: 0.2189237224841379 \n",
            "validation: 0.6691865915182179 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 516ms/step - loss: 0.3877\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 504ms/step - loss: 0.3059\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.2986\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 521ms/step - loss: 0.2754\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 519ms/step - loss: 0.2468\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 514ms/step - loss: 0.2401\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 520ms/step - loss: 0.2146\n",
            "     train: 0.1777991811341131 \n",
            "validation: 0.6406233376535638 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 534ms/step - loss: 0.3568\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 528ms/step - loss: 0.3383\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.3076\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.2771\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 537ms/step - loss: 0.2598\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 532ms/step - loss: 0.2328\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 536ms/step - loss: 0.2180\n",
            "     train: 0.17539751331914458 \n",
            "validation: 0.4603302048245203 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 544ms/step - loss: 0.2941\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.2433\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.2349\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.2127\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.2172\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.1833\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.1770\n",
            "     train: 0.14106011602808113 \n",
            "validation: 0.5588351004793785 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.3173\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.2830\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 555ms/step - loss: 0.2641\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.2502\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 557ms/step - loss: 0.2197\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 555ms/step - loss: 0.2161\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 563ms/step - loss: 0.1958\n",
            "     train: 0.16389752894076268 \n",
            "validation: 0.6210871689563503 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 0.3637\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.3180\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.2899\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 551ms/step - loss: 0.2701\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.2528\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 555ms/step - loss: 0.2406\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 562ms/step - loss: 0.2240\n",
            "     train: 0.1895513257982537 \n",
            "validation: 0.5378193898375742 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.3463\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 557ms/step - loss: 0.3198\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 563ms/step - loss: 0.2827\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.2666\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.2395\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.2365\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 559ms/step - loss: 0.2214\n",
            "     train: 0.1919648374704915 \n",
            "validation: 0.6344160535606432 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.3238\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 559ms/step - loss: 0.2874\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.2826\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 557ms/step - loss: 0.2435\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 551ms/step - loss: 0.2269\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 567ms/step - loss: 0.2115\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 557ms/step - loss: 0.2040\n",
            "     train: 0.15981287594396618 \n",
            "validation: 0.43000698217432937 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.2594\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.2513\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.2275\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 0.2100\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 564ms/step - loss: 0.1982\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.1886\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.1783\n",
            "     train: 0.15368109256280574 \n",
            "validation: 0.3210326391284614 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 562ms/step - loss: 0.2381\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.2126\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 0.2181\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 559ms/step - loss: 0.1977\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.1765\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 557ms/step - loss: 0.1844\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.1632\n",
            "     train: 0.13967145788366556 \n",
            "validation: 0.5971669624066948 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.3152\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 564ms/step - loss: 0.2980\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.2658\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.2529\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.2242\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.2259\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 539ms/step - loss: 0.2067\n",
            "     train: 0.17104603399600357 \n",
            "validation: 0.8581219263711751 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 547ms/step - loss: 0.4125\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.3627\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.3318\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 544ms/step - loss: 0.2949\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 549ms/step - loss: 0.2764\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 568ms/step - loss: 0.2481\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 570ms/step - loss: 0.2385\n",
            "     train: 0.19424671538521565 \n",
            "validation: 0.6230932169524617 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.3619\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 547ms/step - loss: 0.3431\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.3000\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.2856\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 534ms/step - loss: 0.2767\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.2502\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.2446\n",
            "     train: 0.20319243167906517 \n",
            "validation: 0.5175230482024846 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.3523\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 560ms/step - loss: 0.3199\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.2829\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 568ms/step - loss: 0.2575\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 562ms/step - loss: 0.2455\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 560ms/step - loss: 0.2432\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 567ms/step - loss: 0.2252\n",
            "     train: 0.19365675016012782 \n",
            "validation: 1.476325603596914 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 567ms/step - loss: 0.6344\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 562ms/step - loss: 0.5483\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 568ms/step - loss: 0.4834\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 559ms/step - loss: 0.4130\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 572ms/step - loss: 0.3604\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.3215\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 570ms/step - loss: 0.3014\n",
            "     train: 0.23745442143223416 \n",
            "validation: 0.8069971192977264 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.4761\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.3846\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 578ms/step - loss: 0.3563\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.3357\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 568ms/step - loss: 0.3124\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 576ms/step - loss: 0.2845\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 568ms/step - loss: 0.2717\n",
            "     train: 0.2212590602690252 \n",
            "validation: 1.0981929026916644 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.5742\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.5135\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.4617\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.4242\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 559ms/step - loss: 0.3872\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.3534\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.3193\n",
            "     train: 0.2571903504353082 \n",
            "validation: 0.5890652123639752 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 570ms/step - loss: 0.4041\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 563ms/step - loss: 0.3585\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 555ms/step - loss: 0.3416\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.3202\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 0.2857\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.2848\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.2718\n",
            "     train: 0.23751674075335713 \n",
            "validation: 0.7072763401811465 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 557ms/step - loss: 0.4084\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.3800\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.3596\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 559ms/step - loss: 0.3290\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.3135\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.3059\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.2905\n",
            "     train: 0.24302688804793782 \n",
            "validation: 0.6622451356182867 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.4085\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.3809\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.3587\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.3433\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 555ms/step - loss: 0.3275\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.3166\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 565ms/step - loss: 0.3080\n",
            "     train: 0.27088385380992736 \n",
            "validation: 0.4345882912567475 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.3672\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 546ms/step - loss: 0.3251\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.3158\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.3033\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 531ms/step - loss: 0.2892\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.2821\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.2774\n",
            "     train: 0.2409942259132536 \n",
            "validation: 0.25937146539632583 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 542ms/step - loss: 0.2838\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 538ms/step - loss: 0.2695\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.2617\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 539ms/step - loss: 0.2471\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 549ms/step - loss: 0.2412\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.2373\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 542ms/step - loss: 0.2305\n",
            "     train: 0.2063025519926772 \n",
            "validation: 0.32468151251282057 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 562ms/step - loss: 0.2213\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 538ms/step - loss: 0.2155\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 544ms/step - loss: 0.2015\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 539ms/step - loss: 0.1934\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.1932\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.1834\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.1805\n",
            "     train: 0.16314787790814114 \n",
            "validation: 0.2995191464688212 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 550ms/step - loss: 0.2224\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 546ms/step - loss: 0.2150\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 534ms/step - loss: 0.2039\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.2015\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 535ms/step - loss: 0.1983\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.1893\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 0.1924\n",
            "     train: 0.17522726344446082 \n",
            "validation: 0.44363051473705406 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 537ms/step - loss: 0.2935\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 538ms/step - loss: 0.2744\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 547ms/step - loss: 0.2661\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 528ms/step - loss: 0.2634\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.2560\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.2515\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 538ms/step - loss: 0.2446\n",
            "     train: 0.2272771331897843 \n",
            "validation: 1.1714895734815416 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 537ms/step - loss: 0.5571\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.5238\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 536ms/step - loss: 0.5082\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.4689\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 536ms/step - loss: 0.4575\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 536ms/step - loss: 0.4172\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 537ms/step - loss: 0.3996\n",
            "     train: 0.33751340007714653 \n",
            "validation: 0.5870825211824885 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 544ms/step - loss: 0.4905\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 551ms/step - loss: 0.4485\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.4093\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.3769\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 529ms/step - loss: 0.3250\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 537ms/step - loss: 0.3083\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 0.3020\n",
            "     train: 0.26375884059697813 \n",
            "validation: 0.49999803544450083 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 530ms/step - loss: 0.3696\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.3395\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 563ms/step - loss: 0.3087\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 547ms/step - loss: 0.2882\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 535ms/step - loss: 0.2756\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 534ms/step - loss: 0.2712\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 544ms/step - loss: 0.2529\n",
            "     train: 0.21681484402202825 \n",
            "validation: 1.9391708213625127 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 536ms/step - loss: 0.8022\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 557ms/step - loss: 0.6976\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 532ms/step - loss: 0.6272\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 526ms/step - loss: 0.5592\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.4996\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 550ms/step - loss: 0.4883\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.4270\n",
            "     train: 0.32888439850843193 \n",
            "validation: 1.3529327899276282 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.6025\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 549ms/step - loss: 0.5109\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 532ms/step - loss: 0.4312\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 532ms/step - loss: 0.4116\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.3480\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.3214\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 546ms/step - loss: 0.2894\n",
            "     train: 0.2299537132616075 \n",
            "validation: 0.7196225507095709 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 549ms/step - loss: 0.4614\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 531ms/step - loss: 0.3621\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 545ms/step - loss: 0.3379\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.3032\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.3074\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 546ms/step - loss: 0.2691\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.2636\n",
            "     train: 0.22141508865658435 \n",
            "validation: 0.7180580572117267 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.4064\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 517ms/step - loss: 0.3357\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 525ms/step - loss: 0.3064\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 516ms/step - loss: 0.2758\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 534ms/step - loss: 0.2634\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 532ms/step - loss: 0.2372\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 530ms/step - loss: 0.2294\n",
            "     train: 0.1883338041625178 \n",
            "validation: 0.45527323651882695 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 521ms/step - loss: 0.3090\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 522ms/step - loss: 0.2863\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 533ms/step - loss: 0.2576\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 501ms/step - loss: 0.2436\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 515ms/step - loss: 0.2377\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 510ms/step - loss: 0.2222\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 535ms/step - loss: 0.2140\n",
            "     train: 0.1773381829085862 \n",
            "validation: 6.444237769790303 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 2.2571\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 530ms/step - loss: 1.9286\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 538ms/step - loss: 1.5554\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 539ms/step - loss: 1.3207\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 1.2956\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 539ms/step - loss: 1.2522\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 528ms/step - loss: 1.0313\n",
            "     train: 0.8395343860011463 \n",
            "validation: 3.3008498723828215 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 1.8726\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 527ms/step - loss: 1.8422\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 521ms/step - loss: 1.4919\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 1.2507\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 1.1258\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 531ms/step - loss: 0.9879\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.8965\n",
            "     train: 0.7555441577854701 \n",
            "validation: 1.439526537553425 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 544ms/step - loss: 1.1503\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 544ms/step - loss: 1.1054\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 534ms/step - loss: 1.0001\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.8718\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 551ms/step - loss: 0.8540\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 0.8326\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.8155\n",
            "     train: 0.6801195909682249 \n",
            "validation: 2.151475645766832 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "u2zJrRXL6gG9",
        "outputId": "9853cded-c545-47a3-f065-55752b5dee35"
      },
      "source": [
        "pd.DataFrame(scaler.inverse_transform(train_pred)+1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.002378</td>\n",
              "      <td>1.002619</td>\n",
              "      <td>1.002351</td>\n",
              "      <td>1.003180</td>\n",
              "      <td>1.004984</td>\n",
              "      <td>1.001935</td>\n",
              "      <td>1.003995</td>\n",
              "      <td>1.003169</td>\n",
              "      <td>0.999011</td>\n",
              "      <td>1.002974</td>\n",
              "      <td>1.004817</td>\n",
              "      <td>1.002959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.008032</td>\n",
              "      <td>1.006587</td>\n",
              "      <td>1.008192</td>\n",
              "      <td>1.010795</td>\n",
              "      <td>1.011593</td>\n",
              "      <td>1.011804</td>\n",
              "      <td>1.011275</td>\n",
              "      <td>1.010059</td>\n",
              "      <td>1.009192</td>\n",
              "      <td>1.006125</td>\n",
              "      <td>1.008965</td>\n",
              "      <td>1.014159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.009558</td>\n",
              "      <td>1.010869</td>\n",
              "      <td>1.020395</td>\n",
              "      <td>1.005061</td>\n",
              "      <td>1.007466</td>\n",
              "      <td>1.005868</td>\n",
              "      <td>1.009217</td>\n",
              "      <td>1.007681</td>\n",
              "      <td>1.010284</td>\n",
              "      <td>1.005065</td>\n",
              "      <td>1.005884</td>\n",
              "      <td>1.010713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.008305</td>\n",
              "      <td>1.010248</td>\n",
              "      <td>1.016456</td>\n",
              "      <td>1.004312</td>\n",
              "      <td>1.009305</td>\n",
              "      <td>1.008463</td>\n",
              "      <td>1.010186</td>\n",
              "      <td>1.005562</td>\n",
              "      <td>1.011795</td>\n",
              "      <td>1.006100</td>\n",
              "      <td>1.008183</td>\n",
              "      <td>1.014799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.996029</td>\n",
              "      <td>0.997010</td>\n",
              "      <td>1.006602</td>\n",
              "      <td>0.980299</td>\n",
              "      <td>0.996965</td>\n",
              "      <td>0.995938</td>\n",
              "      <td>0.998330</td>\n",
              "      <td>1.004756</td>\n",
              "      <td>1.002105</td>\n",
              "      <td>0.997065</td>\n",
              "      <td>1.001291</td>\n",
              "      <td>0.998593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7015</th>\n",
              "      <td>1.003488</td>\n",
              "      <td>1.007053</td>\n",
              "      <td>0.992608</td>\n",
              "      <td>0.997444</td>\n",
              "      <td>1.000962</td>\n",
              "      <td>1.001356</td>\n",
              "      <td>0.996913</td>\n",
              "      <td>1.007907</td>\n",
              "      <td>1.004697</td>\n",
              "      <td>1.006221</td>\n",
              "      <td>1.002968</td>\n",
              "      <td>1.001685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7016</th>\n",
              "      <td>0.973314</td>\n",
              "      <td>0.977563</td>\n",
              "      <td>0.951799</td>\n",
              "      <td>0.958609</td>\n",
              "      <td>0.981944</td>\n",
              "      <td>0.964594</td>\n",
              "      <td>0.977606</td>\n",
              "      <td>0.969724</td>\n",
              "      <td>0.967360</td>\n",
              "      <td>0.980519</td>\n",
              "      <td>0.972557</td>\n",
              "      <td>0.961496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7017</th>\n",
              "      <td>0.946851</td>\n",
              "      <td>0.957664</td>\n",
              "      <td>0.911454</td>\n",
              "      <td>0.928922</td>\n",
              "      <td>0.960423</td>\n",
              "      <td>0.933270</td>\n",
              "      <td>0.953728</td>\n",
              "      <td>0.948397</td>\n",
              "      <td>0.936850</td>\n",
              "      <td>0.968745</td>\n",
              "      <td>0.958439</td>\n",
              "      <td>0.951526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7018</th>\n",
              "      <td>0.960045</td>\n",
              "      <td>0.966446</td>\n",
              "      <td>0.935860</td>\n",
              "      <td>0.944467</td>\n",
              "      <td>0.963984</td>\n",
              "      <td>0.949808</td>\n",
              "      <td>0.960699</td>\n",
              "      <td>0.964137</td>\n",
              "      <td>0.953775</td>\n",
              "      <td>0.974391</td>\n",
              "      <td>0.959989</td>\n",
              "      <td>0.948994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7019</th>\n",
              "      <td>0.959592</td>\n",
              "      <td>0.965102</td>\n",
              "      <td>0.931670</td>\n",
              "      <td>0.945901</td>\n",
              "      <td>0.970369</td>\n",
              "      <td>0.949746</td>\n",
              "      <td>0.967631</td>\n",
              "      <td>0.962257</td>\n",
              "      <td>0.949820</td>\n",
              "      <td>0.974984</td>\n",
              "      <td>0.971545</td>\n",
              "      <td>0.964050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7020 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "0     1.002378  1.002619  1.002351  ...  1.002974  1.004817  1.002959\n",
              "1     1.008032  1.006587  1.008192  ...  1.006125  1.008965  1.014159\n",
              "2     1.009558  1.010869  1.020395  ...  1.005065  1.005884  1.010713\n",
              "3     1.008305  1.010248  1.016456  ...  1.006100  1.008183  1.014799\n",
              "4     0.996029  0.997010  1.006602  ...  0.997065  1.001291  0.998593\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "7015  1.003488  1.007053  0.992608  ...  1.006221  1.002968  1.001685\n",
              "7016  0.973314  0.977563  0.951799  ...  0.980519  0.972557  0.961496\n",
              "7017  0.946851  0.957664  0.911454  ...  0.968745  0.958439  0.951526\n",
              "7018  0.960045  0.966446  0.935860  ...  0.974391  0.959989  0.948994\n",
              "7019  0.959592  0.965102  0.931670  ...  0.974984  0.971545  0.964050\n",
              "\n",
              "[7020 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "oKHwlTGW6gG9",
        "outputId": "c332212f-3f0f-4a96-cdd7-5f1cef663591"
      },
      "source": [
        "pd.DataFrame(scaler.inverse_transform(val_pred)+1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.982655</td>\n",
              "      <td>0.980964</td>\n",
              "      <td>0.964470</td>\n",
              "      <td>0.968158</td>\n",
              "      <td>0.981726</td>\n",
              "      <td>0.970010</td>\n",
              "      <td>0.977613</td>\n",
              "      <td>0.988501</td>\n",
              "      <td>0.967699</td>\n",
              "      <td>0.990554</td>\n",
              "      <td>0.984038</td>\n",
              "      <td>0.963558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.006547</td>\n",
              "      <td>1.005892</td>\n",
              "      <td>1.005766</td>\n",
              "      <td>1.003910</td>\n",
              "      <td>1.006647</td>\n",
              "      <td>1.006164</td>\n",
              "      <td>1.006361</td>\n",
              "      <td>1.005348</td>\n",
              "      <td>1.005193</td>\n",
              "      <td>1.005213</td>\n",
              "      <td>1.005562</td>\n",
              "      <td>1.010853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.996456</td>\n",
              "      <td>0.996402</td>\n",
              "      <td>0.994518</td>\n",
              "      <td>0.990775</td>\n",
              "      <td>0.999925</td>\n",
              "      <td>0.993281</td>\n",
              "      <td>0.997079</td>\n",
              "      <td>1.000049</td>\n",
              "      <td>0.993463</td>\n",
              "      <td>0.999046</td>\n",
              "      <td>1.000784</td>\n",
              "      <td>0.995080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.011838</td>\n",
              "      <td>1.017436</td>\n",
              "      <td>1.039585</td>\n",
              "      <td>1.006729</td>\n",
              "      <td>1.008846</td>\n",
              "      <td>1.019985</td>\n",
              "      <td>1.023047</td>\n",
              "      <td>1.017100</td>\n",
              "      <td>1.026120</td>\n",
              "      <td>1.007812</td>\n",
              "      <td>1.006350</td>\n",
              "      <td>1.024439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.006947</td>\n",
              "      <td>1.008611</td>\n",
              "      <td>1.021895</td>\n",
              "      <td>0.999836</td>\n",
              "      <td>1.006143</td>\n",
              "      <td>1.008186</td>\n",
              "      <td>1.009583</td>\n",
              "      <td>1.006771</td>\n",
              "      <td>1.013405</td>\n",
              "      <td>1.003679</td>\n",
              "      <td>1.007898</td>\n",
              "      <td>1.011972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2274</th>\n",
              "      <td>1.003678</td>\n",
              "      <td>1.002942</td>\n",
              "      <td>0.987572</td>\n",
              "      <td>0.999635</td>\n",
              "      <td>0.999745</td>\n",
              "      <td>1.001557</td>\n",
              "      <td>0.995198</td>\n",
              "      <td>1.011229</td>\n",
              "      <td>1.002504</td>\n",
              "      <td>1.003168</td>\n",
              "      <td>1.000657</td>\n",
              "      <td>1.003118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2275</th>\n",
              "      <td>1.008835</td>\n",
              "      <td>1.014644</td>\n",
              "      <td>1.012891</td>\n",
              "      <td>1.010606</td>\n",
              "      <td>1.009232</td>\n",
              "      <td>1.011363</td>\n",
              "      <td>1.007803</td>\n",
              "      <td>1.004850</td>\n",
              "      <td>1.014110</td>\n",
              "      <td>1.005812</td>\n",
              "      <td>1.010313</td>\n",
              "      <td>1.005826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2276</th>\n",
              "      <td>1.019246</td>\n",
              "      <td>1.019723</td>\n",
              "      <td>1.029361</td>\n",
              "      <td>1.025424</td>\n",
              "      <td>1.016335</td>\n",
              "      <td>1.033427</td>\n",
              "      <td>1.018821</td>\n",
              "      <td>1.024115</td>\n",
              "      <td>1.035583</td>\n",
              "      <td>1.010959</td>\n",
              "      <td>1.021961</td>\n",
              "      <td>1.023622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2277</th>\n",
              "      <td>1.011083</td>\n",
              "      <td>1.016498</td>\n",
              "      <td>1.014141</td>\n",
              "      <td>1.004401</td>\n",
              "      <td>1.005415</td>\n",
              "      <td>1.012908</td>\n",
              "      <td>1.012813</td>\n",
              "      <td>1.009868</td>\n",
              "      <td>1.012058</td>\n",
              "      <td>1.005009</td>\n",
              "      <td>1.006567</td>\n",
              "      <td>1.010932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2278</th>\n",
              "      <td>1.003778</td>\n",
              "      <td>1.015145</td>\n",
              "      <td>0.997675</td>\n",
              "      <td>0.995688</td>\n",
              "      <td>1.006948</td>\n",
              "      <td>0.999514</td>\n",
              "      <td>1.014110</td>\n",
              "      <td>0.993427</td>\n",
              "      <td>1.005066</td>\n",
              "      <td>1.003785</td>\n",
              "      <td>1.005297</td>\n",
              "      <td>0.998075</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2279 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "0     0.982655  0.980964  0.964470  ...  0.990554  0.984038  0.963558\n",
              "1     1.006547  1.005892  1.005766  ...  1.005213  1.005562  1.010853\n",
              "2     0.996456  0.996402  0.994518  ...  0.999046  1.000784  0.995080\n",
              "3     1.011838  1.017436  1.039585  ...  1.007812  1.006350  1.024439\n",
              "4     1.006947  1.008611  1.021895  ...  1.003679  1.007898  1.011972\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "2274  1.003678  1.002942  0.987572  ...  1.003168  1.000657  1.003118\n",
              "2275  1.008835  1.014644  1.012891  ...  1.005812  1.010313  1.005826\n",
              "2276  1.019246  1.019723  1.029361  ...  1.010959  1.021961  1.023622\n",
              "2277  1.011083  1.016498  1.014141  ...  1.005009  1.006567  1.010932\n",
              "2278  1.003778  1.015145  0.997675  ...  1.003785  1.005297  0.998075\n",
              "\n",
              "[2279 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9nfoPzK96gG9",
        "outputId": "c4374391-6eb0-459c-c3b2-f967e5c0a892"
      },
      "source": [
        "pd.DataFrame(validation_metrics)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.694288</td>\n",
              "      <td>1.338749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.432780</td>\n",
              "      <td>0.492133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.290941</td>\n",
              "      <td>1.259000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.218924</td>\n",
              "      <td>0.669187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.177799</td>\n",
              "      <td>0.640623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.175398</td>\n",
              "      <td>0.460330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.141060</td>\n",
              "      <td>0.558835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.163898</td>\n",
              "      <td>0.621087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.189551</td>\n",
              "      <td>0.537819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.191965</td>\n",
              "      <td>0.634416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.159813</td>\n",
              "      <td>0.430007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.153681</td>\n",
              "      <td>0.321033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.139671</td>\n",
              "      <td>0.597167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.171046</td>\n",
              "      <td>0.858122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.194247</td>\n",
              "      <td>0.623093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.203192</td>\n",
              "      <td>0.517523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.193657</td>\n",
              "      <td>1.476326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.237454</td>\n",
              "      <td>0.806997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.221259</td>\n",
              "      <td>1.098193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.257190</td>\n",
              "      <td>0.589065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.237517</td>\n",
              "      <td>0.707276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.243027</td>\n",
              "      <td>0.662245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.270884</td>\n",
              "      <td>0.434588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.240994</td>\n",
              "      <td>0.259371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.206303</td>\n",
              "      <td>0.324682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.163148</td>\n",
              "      <td>0.299519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.175227</td>\n",
              "      <td>0.443631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.227277</td>\n",
              "      <td>1.171490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.337513</td>\n",
              "      <td>0.587083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.263759</td>\n",
              "      <td>0.499998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.216815</td>\n",
              "      <td>1.939171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.328884</td>\n",
              "      <td>1.352933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.229954</td>\n",
              "      <td>0.719623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.221415</td>\n",
              "      <td>0.718058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.188334</td>\n",
              "      <td>0.455273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.177338</td>\n",
              "      <td>6.444238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.839534</td>\n",
              "      <td>3.300850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.755544</td>\n",
              "      <td>1.439527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.680120</td>\n",
              "      <td>2.151476</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1\n",
              "0   0.694288  1.338749\n",
              "1   0.432780  0.492133\n",
              "2   0.290941  1.259000\n",
              "3   0.218924  0.669187\n",
              "4   0.177799  0.640623\n",
              "5   0.175398  0.460330\n",
              "6   0.141060  0.558835\n",
              "7   0.163898  0.621087\n",
              "8   0.189551  0.537819\n",
              "9   0.191965  0.634416\n",
              "10  0.159813  0.430007\n",
              "11  0.153681  0.321033\n",
              "12  0.139671  0.597167\n",
              "13  0.171046  0.858122\n",
              "14  0.194247  0.623093\n",
              "15  0.203192  0.517523\n",
              "16  0.193657  1.476326\n",
              "17  0.237454  0.806997\n",
              "18  0.221259  1.098193\n",
              "19  0.257190  0.589065\n",
              "20  0.237517  0.707276\n",
              "21  0.243027  0.662245\n",
              "22  0.270884  0.434588\n",
              "23  0.240994  0.259371\n",
              "24  0.206303  0.324682\n",
              "25  0.163148  0.299519\n",
              "26  0.175227  0.443631\n",
              "27  0.227277  1.171490\n",
              "28  0.337513  0.587083\n",
              "29  0.263759  0.499998\n",
              "30  0.216815  1.939171\n",
              "31  0.328884  1.352933\n",
              "32  0.229954  0.719623\n",
              "33  0.221415  0.718058\n",
              "34  0.188334  0.455273\n",
              "35  0.177338  6.444238\n",
              "36  0.839534  3.300850\n",
              "37  0.755544  1.439527\n",
              "38  0.680120  2.151476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3CW3if86gG9",
        "outputId": "0c450b95-df65-421b-98ae-6c6a41ad7b85"
      },
      "source": [
        "# define Trainable model\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                           size,\n",
        "                           embeddings_initializer=Constant(embedding_matrix),\n",
        "                           mask_zero = True,\n",
        "                           input_length=None,\n",
        "                           trainable=True)\n",
        "# Add embedding layer\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# Add a LSTM layer with 50 internal units.\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
        "model.add(LSTM(50, dropout = 0.2))\n",
        "# Add a Dense layer with 12 units.\n",
        "model.add(Dense(12))\n",
        "# Add compiler with XXX\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "# Print summary of model\n",
        "print(model.summary())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         5477100   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 50)          30200     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, None, 50)          20200     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                612       \n",
            "=================================================================\n",
            "Total params: 5,548,312\n",
            "Trainable params: 5,548,312\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSv3gPf06gG9",
        "outputId": "7cd0af9b-f694-4d12-f52f-7a965ddb1be0"
      },
      "source": [
        "# Train model and get MSE & Predicted values\n",
        "train_pred_trained, val_pred_trained, validation_metrics_trained = walk_forward_validation(model = model, epochs = 7, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
        "validation_metrics_trained_total = pd.DataFrame(validation_metrics_trained).mean(axis=0)\n",
        "mean_train_trained_mse = validation_metrics_trained_total.iloc[0]\n",
        "mean_val_trained_mse = validation_metrics_trained_total.iloc[1]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train from 0 to 180 and validate for 181 to 240\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 581ms/step - loss: 1.0575\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 569ms/step - loss: 1.0337\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 567ms/step - loss: 1.0184\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 582ms/step - loss: 0.9800\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 567ms/step - loss: 0.9374\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 579ms/step - loss: 0.8813\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 574ms/step - loss: 0.8001\n",
            "     train: 0.6890397686291699 \n",
            "validation: 1.4540047475432443 \n",
            "\n",
            "Train from 60 to 240 and validate for 241 to 300\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 560ms/step - loss: 1.0742\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 1.0003\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.8331\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 550ms/step - loss: 0.6976\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 550ms/step - loss: 0.5881\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.4836\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 546ms/step - loss: 0.4510\n",
            "     train: 0.3190679527712455 \n",
            "validation: 1.071912761662376 \n",
            "\n",
            "Train from 120 to 300 and validate for 301 to 360\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.5723\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 524ms/step - loss: 0.5784\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.4681\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 552ms/step - loss: 0.3959\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.3717\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 563ms/step - loss: 0.3036\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.2869\n",
            "     train: 0.24110993966007563 \n",
            "validation: 1.1555547810982063 \n",
            "\n",
            "Train from 180 to 360 and validate for 361 to 420\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.4803\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.4007\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.3616\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 542ms/step - loss: 0.3248\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 551ms/step - loss: 0.2957\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.2520\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 558ms/step - loss: 0.2376\n",
            "     train: 0.17761496277705402 \n",
            "validation: 0.5753965595960432 \n",
            "\n",
            "Train from 240 to 420 and validate for 421 to 480\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 560ms/step - loss: 0.3226\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 543ms/step - loss: 0.3070\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 560ms/step - loss: 0.2592\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 541ms/step - loss: 0.2415\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 548ms/step - loss: 0.2066\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 561ms/step - loss: 0.1998\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 549ms/step - loss: 0.1827\n",
            "     train: 0.145711178903783 \n",
            "validation: 0.6411284320881154 \n",
            "\n",
            "Train from 300 to 480 and validate for 481 to 540\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 579ms/step - loss: 0.3338\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 586ms/step - loss: 0.2936\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 568ms/step - loss: 0.2495\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 572ms/step - loss: 0.2240\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 571ms/step - loss: 0.1969\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 576ms/step - loss: 0.1800\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 578ms/step - loss: 0.1688\n",
            "     train: 0.13506095422784006 \n",
            "validation: 0.38085512410188377 \n",
            "\n",
            "Train from 360 to 540 and validate for 541 to 600\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 592ms/step - loss: 0.2309\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 595ms/step - loss: 0.1938\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.1846\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 597ms/step - loss: 0.1652\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 587ms/step - loss: 0.1536\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 588ms/step - loss: 0.1533\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 585ms/step - loss: 0.1326\n",
            "     train: 0.11159153422676825 \n",
            "validation: 0.5750359919025446 \n",
            "\n",
            "Train from 420 to 600 and validate for 601 to 660\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.2766\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 611ms/step - loss: 0.2591\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.2115\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 597ms/step - loss: 0.1918\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 602ms/step - loss: 0.1878\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.1743\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 597ms/step - loss: 0.1644\n",
            "     train: 0.1342228343180711 \n",
            "validation: 0.4988755941495115 \n",
            "\n",
            "Train from 480 to 660 and validate for 661 to 720\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 613ms/step - loss: 0.2974\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.2703\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 615ms/step - loss: 0.2441\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.2146\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.1919\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.1718\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 610ms/step - loss: 0.1688\n",
            "     train: 0.13871469775120446 \n",
            "validation: 0.41223139916318213 \n",
            "\n",
            "Train from 540 to 720 and validate for 721 to 780\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 607ms/step - loss: 0.2668\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.2304\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 619ms/step - loss: 0.1983\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 614ms/step - loss: 0.1824\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.1681\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 602ms/step - loss: 0.1510\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 608ms/step - loss: 0.1502\n",
            "     train: 0.11264744967388858 \n",
            "validation: 0.5213870513999431 \n",
            "\n",
            "Train from 600 to 780 and validate for 781 to 840\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 604ms/step - loss: 0.2664\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.2314\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.2021\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.1691\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 593ms/step - loss: 0.1619\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 584ms/step - loss: 0.1468\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.1330\n",
            "     train: 0.09996858723209473 \n",
            "validation: 0.40734880006364266 \n",
            "\n",
            "Train from 660 to 840 and validate for 841 to 900\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 610ms/step - loss: 0.2342\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.1994\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 612ms/step - loss: 0.1761\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 608ms/step - loss: 0.1546\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.1415\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.1404\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 592ms/step - loss: 0.1254\n",
            "     train: 0.10231650578566232 \n",
            "validation: 0.3314563359824704 \n",
            "\n",
            "Train from 720 to 900 and validate for 901 to 960\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.2039\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 612ms/step - loss: 0.1762\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.1644\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 610ms/step - loss: 0.1485\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.1420\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.1275\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 613ms/step - loss: 0.1199\n",
            "     train: 0.10275860118774498 \n",
            "validation: 0.6466034128770243 \n",
            "\n",
            "Train from 780 to 960 and validate for 961 to 1020\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2958\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 607ms/step - loss: 0.2733\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.2381\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.2137\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.1924\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 594ms/step - loss: 0.1797\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 608ms/step - loss: 0.1670\n",
            "     train: 0.13890325109766044 \n",
            "validation: 0.7485817323922398 \n",
            "\n",
            "Train from 840 to 1020 and validate for 1021 to 1080\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.3525\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 625ms/step - loss: 0.3158\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2784\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.2490\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.2301\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 618ms/step - loss: 0.2144\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.1908\n",
            "     train: 0.16398350898200786 \n",
            "validation: 0.5869242568296892 \n",
            "\n",
            "Train from 900 to 1080 and validate for 1081 to 1140\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 620ms/step - loss: 0.3287\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 612ms/step - loss: 0.2854\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2701\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.2391\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.2359\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 611ms/step - loss: 0.2201\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.1971\n",
            "     train: 0.1847849389026328 \n",
            "validation: 0.5189229777831776 \n",
            "\n",
            "Train from 960 to 1140 and validate for 1141 to 1200\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 602ms/step - loss: 0.3056\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 602ms/step - loss: 0.2668\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2488\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.2347\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.2098\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 614ms/step - loss: 0.1967\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.1859\n",
            "     train: 0.15538076109053903 \n",
            "validation: 1.2755341302338936 \n",
            "\n",
            "Train from 1020 to 1200 and validate for 1201 to 1260\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.5065\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 613ms/step - loss: 0.4362\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 622ms/step - loss: 0.3799\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.3192\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 610ms/step - loss: 0.2668\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 616ms/step - loss: 0.2530\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 616ms/step - loss: 0.2251\n",
            "     train: 0.17816300344971492 \n",
            "validation: 0.7876851978774025 \n",
            "\n",
            "Train from 1080 to 1260 and validate for 1261 to 1320\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.3578\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 610ms/step - loss: 0.3314\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.2849\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 615ms/step - loss: 0.2467\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 611ms/step - loss: 0.2307\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 614ms/step - loss: 0.2160\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.2039\n",
            "     train: 0.16695708199226475 \n",
            "validation: 1.0376450227801397 \n",
            "\n",
            "Train from 1140 to 1320 and validate for 1321 to 1380\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 611ms/step - loss: 0.5175\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.4492\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.3710\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 615ms/step - loss: 0.3154\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2940\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2624\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 620ms/step - loss: 0.2453\n",
            "     train: 0.20737904369599716 \n",
            "validation: 0.5292194655307525 \n",
            "\n",
            "Train from 1200 to 1380 and validate for 1381 to 1440\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.3515\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 616ms/step - loss: 0.3151\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.2960\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 598ms/step - loss: 0.2537\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 613ms/step - loss: 0.2381\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 608ms/step - loss: 0.2257\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2177\n",
            "     train: 0.1834964041141752 \n",
            "validation: 0.5842355526912085 \n",
            "\n",
            "Train from 1260 to 1440 and validate for 1441 to 1500\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 615ms/step - loss: 0.3510\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 608ms/step - loss: 0.3163\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 600ms/step - loss: 0.2716\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 611ms/step - loss: 0.2584\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 627ms/step - loss: 0.2296\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 608ms/step - loss: 0.2049\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 622ms/step - loss: 0.2029\n",
            "     train: 0.16998680018824353 \n",
            "validation: 0.624981781594422 \n",
            "\n",
            "Train from 1320 to 1500 and validate for 1501 to 1560\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 597ms/step - loss: 0.3437\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 614ms/step - loss: 0.3060\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 622ms/step - loss: 0.2813\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.2510\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 600ms/step - loss: 0.2263\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 614ms/step - loss: 0.2130\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 623ms/step - loss: 0.2049\n",
            "     train: 0.16371132930481805 \n",
            "validation: 0.42349995793587675 \n",
            "\n",
            "Train from 1380 to 1560 and validate for 1561 to 1620\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 616ms/step - loss: 0.2953\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 597ms/step - loss: 0.2548\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.2315\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 606ms/step - loss: 0.2147\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 609ms/step - loss: 0.2047\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 594ms/step - loss: 0.1859\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 616ms/step - loss: 0.1792\n",
            "     train: 0.1474513715738495 \n",
            "validation: 0.3227751453432695 \n",
            "\n",
            "Train from 1440 to 1620 and validate for 1621 to 1680\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 592ms/step - loss: 0.2333\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.2087\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 588ms/step - loss: 0.1880\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 588ms/step - loss: 0.1766\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 600ms/step - loss: 0.1624\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 587ms/step - loss: 0.1567\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.1462\n",
            "     train: 0.12136748390466656 \n",
            "validation: 0.3103820227676 \n",
            "\n",
            "Train from 1500 to 1680 and validate for 1681 to 1740\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 590ms/step - loss: 0.1857\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 589ms/step - loss: 0.1718\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.1580\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 602ms/step - loss: 0.1504\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 594ms/step - loss: 0.1425\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 602ms/step - loss: 0.1355\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 615ms/step - loss: 0.1340\n",
            "     train: 0.10996184153702132 \n",
            "validation: 0.3371202544316123 \n",
            "\n",
            "Train from 1560 to 1740 and validate for 1741 to 1800\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 581ms/step - loss: 0.1945\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.1881\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.1722\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 589ms/step - loss: 0.1688\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 585ms/step - loss: 0.1579\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 600ms/step - loss: 0.1524\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 590ms/step - loss: 0.1446\n",
            "     train: 0.12789109615342203 \n",
            "validation: 0.44407384589227067 \n",
            "\n",
            "Train from 1620 to 1800 and validate for 1801 to 1860\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 581ms/step - loss: 0.2603\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 594ms/step - loss: 0.2457\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 589ms/step - loss: 0.2229\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 588ms/step - loss: 0.2078\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 585ms/step - loss: 0.1916\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 584ms/step - loss: 0.1914\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 594ms/step - loss: 0.1792\n",
            "     train: 0.15271585421022993 \n",
            "validation: 1.1759590491823761 \n",
            "\n",
            "Train from 1680 to 1860 and validate for 1861 to 1920\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 577ms/step - loss: 0.5199\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 593ms/step - loss: 0.4532\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 581ms/step - loss: 0.4193\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 586ms/step - loss: 0.3548\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 583ms/step - loss: 0.3031\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 587ms/step - loss: 0.2605\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 594ms/step - loss: 0.2312\n",
            "     train: 0.1807399911003101 \n",
            "validation: 0.5458102130640062 \n",
            "\n",
            "Train from 1740 to 1920 and validate for 1921 to 1980\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 600ms/step - loss: 0.3574\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 590ms/step - loss: 0.3320\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 581ms/step - loss: 0.2908\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.2749\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 590ms/step - loss: 0.2455\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 574ms/step - loss: 0.2372\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 604ms/step - loss: 0.2157\n",
            "     train: 0.18241107897951037 \n",
            "validation: 0.4727062713097672 \n",
            "\n",
            "Train from 1800 to 1980 and validate for 1981 to 2040\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 583ms/step - loss: 0.3125\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 586ms/step - loss: 0.2696\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 622ms/step - loss: 0.2431\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 598ms/step - loss: 0.2285\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 608ms/step - loss: 0.2158\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.2041\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 600ms/step - loss: 0.1897\n",
            "     train: 0.16082160365316078 \n",
            "validation: 1.9481462342063145 \n",
            "\n",
            "Train from 1860 to 2040 and validate for 2041 to 2100\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 586ms/step - loss: 0.7933\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.6782\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 620ms/step - loss: 0.5818\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 612ms/step - loss: 0.4662\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 592ms/step - loss: 0.3754\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.3495\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.3111\n",
            "     train: 0.23139323650770208 \n",
            "validation: 0.7641700008286086 \n",
            "\n",
            "Train from 1920 to 2100 and validate for 2101 to 2160\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 586ms/step - loss: 0.4447\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 601ms/step - loss: 0.3307\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 617ms/step - loss: 0.2805\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 610ms/step - loss: 0.2463\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 584ms/step - loss: 0.2199\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.2088\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 610ms/step - loss: 0.1973\n",
            "     train: 0.1523194160766046 \n",
            "validation: 0.7217100125665588 \n",
            "\n",
            "Train from 1980 to 2160 and validate for 2161 to 2220\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 607ms/step - loss: 0.3768\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.3061\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 585ms/step - loss: 0.2620\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 583ms/step - loss: 0.2506\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 0.2460\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.2225\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 585ms/step - loss: 0.2200\n",
            "     train: 0.1772482026949854 \n",
            "validation: 0.5681480341946828 \n",
            "\n",
            "Train from 2040 to 2220 and validate for 2221 to 2280\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 583ms/step - loss: 0.3400\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.2826\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 0.2557\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 573ms/step - loss: 0.2263\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 570ms/step - loss: 0.2025\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 591ms/step - loss: 0.1861\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 575ms/step - loss: 0.1859\n",
            "     train: 0.1466030030682889 \n",
            "validation: 0.4683529051341863 \n",
            "\n",
            "Train from 2100 to 2280 and validate for 2281 to 2340\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 564ms/step - loss: 0.2832\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 554ms/step - loss: 0.2289\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 3s 550ms/step - loss: 0.2083\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 575ms/step - loss: 0.1977\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 556ms/step - loss: 0.1837\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 568ms/step - loss: 0.1720\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 564ms/step - loss: 0.1591\n",
            "     train: 0.13817038223296493 \n",
            "validation: 6.309177644238571 \n",
            "\n",
            "Train from 2160 to 2340 and validate for 2341 to 2400\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 575ms/step - loss: 2.1609\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 3s 570ms/step - loss: 1.9082\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 592ms/step - loss: 1.4328\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 3s 566ms/step - loss: 1.1636\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.8884\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.7719\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 3s 567ms/step - loss: 0.6892\n",
            "     train: 0.697635865677935 \n",
            "validation: 3.4414882065551544 \n",
            "\n",
            "Train from 2220 to 2400 and validate for 2401 to 2460\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 3s 576ms/step - loss: 1.5969\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 585ms/step - loss: 1.3294\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 588ms/step - loss: 1.0615\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 588ms/step - loss: 0.8479\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 3s 573ms/step - loss: 0.7672\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 4s 594ms/step - loss: 0.6498\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 595ms/step - loss: 0.6232\n",
            "     train: 0.5285123913226031 \n",
            "validation: 1.1905651230297596 \n",
            "\n",
            "Train from 2280 to 2460 and validate for 2461 to 2520\n",
            "Epoch 1/7\n",
            "6/6 [==============================] - 4s 599ms/step - loss: 0.8485\n",
            "Epoch 2/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.7857\n",
            "Epoch 3/7\n",
            "6/6 [==============================] - 4s 590ms/step - loss: 0.6757\n",
            "Epoch 4/7\n",
            "6/6 [==============================] - 4s 603ms/step - loss: 0.6300\n",
            "Epoch 5/7\n",
            "6/6 [==============================] - 4s 596ms/step - loss: 0.6179\n",
            "Epoch 6/7\n",
            "6/6 [==============================] - 3s 581ms/step - loss: 0.5981\n",
            "Epoch 7/7\n",
            "6/6 [==============================] - 4s 592ms/step - loss: 0.6035\n",
            "     train: 0.4943140340537295 \n",
            "validation: 1.4368209559631857 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "0uZlNHMG6gG9",
        "outputId": "8186735c-4625-4c8e-d58a-c300c5e2edd1"
      },
      "source": [
        "pd.DataFrame(scaler.inverse_transform(train_pred_trained)+1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.008906</td>\n",
              "      <td>1.008663</td>\n",
              "      <td>1.016516</td>\n",
              "      <td>1.006302</td>\n",
              "      <td>1.006992</td>\n",
              "      <td>1.009500</td>\n",
              "      <td>1.010035</td>\n",
              "      <td>1.004776</td>\n",
              "      <td>1.010345</td>\n",
              "      <td>1.004994</td>\n",
              "      <td>1.005722</td>\n",
              "      <td>1.011072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.015726</td>\n",
              "      <td>1.012708</td>\n",
              "      <td>1.030993</td>\n",
              "      <td>1.019245</td>\n",
              "      <td>1.013907</td>\n",
              "      <td>1.024698</td>\n",
              "      <td>1.017327</td>\n",
              "      <td>1.015534</td>\n",
              "      <td>1.018949</td>\n",
              "      <td>1.010103</td>\n",
              "      <td>1.013376</td>\n",
              "      <td>1.026368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.010427</td>\n",
              "      <td>1.011111</td>\n",
              "      <td>1.015287</td>\n",
              "      <td>1.008230</td>\n",
              "      <td>1.006276</td>\n",
              "      <td>1.013577</td>\n",
              "      <td>1.010613</td>\n",
              "      <td>1.008247</td>\n",
              "      <td>1.009113</td>\n",
              "      <td>1.007270</td>\n",
              "      <td>1.009284</td>\n",
              "      <td>1.013095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.003794</td>\n",
              "      <td>1.001518</td>\n",
              "      <td>1.007417</td>\n",
              "      <td>0.998763</td>\n",
              "      <td>1.003612</td>\n",
              "      <td>1.001927</td>\n",
              "      <td>1.003831</td>\n",
              "      <td>1.001268</td>\n",
              "      <td>1.002545</td>\n",
              "      <td>1.003609</td>\n",
              "      <td>1.003189</td>\n",
              "      <td>1.004831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.988154</td>\n",
              "      <td>0.989676</td>\n",
              "      <td>0.986491</td>\n",
              "      <td>0.984382</td>\n",
              "      <td>0.991009</td>\n",
              "      <td>0.979820</td>\n",
              "      <td>0.987943</td>\n",
              "      <td>0.997553</td>\n",
              "      <td>0.979347</td>\n",
              "      <td>0.997385</td>\n",
              "      <td>0.996468</td>\n",
              "      <td>0.991854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7015</th>\n",
              "      <td>1.002259</td>\n",
              "      <td>1.007308</td>\n",
              "      <td>0.982462</td>\n",
              "      <td>0.989313</td>\n",
              "      <td>1.008395</td>\n",
              "      <td>0.998179</td>\n",
              "      <td>1.002035</td>\n",
              "      <td>1.007547</td>\n",
              "      <td>1.000570</td>\n",
              "      <td>1.009927</td>\n",
              "      <td>1.004589</td>\n",
              "      <td>1.001759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7016</th>\n",
              "      <td>0.990869</td>\n",
              "      <td>0.998825</td>\n",
              "      <td>0.971029</td>\n",
              "      <td>0.975947</td>\n",
              "      <td>0.987920</td>\n",
              "      <td>0.980295</td>\n",
              "      <td>0.993379</td>\n",
              "      <td>0.986257</td>\n",
              "      <td>0.985383</td>\n",
              "      <td>0.992853</td>\n",
              "      <td>0.990931</td>\n",
              "      <td>0.980423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7017</th>\n",
              "      <td>0.984941</td>\n",
              "      <td>0.992160</td>\n",
              "      <td>0.953221</td>\n",
              "      <td>0.963255</td>\n",
              "      <td>0.989227</td>\n",
              "      <td>0.972298</td>\n",
              "      <td>0.988260</td>\n",
              "      <td>0.993118</td>\n",
              "      <td>0.971921</td>\n",
              "      <td>0.994564</td>\n",
              "      <td>0.990175</td>\n",
              "      <td>0.988682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7018</th>\n",
              "      <td>0.960933</td>\n",
              "      <td>0.962267</td>\n",
              "      <td>0.913165</td>\n",
              "      <td>0.941112</td>\n",
              "      <td>0.969350</td>\n",
              "      <td>0.938529</td>\n",
              "      <td>0.960103</td>\n",
              "      <td>0.967008</td>\n",
              "      <td>0.946122</td>\n",
              "      <td>0.978518</td>\n",
              "      <td>0.960568</td>\n",
              "      <td>0.956065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7019</th>\n",
              "      <td>0.932591</td>\n",
              "      <td>0.939809</td>\n",
              "      <td>0.861364</td>\n",
              "      <td>0.904110</td>\n",
              "      <td>0.950113</td>\n",
              "      <td>0.917385</td>\n",
              "      <td>0.934320</td>\n",
              "      <td>0.945777</td>\n",
              "      <td>0.923886</td>\n",
              "      <td>0.963808</td>\n",
              "      <td>0.935650</td>\n",
              "      <td>0.928406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7020 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "0     1.008906  1.008663  1.016516  ...  1.004994  1.005722  1.011072\n",
              "1     1.015726  1.012708  1.030993  ...  1.010103  1.013376  1.026368\n",
              "2     1.010427  1.011111  1.015287  ...  1.007270  1.009284  1.013095\n",
              "3     1.003794  1.001518  1.007417  ...  1.003609  1.003189  1.004831\n",
              "4     0.988154  0.989676  0.986491  ...  0.997385  0.996468  0.991854\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "7015  1.002259  1.007308  0.982462  ...  1.009927  1.004589  1.001759\n",
              "7016  0.990869  0.998825  0.971029  ...  0.992853  0.990931  0.980423\n",
              "7017  0.984941  0.992160  0.953221  ...  0.994564  0.990175  0.988682\n",
              "7018  0.960933  0.962267  0.913165  ...  0.978518  0.960568  0.956065\n",
              "7019  0.932591  0.939809  0.861364  ...  0.963808  0.935650  0.928406\n",
              "\n",
              "[7020 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "A9aTV_UK6gG9",
        "outputId": "f920e33f-e70e-49a6-ec4f-65f100977e3f"
      },
      "source": [
        "pd.DataFrame(scaler.inverse_transform(val_pred_trained)+1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.980340</td>\n",
              "      <td>0.978696</td>\n",
              "      <td>0.977930</td>\n",
              "      <td>0.964980</td>\n",
              "      <td>0.981613</td>\n",
              "      <td>0.966929</td>\n",
              "      <td>0.976902</td>\n",
              "      <td>0.984380</td>\n",
              "      <td>0.978468</td>\n",
              "      <td>0.987711</td>\n",
              "      <td>0.983910</td>\n",
              "      <td>0.976516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.007134</td>\n",
              "      <td>1.005375</td>\n",
              "      <td>1.012762</td>\n",
              "      <td>1.010246</td>\n",
              "      <td>1.005893</td>\n",
              "      <td>1.008425</td>\n",
              "      <td>1.007045</td>\n",
              "      <td>1.011490</td>\n",
              "      <td>1.003330</td>\n",
              "      <td>1.009841</td>\n",
              "      <td>1.010821</td>\n",
              "      <td>1.014881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.985539</td>\n",
              "      <td>0.989130</td>\n",
              "      <td>0.980321</td>\n",
              "      <td>0.978383</td>\n",
              "      <td>0.989299</td>\n",
              "      <td>0.978776</td>\n",
              "      <td>0.986874</td>\n",
              "      <td>0.996835</td>\n",
              "      <td>0.982431</td>\n",
              "      <td>0.995107</td>\n",
              "      <td>0.993456</td>\n",
              "      <td>0.982913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.005230</td>\n",
              "      <td>1.012054</td>\n",
              "      <td>1.013859</td>\n",
              "      <td>1.009503</td>\n",
              "      <td>1.006733</td>\n",
              "      <td>1.017796</td>\n",
              "      <td>1.013596</td>\n",
              "      <td>1.020197</td>\n",
              "      <td>1.008158</td>\n",
              "      <td>1.005533</td>\n",
              "      <td>1.009081</td>\n",
              "      <td>1.016441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.995514</td>\n",
              "      <td>0.994851</td>\n",
              "      <td>0.993424</td>\n",
              "      <td>0.989105</td>\n",
              "      <td>0.995862</td>\n",
              "      <td>0.991546</td>\n",
              "      <td>0.994622</td>\n",
              "      <td>1.002550</td>\n",
              "      <td>0.990351</td>\n",
              "      <td>1.001488</td>\n",
              "      <td>0.999153</td>\n",
              "      <td>0.993511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2274</th>\n",
              "      <td>1.008601</td>\n",
              "      <td>1.014552</td>\n",
              "      <td>0.994254</td>\n",
              "      <td>1.001706</td>\n",
              "      <td>1.013179</td>\n",
              "      <td>1.004730</td>\n",
              "      <td>1.008902</td>\n",
              "      <td>1.008404</td>\n",
              "      <td>1.007071</td>\n",
              "      <td>1.013157</td>\n",
              "      <td>1.003748</td>\n",
              "      <td>1.008474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2275</th>\n",
              "      <td>0.979647</td>\n",
              "      <td>0.987842</td>\n",
              "      <td>0.955669</td>\n",
              "      <td>0.963329</td>\n",
              "      <td>0.982458</td>\n",
              "      <td>0.970807</td>\n",
              "      <td>0.980951</td>\n",
              "      <td>0.975655</td>\n",
              "      <td>0.967841</td>\n",
              "      <td>0.989195</td>\n",
              "      <td>0.982011</td>\n",
              "      <td>0.967332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2276</th>\n",
              "      <td>1.006228</td>\n",
              "      <td>1.013326</td>\n",
              "      <td>0.990102</td>\n",
              "      <td>0.993171</td>\n",
              "      <td>1.005824</td>\n",
              "      <td>0.999496</td>\n",
              "      <td>1.006580</td>\n",
              "      <td>1.008291</td>\n",
              "      <td>1.006303</td>\n",
              "      <td>1.010767</td>\n",
              "      <td>1.007494</td>\n",
              "      <td>1.001519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2277</th>\n",
              "      <td>1.020187</td>\n",
              "      <td>1.022118</td>\n",
              "      <td>1.005721</td>\n",
              "      <td>1.018025</td>\n",
              "      <td>1.017046</td>\n",
              "      <td>1.018635</td>\n",
              "      <td>1.019288</td>\n",
              "      <td>1.015648</td>\n",
              "      <td>1.019752</td>\n",
              "      <td>1.013630</td>\n",
              "      <td>1.017734</td>\n",
              "      <td>1.016327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2278</th>\n",
              "      <td>1.022234</td>\n",
              "      <td>1.027548</td>\n",
              "      <td>1.017188</td>\n",
              "      <td>1.022243</td>\n",
              "      <td>1.017643</td>\n",
              "      <td>1.022582</td>\n",
              "      <td>1.028632</td>\n",
              "      <td>1.017010</td>\n",
              "      <td>1.029166</td>\n",
              "      <td>1.014161</td>\n",
              "      <td>1.023738</td>\n",
              "      <td>1.020636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2279 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        9         10        11\n",
              "0     0.980340  0.978696  0.977930  ...  0.987711  0.983910  0.976516\n",
              "1     1.007134  1.005375  1.012762  ...  1.009841  1.010821  1.014881\n",
              "2     0.985539  0.989130  0.980321  ...  0.995107  0.993456  0.982913\n",
              "3     1.005230  1.012054  1.013859  ...  1.005533  1.009081  1.016441\n",
              "4     0.995514  0.994851  0.993424  ...  1.001488  0.999153  0.993511\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "2274  1.008601  1.014552  0.994254  ...  1.013157  1.003748  1.008474\n",
              "2275  0.979647  0.987842  0.955669  ...  0.989195  0.982011  0.967332\n",
              "2276  1.006228  1.013326  0.990102  ...  1.010767  1.007494  1.001519\n",
              "2277  1.020187  1.022118  1.005721  ...  1.013630  1.017734  1.016327\n",
              "2278  1.022234  1.027548  1.017188  ...  1.014161  1.023738  1.020636\n",
              "\n",
              "[2279 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wyI8y_2S6gG9",
        "outputId": "17bdec53-2d93-4520-b9c8-c76af8fe9045"
      },
      "source": [
        "pd.DataFrame(validation_metrics_trained)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.689040</td>\n",
              "      <td>1.454005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.319068</td>\n",
              "      <td>1.071913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.241110</td>\n",
              "      <td>1.155555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.177615</td>\n",
              "      <td>0.575397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.145711</td>\n",
              "      <td>0.641128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.135061</td>\n",
              "      <td>0.380855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.111592</td>\n",
              "      <td>0.575036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.134223</td>\n",
              "      <td>0.498876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.138715</td>\n",
              "      <td>0.412231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.112647</td>\n",
              "      <td>0.521387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.099969</td>\n",
              "      <td>0.407349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.102317</td>\n",
              "      <td>0.331456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.102759</td>\n",
              "      <td>0.646603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.138903</td>\n",
              "      <td>0.748582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.163984</td>\n",
              "      <td>0.586924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.184785</td>\n",
              "      <td>0.518923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.155381</td>\n",
              "      <td>1.275534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.178163</td>\n",
              "      <td>0.787685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.166957</td>\n",
              "      <td>1.037645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.207379</td>\n",
              "      <td>0.529219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.183496</td>\n",
              "      <td>0.584236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.169987</td>\n",
              "      <td>0.624982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.163711</td>\n",
              "      <td>0.423500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.147451</td>\n",
              "      <td>0.322775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.121367</td>\n",
              "      <td>0.310382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.109962</td>\n",
              "      <td>0.337120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.127891</td>\n",
              "      <td>0.444074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.152716</td>\n",
              "      <td>1.175959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.180740</td>\n",
              "      <td>0.545810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.182411</td>\n",
              "      <td>0.472706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.160822</td>\n",
              "      <td>1.948146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.231393</td>\n",
              "      <td>0.764170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.152319</td>\n",
              "      <td>0.721710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.177248</td>\n",
              "      <td>0.568148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.146603</td>\n",
              "      <td>0.468353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.138170</td>\n",
              "      <td>6.309178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.697636</td>\n",
              "      <td>3.441488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.528512</td>\n",
              "      <td>1.190565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.494314</td>\n",
              "      <td>1.436821</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1\n",
              "0   0.689040  1.454005\n",
              "1   0.319068  1.071913\n",
              "2   0.241110  1.155555\n",
              "3   0.177615  0.575397\n",
              "4   0.145711  0.641128\n",
              "5   0.135061  0.380855\n",
              "6   0.111592  0.575036\n",
              "7   0.134223  0.498876\n",
              "8   0.138715  0.412231\n",
              "9   0.112647  0.521387\n",
              "10  0.099969  0.407349\n",
              "11  0.102317  0.331456\n",
              "12  0.102759  0.646603\n",
              "13  0.138903  0.748582\n",
              "14  0.163984  0.586924\n",
              "15  0.184785  0.518923\n",
              "16  0.155381  1.275534\n",
              "17  0.178163  0.787685\n",
              "18  0.166957  1.037645\n",
              "19  0.207379  0.529219\n",
              "20  0.183496  0.584236\n",
              "21  0.169987  0.624982\n",
              "22  0.163711  0.423500\n",
              "23  0.147451  0.322775\n",
              "24  0.121367  0.310382\n",
              "25  0.109962  0.337120\n",
              "26  0.127891  0.444074\n",
              "27  0.152716  1.175959\n",
              "28  0.180740  0.545810\n",
              "29  0.182411  0.472706\n",
              "30  0.160822  1.948146\n",
              "31  0.231393  0.764170\n",
              "32  0.152319  0.721710\n",
              "33  0.177248  0.568148\n",
              "34  0.146603  0.468353\n",
              "35  0.138170  6.309178\n",
              "36  0.697636  3.441488\n",
              "37  0.528512  1.190565\n",
              "38  0.494314  1.436821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "X0nS4PCz8PGy",
        "outputId": "00b33c6d-fe23-460e-82ac-3fb828f04d09"
      },
      "source": [
        "import plotly.express as px\n",
        "# import untrainable validation metrics\n",
        "#un_val = pd.read_csv('validation_metrics_trainable_false.csv', names=['quarters', 'train_un', 'validation_un'], header=0)\n",
        "un_val = pd.DataFrame(validation_metrics)\n",
        "un_val.columns = ['train_un', 'validation_un']\n",
        "# import trainable validation metrics\n",
        "#tr_val = pd.read_csv('validation_metrics_train.csv', names=['train', 'validation'], header=0, usecols=[1,2])\n",
        "tr_val = pd.DataFrame(validation_metrics_trained)\n",
        "tr_val.columns = ['train', 'validation']\n",
        "\n",
        "# The two dataframes are merged in order to be able to plot training and validation together\n",
        "val_metrics = pd.DataFrame(pd.concat([un_val, tr_val], axis=1))\n",
        "val_metrics = val_metrics.reset_index(drop=False)\n",
        "\n",
        "# Plot untrained and trained validation sets\n",
        "\n",
        "fig4 = px.line(val_metrics, x='index', y=['validation_un', 'validation'], color_discrete_sequence=['cornflowerblue', 'indigo'])\n",
        "fig4.update_xaxes(title_text='Quarters', showgrid=False)\n",
        "fig4.update_yaxes(title_text='MSE')\n",
        "fig4.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>                <div id=\"c3c8ab88-cc86-4950-9d71-73ced6f691cb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c3c8ab88-cc86-4950-9d71-73ced6f691cb\")) {                    Plotly.newPlot(                        \"c3c8ab88-cc86-4950-9d71-73ced6f691cb\",                        [{\"hovertemplate\": \"variable=validation_un<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"validation_un\", \"line\": {\"color\": \"cornflowerblue\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"validation_un\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38], \"xaxis\": \"x\", \"y\": [1.3387487139963603, 0.4921329955089613, 1.2590004770169247, 0.6691865915182179, 0.6406233376535638, 0.4603302048245203, 0.5588351004793785, 0.6210871689563503, 0.5378193898375742, 0.6344160535606432, 0.43000698217432937, 0.3210326391284614, 0.5971669624066948, 0.8581219263711751, 0.6230932169524617, 0.5175230482024846, 1.476325603596914, 0.8069971192977264, 1.0981929026916644, 0.5890652123639752, 0.7072763401811465, 0.6622451356182867, 0.4345882912567475, 0.25937146539632583, 0.32468151251282057, 0.2995191464688212, 0.44363051473705406, 1.1714895734815416, 0.5870825211824885, 0.49999803544450083, 1.9391708213625127, 1.3529327899276282, 0.7196225507095709, 0.7180580572117267, 0.45527323651882695, 6.444237769790303, 3.3008498723828215, 1.439526537553425, 2.151475645766832], \"yaxis\": \"y\"}, {\"hovertemplate\": \"variable=validation<br>index=%{x}<br>value=%{y}<extra></extra>\", \"legendgroup\": \"validation\", \"line\": {\"color\": \"indigo\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"validation\", \"orientation\": \"v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38], \"xaxis\": \"x\", \"y\": [1.4540047475432443, 1.071912761662376, 1.1555547810982063, 0.5753965595960432, 0.6411284320881154, 0.38085512410188377, 0.5750359919025446, 0.4988755941495115, 0.41223139916318213, 0.5213870513999431, 0.40734880006364266, 0.3314563359824704, 0.6466034128770243, 0.7485817323922398, 0.5869242568296892, 0.5189229777831776, 1.2755341302338936, 0.7876851978774025, 1.0376450227801397, 0.5292194655307525, 0.5842355526912085, 0.624981781594422, 0.42349995793587675, 0.3227751453432695, 0.3103820227676, 0.3371202544316123, 0.44407384589227067, 1.1759590491823761, 0.5458102130640062, 0.4727062713097672, 1.9481462342063145, 0.7641700008286086, 0.7217100125665588, 0.5681480341946828, 0.4683529051341863, 6.309177644238571, 3.4414882065551544, 1.1905651230297596, 1.4368209559631857], \"yaxis\": \"y\"}],                        {\"legend\": {\"title\": {\"text\": \"variable\"}, \"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"showgrid\": false, \"title\": {\"text\": \"Quarters\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"MSE\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c3c8ab88-cc86-4950-9d71-73ced6f691cb');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SFhCgJnlEJNL",
        "outputId": "f95d5fa0-ba7e-4b2c-c1c0-80624b6bf93b"
      },
      "source": [
        "val_metrics"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>train_un</th>\n",
              "      <th>validation_un</th>\n",
              "      <th>train</th>\n",
              "      <th>validation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.694288</td>\n",
              "      <td>1.338749</td>\n",
              "      <td>0.689040</td>\n",
              "      <td>1.454005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.432780</td>\n",
              "      <td>0.492133</td>\n",
              "      <td>0.319068</td>\n",
              "      <td>1.071913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.290941</td>\n",
              "      <td>1.259000</td>\n",
              "      <td>0.241110</td>\n",
              "      <td>1.155555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.218924</td>\n",
              "      <td>0.669187</td>\n",
              "      <td>0.177615</td>\n",
              "      <td>0.575397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.177799</td>\n",
              "      <td>0.640623</td>\n",
              "      <td>0.145711</td>\n",
              "      <td>0.641128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.175398</td>\n",
              "      <td>0.460330</td>\n",
              "      <td>0.135061</td>\n",
              "      <td>0.380855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.141060</td>\n",
              "      <td>0.558835</td>\n",
              "      <td>0.111592</td>\n",
              "      <td>0.575036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.163898</td>\n",
              "      <td>0.621087</td>\n",
              "      <td>0.134223</td>\n",
              "      <td>0.498876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.189551</td>\n",
              "      <td>0.537819</td>\n",
              "      <td>0.138715</td>\n",
              "      <td>0.412231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.191965</td>\n",
              "      <td>0.634416</td>\n",
              "      <td>0.112647</td>\n",
              "      <td>0.521387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.159813</td>\n",
              "      <td>0.430007</td>\n",
              "      <td>0.099969</td>\n",
              "      <td>0.407349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.153681</td>\n",
              "      <td>0.321033</td>\n",
              "      <td>0.102317</td>\n",
              "      <td>0.331456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>0.139671</td>\n",
              "      <td>0.597167</td>\n",
              "      <td>0.102759</td>\n",
              "      <td>0.646603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.171046</td>\n",
              "      <td>0.858122</td>\n",
              "      <td>0.138903</td>\n",
              "      <td>0.748582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>0.194247</td>\n",
              "      <td>0.623093</td>\n",
              "      <td>0.163984</td>\n",
              "      <td>0.586924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>0.203192</td>\n",
              "      <td>0.517523</td>\n",
              "      <td>0.184785</td>\n",
              "      <td>0.518923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>0.193657</td>\n",
              "      <td>1.476326</td>\n",
              "      <td>0.155381</td>\n",
              "      <td>1.275534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>0.237454</td>\n",
              "      <td>0.806997</td>\n",
              "      <td>0.178163</td>\n",
              "      <td>0.787685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>0.221259</td>\n",
              "      <td>1.098193</td>\n",
              "      <td>0.166957</td>\n",
              "      <td>1.037645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>0.257190</td>\n",
              "      <td>0.589065</td>\n",
              "      <td>0.207379</td>\n",
              "      <td>0.529219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>0.237517</td>\n",
              "      <td>0.707276</td>\n",
              "      <td>0.183496</td>\n",
              "      <td>0.584236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>0.243027</td>\n",
              "      <td>0.662245</td>\n",
              "      <td>0.169987</td>\n",
              "      <td>0.624982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>0.270884</td>\n",
              "      <td>0.434588</td>\n",
              "      <td>0.163711</td>\n",
              "      <td>0.423500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>0.240994</td>\n",
              "      <td>0.259371</td>\n",
              "      <td>0.147451</td>\n",
              "      <td>0.322775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>0.206303</td>\n",
              "      <td>0.324682</td>\n",
              "      <td>0.121367</td>\n",
              "      <td>0.310382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>0.163148</td>\n",
              "      <td>0.299519</td>\n",
              "      <td>0.109962</td>\n",
              "      <td>0.337120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>0.175227</td>\n",
              "      <td>0.443631</td>\n",
              "      <td>0.127891</td>\n",
              "      <td>0.444074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>0.227277</td>\n",
              "      <td>1.171490</td>\n",
              "      <td>0.152716</td>\n",
              "      <td>1.175959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>28</td>\n",
              "      <td>0.337513</td>\n",
              "      <td>0.587083</td>\n",
              "      <td>0.180740</td>\n",
              "      <td>0.545810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>0.263759</td>\n",
              "      <td>0.499998</td>\n",
              "      <td>0.182411</td>\n",
              "      <td>0.472706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>30</td>\n",
              "      <td>0.216815</td>\n",
              "      <td>1.939171</td>\n",
              "      <td>0.160822</td>\n",
              "      <td>1.948146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>31</td>\n",
              "      <td>0.328884</td>\n",
              "      <td>1.352933</td>\n",
              "      <td>0.231393</td>\n",
              "      <td>0.764170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>32</td>\n",
              "      <td>0.229954</td>\n",
              "      <td>0.719623</td>\n",
              "      <td>0.152319</td>\n",
              "      <td>0.721710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>33</td>\n",
              "      <td>0.221415</td>\n",
              "      <td>0.718058</td>\n",
              "      <td>0.177248</td>\n",
              "      <td>0.568148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>0.188334</td>\n",
              "      <td>0.455273</td>\n",
              "      <td>0.146603</td>\n",
              "      <td>0.468353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>35</td>\n",
              "      <td>0.177338</td>\n",
              "      <td>6.444238</td>\n",
              "      <td>0.138170</td>\n",
              "      <td>6.309178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>0.839534</td>\n",
              "      <td>3.300850</td>\n",
              "      <td>0.697636</td>\n",
              "      <td>3.441488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>37</td>\n",
              "      <td>0.755544</td>\n",
              "      <td>1.439527</td>\n",
              "      <td>0.528512</td>\n",
              "      <td>1.190565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>0.680120</td>\n",
              "      <td>2.151476</td>\n",
              "      <td>0.494314</td>\n",
              "      <td>1.436821</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  train_un  validation_un     train  validation\n",
              "0       0  0.694288       1.338749  0.689040    1.454005\n",
              "1       1  0.432780       0.492133  0.319068    1.071913\n",
              "2       2  0.290941       1.259000  0.241110    1.155555\n",
              "3       3  0.218924       0.669187  0.177615    0.575397\n",
              "4       4  0.177799       0.640623  0.145711    0.641128\n",
              "5       5  0.175398       0.460330  0.135061    0.380855\n",
              "6       6  0.141060       0.558835  0.111592    0.575036\n",
              "7       7  0.163898       0.621087  0.134223    0.498876\n",
              "8       8  0.189551       0.537819  0.138715    0.412231\n",
              "9       9  0.191965       0.634416  0.112647    0.521387\n",
              "10     10  0.159813       0.430007  0.099969    0.407349\n",
              "11     11  0.153681       0.321033  0.102317    0.331456\n",
              "12     12  0.139671       0.597167  0.102759    0.646603\n",
              "13     13  0.171046       0.858122  0.138903    0.748582\n",
              "14     14  0.194247       0.623093  0.163984    0.586924\n",
              "15     15  0.203192       0.517523  0.184785    0.518923\n",
              "16     16  0.193657       1.476326  0.155381    1.275534\n",
              "17     17  0.237454       0.806997  0.178163    0.787685\n",
              "18     18  0.221259       1.098193  0.166957    1.037645\n",
              "19     19  0.257190       0.589065  0.207379    0.529219\n",
              "20     20  0.237517       0.707276  0.183496    0.584236\n",
              "21     21  0.243027       0.662245  0.169987    0.624982\n",
              "22     22  0.270884       0.434588  0.163711    0.423500\n",
              "23     23  0.240994       0.259371  0.147451    0.322775\n",
              "24     24  0.206303       0.324682  0.121367    0.310382\n",
              "25     25  0.163148       0.299519  0.109962    0.337120\n",
              "26     26  0.175227       0.443631  0.127891    0.444074\n",
              "27     27  0.227277       1.171490  0.152716    1.175959\n",
              "28     28  0.337513       0.587083  0.180740    0.545810\n",
              "29     29  0.263759       0.499998  0.182411    0.472706\n",
              "30     30  0.216815       1.939171  0.160822    1.948146\n",
              "31     31  0.328884       1.352933  0.231393    0.764170\n",
              "32     32  0.229954       0.719623  0.152319    0.721710\n",
              "33     33  0.221415       0.718058  0.177248    0.568148\n",
              "34     34  0.188334       0.455273  0.146603    0.468353\n",
              "35     35  0.177338       6.444238  0.138170    6.309178\n",
              "36     36  0.839534       3.300850  0.697636    3.441488\n",
              "37     37  0.755544       1.439527  0.528512    1.190565\n",
              "38     38  0.680120       2.151476  0.494314    1.436821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}