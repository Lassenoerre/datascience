{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5OX6x3b6gG6"
   },
   "source": [
    "# 1. Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csrQZGwb6gG6"
   },
   "source": [
    "## 1.1 Crawling CNBC to get the article links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSwsI-856gG6"
   },
   "outputs": [],
   "source": [
    "def get_links_to_scrape(start_year, end_year):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes a starting year and an ending year and crawls through\n",
    "    CNBC's SiteMap structure to find all links to articles that are written\n",
    "    in the given time period.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import of the required packages\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "    # First, we define every month and year that should be scraped.\n",
    "    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    years = [year for year in range(start_year, end_year+1)]\n",
    "\n",
    "    # In order to scrape the links faster, we make use of SoupStrainer\n",
    "    # to only parse the relevant part of the site: ALl the article links.\n",
    "    strainer = SoupStrainer('a', class_ = 'SiteMapArticleList-link')\n",
    "\n",
    "    # We instantiate an empty list that we can store all the links in.\n",
    "    article_links = []\n",
    "\n",
    "    # For every day, in every month in every year, construct a link\n",
    "    # that should be scraped. We do this, because CNBC's link structure\n",
    "    # in their archive is given af .../site-map/YEAR/MONTH/DAY_OF_MONTH.\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in range(1,32):\n",
    "                link = f'https://www.cnbc.com/site-map/{year}/{month}/{day}/'\n",
    "\n",
    "                # Take the generated link and load it in through requests,\n",
    "                # then take the contents and convert it to a string.\n",
    "                page = str(requests.get(link).content)\n",
    "\n",
    "                # CNBC load in their CSS directly into the HTML, with usually\n",
    "                # would make it very heavy to scrape. We can fix this, simply\n",
    "                # by cutting remove the entire <head></head> part of the html.\n",
    "                page = page[str(page).find('body'):]\n",
    "\n",
    "                # Now, pass the remaining content through BeautifulSoup's parser\n",
    "                # and apply the strainer. This means it will only parse link\n",
    "                # elements with the class of SiteMapArticleList-Link and it\n",
    "                # will only search through body-section of the document.\n",
    "                soup = BeautifulSoup(page, parse_only = strainer)\n",
    "\n",
    "                # Now we make use of the find_all function in BeautifulSoup\n",
    "                # that simply generates a list of codeblocks that match the\n",
    "                # query. This means we get a list of links, and we store the\n",
    "                # href (actual links) in our instantiated list from before.\n",
    "                links = soup.find_all('a', class_='SiteMapArticleList-link')\n",
    "                if links != []:\n",
    "                    for a_link in links:\n",
    "                        article_links.append(a_link['href'])\n",
    "\n",
    "    # Return the final list of article links that we want to scrape.\n",
    "    return article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrWpdIcT6gG6"
   },
   "outputs": [],
   "source": [
    "article_links = get_links_to_scrape(2006, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdU515V_6gG7"
   },
   "source": [
    "## 1.2 Scrape articles from link list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceWCsCu26gG7"
   },
   "outputs": [],
   "source": [
    "def scrape_cnbc_articles(list_of_links):\n",
    "\n",
    "    \"\"\"\n",
    "    This function take a list of article links and attempts to scrape\n",
    "    them according to CNBC's HTML structure. It will return a pandas\n",
    "    DataFrame with the scraped data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import of the required packages\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup, SoupStrainer\n",
    "    import pandas as pd\n",
    "\n",
    "    # First, we instantiate an empty list, that we can append the scraped\n",
    "    # data to. Then we instantiate an index, which enables us to follow the\n",
    "    # scrapers progress, and lastly, we instantiate a request Session, which\n",
    "    # minimizes our load time per request by little bit, but that time will\n",
    "    # accumulate for all our links and should have quite an impact.\n",
    "    df = []\n",
    "    index = 0\n",
    "    request = requests.Session()\n",
    "\n",
    "    # For every link in the list, get the source code and add 1 to the index.\n",
    "    for link in list_of_links:\n",
    "        page = request.get(link)\n",
    "        index += 1\n",
    "\n",
    "        # If the pages HTTP request Code is 200 (Meaning \"I loaded the site\n",
    "        # correctly, and the server sent me the data correctly\"), then\n",
    "        # attempt to scrape the site.\n",
    "        if page.status_code == 200:\n",
    "            try:\n",
    "\n",
    "                # First, we convert the page content to a string, so we can\n",
    "                # remove most of the irrelevant HTML. We remove everything\n",
    "                # before a box with the class \"MainContent\"\n",
    "                page = str(page.content)\n",
    "                page = page[page.find('<div id=\"MainContent\"'):]\n",
    "\n",
    "                # Now we parse the remaining content of the page into\n",
    "                # BeautifulSoup and try to extract the text of:\n",
    "                soup_link = BeautifulSoup(page)\n",
    "\n",
    "                # The Header-1 tag with a class of 'ArticleHeader-headline'\n",
    "                title = soup_link.find('h1', class_='ArticleHeader-headline').get_text()\n",
    "\n",
    "                # The div (box) tag with a class of 'ArticleBody-articleBody'\n",
    "                article = soup_link.find('div', class_='ArticleBody-articleBody').get_text()\n",
    "\n",
    "                # The date generated from the link. The link will always\n",
    "                # contain the date if it is an article. We save it as\n",
    "                # DD/MM/YYYY.\n",
    "                date = f'{link[29:31]}-{link[26:28]}-{link[21:25]}'\n",
    "\n",
    "                # The link that have a class of 'ArticleHeader-eyebrow',\n",
    "                # which is their article topic.\n",
    "                topic = soup_link.find('a', class_='ArticleHeader-eyebrow').get_text()\n",
    "\n",
    "                # If they are all successfully gathered, we append it all\n",
    "                # into our list called df.\n",
    "                df.append([title, topic, date, article, link])\n",
    "\n",
    "                # If successful, print the progress as well as the link.\n",
    "                print(f'({index}/{len(list_of_links)}) : {link}')\n",
    "            except:\n",
    "                # If we get a status code 200, but somehow some of the elements\n",
    "                # wasn't there, then skip the entire article. This ensures that\n",
    "                # we get a dataset without missing variables.\n",
    "                print(f'({index}/{len(list_of_links)}) : Skipped')\n",
    "        else:\n",
    "            # If we didn't get a status code 200 (Meaning something went wrong\n",
    "            #  in the loading of the page), then skip the article.\n",
    "            print(f'({index}/{len(list_of_links)}) : Skipped')\n",
    "\n",
    "    # Lastly, we return a dataframe that contains all of our scraped articles.\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm72WwD46gG7"
   },
   "outputs": [],
   "source": [
    "df = scrape_cnbc_articles(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNCCxlAH6gG7"
   },
   "source": [
    "## 1.3 Collecting the datasets, if collected over multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gyr8FjXF6gG7"
   },
   "outputs": [],
   "source": [
    "def collect_dataset(datasets_path):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes all the files in a folder and tries to concatenate them\n",
    "    into one dataframe. This is of course only possible if your datasets is\n",
    "    the only csv-files in the folder and if they have the same data-structure.\n",
    "    We made this, because we scraped the links over several sessions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import neccesary packages\n",
    "    import pandas as pd\n",
    "\n",
    "    # List all of our datasets in the folder\n",
    "    datasets = [ds for ds in datasets_path if '.csv' in ds]\n",
    "\n",
    "    # Load all the datasets in and append it to a list called 'frames'. We can\n",
    "    # use this list to concatenate all the dataframes according to the Pandas\n",
    "    # documentation.\n",
    "    frames = []\n",
    "    for dataset in datasets:\n",
    "        df = pd.read_csv(dataset, error_bad_lines=False, index_col=False)\n",
    "        df = df[df.columns[-5:]]\n",
    "        df.columns = ['Title', 'Topic', 'Date', 'Content', 'Link']\n",
    "        frames.append(df)\n",
    "\n",
    "    # Return a concatenated dataframe of all the dataframes.\n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzu4Jj9A6gG7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "df = collect_dataset(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocK1JaFC6gG7"
   },
   "source": [
    "# 2. Preprocessing & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KNBtdJ96gG7"
   },
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].astype('str')\n",
    "df['Title'] = df['Title'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aub7BxXl6gG7"
   },
   "outputs": [],
   "source": [
    "def remove_clutter(text):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes a string and removes what we consider as clutter\n",
    "    in our data. This includes things such as special unicode characters\n",
    "    and video timestamps.\n",
    "    \"\"\"\n",
    "\n",
    "    # Importing neccesary packages\n",
    "    import re\n",
    "\n",
    "    # Trying to remove special unicode characters. Our regular expression finds\n",
    "    # any substrings that starts with a \\x followed by two char/num combinations\n",
    "    text = re.sub(r'\\\\x[A-Za-z0-9_]{2}', '', text)\n",
    "\n",
    "    # Trying to remove video annotation. We are using a regular expression,\n",
    "    # to find any pattern that matches the word video followed by a\n",
    "    # timestamp (length of video). We believe this is just clutter in\n",
    "    # our data as well.\n",
    "    text = re.sub(r'VIDEO([0-9]|[0-9]{2}):[0-9]{4}:[0-9]{2}', ' ', text)\n",
    "\n",
    "    # Trying to remove image references. Whenever an article contains an\n",
    "    # image, the page returns a string representation of the image as the\n",
    "    # source \"Getty Images\". We remove this representation, as it brings\n",
    "    # no value to the analysis.\n",
    "    text = text.replace('Getty Images', '')\n",
    "\n",
    "    # We now remove commas, apostrophes, and double spaces. We introduce\n",
    "    # double spaces in the line above, however this could mess up our\n",
    "    # tokenization, so we simply convert any doublespaces to single spaces.\n",
    "    # We remove apostrophes after n's to normalize contracted words like\n",
    "    # wasn't, couldn't etc. Some of these words are already normalized\n",
    "    # since some of these apostrophes already have been remove by the regex\n",
    "    # unicode decluttering.\n",
    "    text = re.sub(r',','', text)\n",
    "    text = re.sub(r\"n'\",'n', text)\n",
    "    text = re.sub(r'  ',' ', text)\n",
    "\n",
    "    # Finally, we return the decluttered text.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zk64FrPL6gG7"
   },
   "outputs": [],
   "source": [
    "def cleaning(df,column):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes a dataframe and a column name and cleans the entire\n",
    "    column for clutter (using remove_clutter function), then pass it through\n",
    "    spaCy to further clean and tokenize. It returns the original dataframe,\n",
    "    but the given column is now cleared of clutter and it contains two\n",
    "    additional columns (Tokens, cleaned_text) as well.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import neccesary packages\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    import pandas as pd\n",
    "\n",
    "    # First we instantiate a list that we can append all processed tokens in.\n",
    "    # This makes it possible for us to append it to the dataframe at a later\n",
    "    # stage.\n",
    "    tokens = []\n",
    "\n",
    "    # Now, we apply our remove_clutter function to the chosen column in the\n",
    "    # dataframe. This runs the remove_clutter function for every entry in\n",
    "    # the column.\n",
    "    df[column].apply(remove_clutter)\n",
    "\n",
    "    # Define an variable to count the progress of our cleaning.\n",
    "    index = 0\n",
    "\n",
    "    # Now, we iterate over all entries (articles in our case) in the column\n",
    "    # and create a nlp object for each, which we can work with.\n",
    "    for article in nlp.pipe(df[column], disable=['parser']):\n",
    "\n",
    "        # Now, we store all tokens that pass our requirements in a list for each\n",
    "        # article. That means that each article will have their own\n",
    "        # list of tokens.\n",
    "        article_tok = [token.lemma_.lower() for token in article if _\n",
    "            token.is_alpha _\n",
    "            and not token.is_stop _\n",
    "            and token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV', 'VERB'] _\n",
    "            and token.ent_type_ not in ['PERSON', 'MONEY', 'PERCENT', 'LOC', 'DATE', 'TIME', 'QUANTITY', 'ORDINAL'] _\n",
    "            and len(token)>1]\n",
    "\n",
    "        # Now, we append said list of tokens for each article in our tokens list.\n",
    "        tokens.append(article_tok)\n",
    "\n",
    "        # When each article is processed, we increase the index by one and print\n",
    "        # the progress. This allows us to keep track of how far it is in the\n",
    "        # cleaning process. When you are dealing with many thousands of\n",
    "        # articles, it might take a while, so this feature is quite nice.\n",
    "        index += 1\n",
    "        print(f'Processed {index}/{len(df[column])}')\n",
    "\n",
    "    # When all cleaned articles are appended to our tokens list, we simply\n",
    "    # add the list as a column in the original dataframe.\n",
    "    df['tokens'] = tokens\n",
    "\n",
    "    # Lastly, we reconstruct all the articles from the tokens, simply by joining\n",
    "    # all the tokens in each article_tok list. We achieve this by a simple\n",
    "    # combination of map & lambda functions.\n",
    "    df['clean_articles'] = df['tokens'].map(lambda row: \" \".join(row))\n",
    "\n",
    "    # Returning the df that contains cleaned data and new columns.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFJ3GyZe6gG7"
   },
   "outputs": [],
   "source": [
    "df = cleaning(df, 'Content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFofqhZD6gG7"
   },
   "source": [
    "# 3. Attempting to classify topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTd0lPxW6gG7"
   },
   "outputs": [],
   "source": [
    "# Output topics to construct final_topic_list manually\n",
    "pd.DataFrame(df.Topic.unique()).to_csv('unique_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKEbJTno6gG7"
   },
   "outputs": [],
   "source": [
    "# Read topic mapping\n",
    "topic_list = pd.read_csv('final_topic_list.csv', sep = \";\")\n",
    "topic_list_clean = pd.DataFrame.dropna(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1G3lbfr6gG7"
   },
   "outputs": [],
   "source": [
    "# Map Topics to articles\n",
    "predetermined = []\n",
    "index = 0\n",
    "for topic in df[\"Topic\"]:\n",
    "    index += 1\n",
    "    if topic in list(topic_list_clean[\"Topic\"]):\n",
    "        predetermined.append(topic_list_clean[topic_list_clean[\"Topic\"] == topic][\"Predetermined topic\"].to_numpy()[0])\n",
    "    else:\n",
    "        predetermined.append(\"Other\")\n",
    "    print(f'{index}')\n",
    "df['final_topic'] = predetermined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkifbRmn6gG7"
   },
   "outputs": [],
   "source": [
    "# Splitting to train and predict\n",
    "df_labelled = df[df['final_topic'] != 'Other']\n",
    "df_predict = df[df['final_topic'] == 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp_F8gfn6gG7"
   },
   "outputs": [],
   "source": [
    "# Split training into train and test and construct LSA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "r_state = 123\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_labelled['tokens'], df_labelled['final_topic'], test_size=0.25, random_state=r_state)\n",
    "\n",
    "from gensim import models, corpora\n",
    "import ast\n",
    "\n",
    "data_processed = x_train.to_numpy()\n",
    "data_conversion = []\n",
    "for line in data_processed:\n",
    "    line = ast.literal_eval(line)\n",
    "    data_conversion.append(line)\n",
    "data_processed = data_conversion\n",
    "\n",
    "dictionary = corpora.Dictionary(data_processed)\n",
    "corpus = [dictionary.doc2bow(line) for line in data_processed]\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "lsa_model = models.LsiModel(tfidf[corpus], id2word=dictionary, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPeG5aVf6gG7"
   },
   "outputs": [],
   "source": [
    "# Evaluate XGBoost and conclude that it fails...\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(random_state = r_state)\n",
    "xgb_model.fit(x_train, y_train)\n",
    "xgb_prediction = xgb_model.predict(x_test)\n",
    "print(classification_report(y_test,xgb_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rhuviNn6gG7"
   },
   "source": [
    "## 3.1 Filtering articles instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOcLtoXY6gG7"
   },
   "outputs": [],
   "source": [
    "df = df[df['final_topic'] != 'Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85amaHml6gG7"
   },
   "source": [
    "# 4. Collecting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DofYZs246gG7"
   },
   "source": [
    "## 4.1 Collecting articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4v1jg076gG7"
   },
   "outputs": [],
   "source": [
    "def rolling_articles(start_date, end_date, df, start_range, end_range):\n",
    "\n",
    "    \"\"\"\n",
    "    Just like the rolling returns, this function concatenates all the articles\n",
    "    into a dataframe consisting of dates as rows. Intially, we played with the\n",
    "    thought of concatenating the articles rolling with a weeks lag, which is\n",
    "    why it supprts end and start range, however this demanded too much\n",
    "    compututional power for our time scope, which is why we simply used\n",
    "    concatenated them daily.\n",
    "    \"\"\"\n",
    "\n",
    "    # Importing neccesary packages\n",
    "    from datetime import timedelta, datetime\n",
    "    import ast\n",
    "\n",
    "    # Generating a list of dates that we can use to filter the articles from.\n",
    "    date_list = [start_date + timedelta(days=x) for x in range(0,int((end_date - start_date).days)+1)]\n",
    "\n",
    "    # Converting all date-strings in date column to actual date objects\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "\n",
    "    # Generate new dataframe and instantiating a count variable that we can use\n",
    "    # to display the progress whilst running it.\n",
    "    date_index = []\n",
    "    count = 0\n",
    "\n",
    "    # For every date in our generated list of dates, find all the articles that\n",
    "    # lies within the range. Then take their tokens (because of re-import,\n",
    "    # these were actually a string) and convert to a string objects without\n",
    "    # list characters. Also, if any date has more than 30 articles,\n",
    "    # just take the 30 first articles. We integrated the last condition because\n",
    "    # of diminishing marginal benefit compared to the extra compututional effort.\n",
    "    for date in date_list:\n",
    "\n",
    "        # Here we get the articles\n",
    "        articles = df[(df['Date'] <= date + timedelta(days=end_range)) & (df['Date'] >= date + timedelta(days=start_range))].head(30*(1+end_range-start_range))\n",
    "        count += len(articles)\n",
    "        processed = \"\"\n",
    "        for article in articles['tokens']:\n",
    "            try:\n",
    "                # Here we attempt to remove the list characters. I didn't matter\n",
    "                # if we passed our articles as tokens or strings to gensim's\n",
    "                # Word2Vec algo, so we chose as string for the ease of it.\n",
    "                article = str(article).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\",\"\")\n",
    "                processed += article\n",
    "                processed += \" \"\n",
    "            except:\n",
    "                pass\n",
    "        # Now, append the date and the related articles to our date_index list,\n",
    "        # which we can turn into a dataframe, once it is returned.\n",
    "        date_index.append([date, processed])\n",
    "        print(f'{date}: {count}')\n",
    "\n",
    "    # Finally, return the date_index\n",
    "    return date_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JjnRhQb6gG7"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "sdate = date(2006, 11, 27)\n",
    "edate = date(2020, 11, 30)\n",
    "\n",
    "x = rolling_articles(sdate, edate, df, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGpliYIr6gG7"
   },
   "source": [
    "## 4.2 Calculating returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJf88LQU6gG7"
   },
   "outputs": [],
   "source": [
    "bb = pd.read_csv('bb_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhQgMPsy6gG7"
   },
   "outputs": [],
   "source": [
    "def calculate_returns(prices, interval):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes a dataset with prices for different securities over\n",
    "    time, where each row is a point in time and each column is a security.\n",
    "    It then calculates the returns for each security for a given interval\n",
    "    for at each date. It returns a dataset with dates as rows and securities as\n",
    "    columns, with returns for the given interval as values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Importing neccesary packages\n",
    "    import pandas as pd\n",
    "\n",
    "    # Converting all date-strings in date column to actual date objects. We can\n",
    "    # use these at a later stage to match returns to news articles.\n",
    "    prices['Dates'] = pd.to_datetime(prices['Dates']).dt.date\n",
    "\n",
    "    # Now we instantiate a new list to store our returns in.\n",
    "    date_index = []\n",
    "\n",
    "    # For every entry in the prices dataframe, try to fetch the current prices\n",
    "    # and the prices 'interval' periods in the future. If successful, get the\n",
    "    # return and append it to a list called 'returns'\n",
    "    for i in range(0,len(prices)):\n",
    "        try:\n",
    "            # Getting the current date of the entry\n",
    "            date = prices.iloc[i,0]\n",
    "\n",
    "            # Getting the prices for said date\n",
    "            prices_at_date = prices.iloc[i,1:]\n",
    "\n",
    "            # Getting the prices 'interval' periods in the future\n",
    "            prices_at_future_date = prices.iloc[i+interval,1:]\n",
    "\n",
    "            # Attempt to calculate the returns between the two periods.\n",
    "            return_at_date = list(prices_at_future_date / prices_at_date)\n",
    "\n",
    "            # Create a list called returns that contains the date. We can then\n",
    "            # append the returns in this list as well.\n",
    "            returns = [date]\n",
    "            for sector in return_at_date:\n",
    "                # For every column (sector) in our returns data, append it to\n",
    "                # the returns list.\n",
    "                returns.append(sector)\n",
    "\n",
    "            # Now, we can take the returns for each date and append it to our\n",
    "            # date_index list, which will make up our final dataframe in the end.\n",
    "            date_index.append(returns)\n",
    "        except:\n",
    "            # If we can't calculate the returns, simply pass the date.\n",
    "            pass\n",
    "\n",
    "    # Now, convert date_index to a dataframe and return the dataframe.\n",
    "    df = pd.DataFrame(date_index, columns = prices.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BokfCEUB6gG7"
   },
   "outputs": [],
   "source": [
    "y = calculate_returns(bb, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMNWr5OR6gG7"
   },
   "source": [
    "## 4.3 Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKOCN-qS6gG7"
   },
   "outputs": [],
   "source": [
    "x = x.set_index('Date')\n",
    "y = y.set_index('Dates')\n",
    "df = pd.concat([y, x.reindex(y.index)], axis = 1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZEkBe286gG7"
   },
   "source": [
    "# 5. Neural Network Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6-MTGzG6gG8"
   },
   "outputs": [],
   "source": [
    "#Plotly +4.8 is neccesary to run the code\n",
    "!pip install plotly==4.13.0 \n",
    "from numpy import array\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "kHxqct_t6gG8",
    "outputId": "8eefd866-a04a-482a-a215-db65d7117b00"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://www.dropbox.com/s/llxun0a85lpf6e3/df_final.csv?dl=1')\n",
    "df['Dates'] = pd.to_datetime(df['Dates']).dt.date\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhRPlVD26gG8"
   },
   "outputs": [],
   "source": [
    "# list of tokens in list of articles(in a day) - We train the gensim model on all data prior to 2011\n",
    "from datetime import date\n",
    "token_list = list([token.split(\" \") for token in df[df['Dates'] <= date(2010, 12, 31)]['tokens']])\n",
    "size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YM0oogiq6gG8",
    "outputId": "84d29680-949c-428e-9495-b731896a7497"
   },
   "outputs": [],
   "source": [
    "# Training gensim word2vec\n",
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences = token_list, size = size, window = 5, workers = 4, min_count = 20)\n",
    "# Vocab size:\n",
    "words = list(model.wv.vocab)\n",
    "print('vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a2WTPbK6gG9"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename = 'article_embeddings.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlU598ho6gG9"
   },
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index= {}\n",
    "f = open(os.path.join('', 'article_embeddings.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGQ1k3vY6gG9"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from datetime import date\n",
    "\n",
    "# vectorise the text samples into a 2D integer tensor with the data for token list before 2011\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(token_list)\n",
    "\n",
    "# Now convert all the tokens to sequences\n",
    "token_list = list([token.split(\" \") for token in df['tokens']])\n",
    "sequences = tokenizer.texts_to_sequences(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUMdsZTS6gG9",
    "outputId": "cf6a5332-06d1-4505-b829-e0530cf04f58"
   },
   "outputs": [],
   "source": [
    "# pad sequences to make input length constant\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# Get average of list\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "\n",
    "max_length = int(Average([len(doc) for doc in token_list]))\n",
    "articles_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print('Shape of article tensor:', articles_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4wnx5aX6gG9"
   },
   "outputs": [],
   "source": [
    "# Constructing the embedding matrix\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjo342Z76gG9"
   },
   "outputs": [],
   "source": [
    "# Import neccesary packages for NN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1BBbVsf6gG9",
    "outputId": "88fb0cc8-cc4c-4329-9850-4d7459afe54b"
   },
   "outputs": [],
   "source": [
    "# define Untrainable model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                           size,\n",
    "                           embeddings_initializer=Constant(embedding_matrix),\n",
    "                           mask_zero = True,\n",
    "                           input_length=None,\n",
    "                           trainable=False)\n",
    "# Add embedding layer\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# Add a LSTM layer with 50 internal units.\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, dropout = 0.2))\n",
    "# Add a Dense layer with 12 units.\n",
    "model.add(Dense(12))\n",
    "# Add compiler with XXX\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "# Print summary of model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1KUGcrq6gG9"
   },
   "outputs": [],
   "source": [
    "# Scale our Y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df.iloc[:,1:13]\n",
    "y = y - 1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(y)\n",
    "y = scaler.transform(y)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCblQmx46gG9"
   },
   "outputs": [],
   "source": [
    "def walk_forward_validation(model, epochs, x, y, step_size, train_steps, val_window):\n",
    "\n",
    "    \"\"\"\n",
    "    This function takes a model, specifically our neural net with multiple\n",
    "    LSTM-layers, the desired number of epochs, x data, y data, desired step\n",
    "    size, desired number of steps that should be trained per round, and the\n",
    "    desired validation window. It then trains a model through a Walk Forward\n",
    "    Validation method that stores MSE-scores for both training and validation\n",
    "    steps over time. It returns our backtesting predicted y, our trained y's\n",
    "    and our MSE-scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # First, we import the required packages. We only have one dependency which\n",
    "    # we uses to calculate the mean squared error each period\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    # Now we instantiate a couple of things. First we define how many records\n",
    "    # that we have - We use this to loop through our data. Then we define the\n",
    "    # initial training size, which gives us the point in time where we\n",
    "    # should start over test. Then we instantiate three empty lists that we\n",
    "    # later will use to store our results.\n",
    "    n_records = len(x)\n",
    "    n_init_train = step_size * train_steps\n",
    "    train_pred = []\n",
    "    val_pred = []\n",
    "    mse_scores = []\n",
    "\n",
    "    # This for loop goes from the starting point in time (as defined above)\n",
    "    # to the end of our data and step through the data, enabling us to make the\n",
    "    # walk forward validation. Our current point in time, i, will jump by the\n",
    "    # step size each iteration.\n",
    "    for i in range(n_init_train, n_records, step_size):\n",
    "\n",
    "        # We know that the starting point for the training data, must be the\n",
    "        # current point in time minus the training period.\n",
    "        train_from = i-n_init_train\n",
    "\n",
    "        # We need to train it to the current point in time.\n",
    "        train_to = i\n",
    "\n",
    "        # We then need to validate starting from tomorrow relative to\n",
    "        # the current point in time.\n",
    "        test_from = i+1\n",
    "        # And validate the desired window in the future relative to the\n",
    "        # point in time\n",
    "        test_to = i+val_window\n",
    "\n",
    "        # Now we can split our data at this point in time\n",
    "        x_train, x_test = x[train_from:train_to], x[test_from:test_to]\n",
    "        y_train, y_test = y[train_from:train_to], y[test_from:test_to]\n",
    "\n",
    "        # And then use the data to train the model\n",
    "        print(f'Train from {i-n_init_train} to {i} and validate for {i+1} to {i+val_window}')\n",
    "        model.fit(x_train, y_train, epochs=epochs, verbose=1)\n",
    "\n",
    "        # Here, we can store the training phase's historical predictions of seen y.\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        for y_train_day in y_train_pred:\n",
    "            train_pred.append(y_train_day.tolist())\n",
    "\n",
    "        # Here, we store the validation phase's future predictions of unseen y.\n",
    "        y_pred = model.predict(x_test)\n",
    "        for y_test_day in y_pred:\n",
    "            val_pred.append(y_test_day.tolist())\n",
    "\n",
    "        # Here, we calculate MSE for both and append it to our MSE-scores list.\n",
    "        train_mse = mean_squared_error(y_train,y_train_pred)\n",
    "        val_mse = mean_squared_error(y_test,y_pred)\n",
    "        mse_scores.append([train_mse, val_mse])\n",
    "\n",
    "        print(f'     train: {train_mse} \\nvalidation: {val_mse} \\n')\n",
    "\n",
    "    # Lastly, we return the training predictions, the actual validation\n",
    "    # predictions as well as the observed MSE-scores.\n",
    "    return train_pred, val_pred, mse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "BloIK4a16gG9",
    "outputId": "17a7bcc9-eb0a-4b27-b945-9758ed5f5312"
   },
   "outputs": [],
   "source": [
    "# Epoch Hyper parameter tuning\n",
    "epoch_tuning_performance = []\n",
    "for epoch in range(1,11):\n",
    "    train_pred, val_pred, validation_metrics = walk_forward_validation(model = model, epochs = epoch, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
    "    validation_metrics_total = pd.DataFrame(validation_metrics).mean(axis=0)\n",
    "    mean_train_mse = validation_metrics_total.iloc[0]\n",
    "    mean_val_mse = validation_metrics_total.iloc[1]\n",
    "    epoch_tuning_performance.append([mean_train_mse, mean_val_mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "mvqMvjJt6gG9",
    "outputId": "e5650295-6d22-4584-ede5-61893c238ae7"
   },
   "outputs": [],
   "source": [
    "# Using plotly.express to display epoch tuning\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "epoch_tuning = pd.DataFrame(epoch_tuning_performance).reset_index(drop=False)\n",
    "epoch_tuning.columns = ['epochs', 'train', 'validation']\n",
    "fig = px.line(epoch_tuning, x='epochs', y=[\"train\",\"validation\"], color_discrete_sequence=['cornflowerblue', 'indigo'])\n",
    "fig.update_xaxes(title_text='Epochs', showgrid=False)\n",
    "fig.update_yaxes(title_text='MSE')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4DIqEbI6gG9",
    "outputId": "68f4d015-1f05-44ca-ede8-14aecd122a5c"
   },
   "outputs": [],
   "source": [
    "# Train model and get MSE & Predicted values\n",
    "train_pred, val_pred, validation_metrics = walk_forward_validation(model = model, epochs = 7, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
    "validation_metrics_total = pd.DataFrame(validation_metrics).mean(axis=0)\n",
    "mean_train_mse = validation_metrics_total.iloc[0]\n",
    "mean_val_mse = validation_metrics_total.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "u2zJrRXL6gG9",
    "outputId": "9853cded-c545-47a3-f065-55752b5dee35"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(train_pred)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "oKHwlTGW6gG9",
    "outputId": "c332212f-3f0f-4a96-cdd7-5f1cef663591"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(val_pred)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9nfoPzK96gG9",
    "outputId": "c4374391-6eb0-459c-c3b2-f967e5c0a892"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(validation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r3CW3if86gG9",
    "outputId": "0c450b95-df65-421b-98ae-6c6a41ad7b85"
   },
   "outputs": [],
   "source": [
    "# define Trainable model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                           size,\n",
    "                           embeddings_initializer=Constant(embedding_matrix),\n",
    "                           mask_zero = True,\n",
    "                           input_length=None,\n",
    "                           trainable=True)\n",
    "# Add embedding layer\n",
    "model.add(embedding_layer)\n",
    "\n",
    "# Add a LSTM layer with 50 internal units.\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(100,12), dropout = 0.2))\n",
    "model.add(LSTM(50, dropout = 0.2))\n",
    "# Add a Dense layer with 12 units.\n",
    "model.add(Dense(12))\n",
    "# Add compiler with XXX\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "# Print summary of model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSv3gPf06gG9",
    "outputId": "7cd0af9b-f694-4d12-f52f-7a965ddb1be0"
   },
   "outputs": [],
   "source": [
    "# Train model and get MSE & Predicted values\n",
    "train_pred_trained, val_pred_trained, validation_metrics_trained = walk_forward_validation(model = model, epochs = 7, x = pd.DataFrame(articles_pad[970:]), y = y[970:], step_size = 60, train_steps = 3, val_window = 60)\n",
    "validation_metrics_trained_total = pd.DataFrame(validation_metrics_trained).mean(axis=0)\n",
    "mean_train_trained_mse = validation_metrics_trained_total.iloc[0]\n",
    "mean_val_trained_mse = validation_metrics_trained_total.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "0uZlNHMG6gG9",
    "outputId": "8186735c-4625-4c8e-d58a-c300c5e2edd1"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(train_pred_trained)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "A9aTV_UK6gG9",
    "outputId": "f920e33f-e70e-49a6-ec4f-65f100977e3f"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(scaler.inverse_transform(val_pred_trained)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wyI8y_2S6gG9",
    "outputId": "17bdec53-2d93-4520-b9c8-c76af8fe9045"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(validation_metrics_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "X0nS4PCz8PGy",
    "outputId": "00b33c6d-fe23-460e-82ac-3fb828f04d09"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# import untrainable validation metrics\n",
    "#un_val = pd.read_csv('validation_metrics_trainable_false.csv', names=['quarters', 'train_un', 'validation_un'], header=0)\n",
    "un_val = pd.DataFrame(validation_metrics)\n",
    "un_val.columns = ['train_un', 'validation_un']\n",
    "# import trainable validation metrics\n",
    "#tr_val = pd.read_csv('validation_metrics_train.csv', names=['train', 'validation'], header=0, usecols=[1,2])\n",
    "tr_val = pd.DataFrame(validation_metrics_trained)\n",
    "tr_val.columns = ['train', 'validation']\n",
    "\n",
    "# The two dataframes are merged in order to be able to plot training and validation together\n",
    "val_metrics = pd.DataFrame(pd.concat([un_val, tr_val], axis=1))\n",
    "val_metrics = val_metrics.reset_index(drop=False)\n",
    "\n",
    "# Plot untrained and trained validation sets\n",
    "\n",
    "fig4 = px.line(val_metrics, x='index', y=['validation_un', 'validation'], color_discrete_sequence=['cornflowerblue', 'indigo'])\n",
    "fig4.update_xaxes(title_text='Quarters', showgrid=False)\n",
    "fig4.update_yaxes(title_text='MSE')\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SFhCgJnlEJNL",
    "outputId": "f95d5fa0-ba7e-4b2c-c1c0-80624b6bf93b"
   },
   "outputs": [],
   "source": [
    "val_metrics"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "L5OX6x3b6gG6",
    "csrQZGwb6gG6",
    "KdU515V_6gG7",
    "nNCCxlAH6gG7",
    "ocK1JaFC6gG7",
    "mFofqhZD6gG7",
    "3rhuviNn6gG7",
    "85amaHml6gG7",
    "DofYZs246gG7",
    "mGpliYIr6gG7",
    "bMNWr5OR6gG7"
   ],
   "name": "Executable notebook copy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
